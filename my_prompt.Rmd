
Below are relevant files from connected R projects.

# R code (project repboxAI)


# FILE: data_to_prompt.R
```
example = function() {
  # dummy data
  set.seed(42)
  df = data.frame(
    respondent_id = sprintf("ID%03d", 1:100),
    name          = sample(c("Alice", "Bob", NA, "Diane"), 100, TRUE),
    income        = rnorm(100, 50000, 12000),
    birth_date    = as.Date("1980-01-01") + sample(0:15000, 100, TRUE),
    stringsAsFactors = FALSE
  )
  attr(df$income, "label") = "Annual income in USD"
  
  cat(data_set_to_prompt(df, "demo_data"))
  data_set_quick_overview(df, "mydata")
}


#' Compact, prompt‑ready data‑set overview (no parentheses)
#'
#' @param dat         data.frame / tibble
#' @param data_name   Header label
#' @param top_n       How many examples or levels to show (default 3)
#' @param trunc_chars Truncate long strings / levels to this many chars (default 10)
#' @return            Single character string
#'
data_set_to_prompt = function(dat, data_name,
                                top_n = 3, trunc_chars = 10) {

  if (!requireNamespace("stringi", quietly = TRUE))
    stop("Install the 'stringi' package.")
  if (!requireNamespace("dplyr", quietly = TRUE))
    stop("Install the 'dplyr' package.")

  library(stringi)
  library(dplyr)
  library(purrr)

  # ------------------------------------------------------------------
  # helpers
  # ------------------------------------------------------------------
  get_type = function(x) {
    cls <- class(x)[1]
    if (cls %in% c("numeric", "double"))  "num"
    else if (cls == "integer")            "int"
    else if (inherits(x, "Date"))         "date"
    else if (inherits(x, "POSIXt"))       "datetime"
    else if (is.factor(x))                "factor"
    else                                  "chr"
  }

  trunc_str = function(s, k = trunc_chars) {
    s <- as.character(s)
    too_long <- nchar(s, type = "chars") > k
    s[too_long] <- paste0(substr(s[too_long], 1, k), "...")
    s
  }

  summarise_var = function(x, v_type) {
    if (v_type %in% c("num", "int")) {
      s <- summary(x)
      sprintf("stats=min=%s med=%s max=%s",
              format(s["Min."], trim = TRUE),
              format(s["Median"], trim = TRUE),
              format(s["Max."], trim = TRUE))
    } else if (v_type == "chr") {
      uvals <- unique(x[!is.na(x)])
      if (length(uvals) == 0) return("")
      samp  <- trunc_str(sample(uvals, min(top_n, length(uvals))))
      paste0("sample=", stri_join(samp, collapse = ", "))
    } else if (v_type == "factor") {
      tbl <- sort(table(x, useNA = "no"), decreasing = TRUE)
      if (length(tbl) == 0) return("")
      top <- head(tbl, top_n)
      names(top) <- trunc_str(names(top))
      paste0("main_levels=", stri_join(names(top), "(", as.integer(top), ")",
                                       collapse = ", "))
    } else if (v_type %in% c("date", "datetime")) {
      if (all(is.na(x))) return("")
      rng <- range(x, na.rm = TRUE)
      paste0("range=", format(rng[1]), " ... ", format(rng[2]))
    } else ""
  }

  # ------------------------------------------------------------------
  # pre‑compute column widths for tidy alignment
  # ------------------------------------------------------------------
  var_names_trunc <- trunc_str(names(dat), 30)            # never wider than 30
  var_width  <- max(nchar(var_names_trunc))               # pad to longest
  type_width <- 8                                         # "datetime" fits
  int_width  <- nchar(as.character(nrow(dat)))            # for non‑NA/unique

  # ------------------------------------------------------------------
  # build variable lines
  # ------------------------------------------------------------------
  lines <- map_chr(
    seq_along(dat),
    function(i) {
      var      <- var_names_trunc[i]
      x        <- dat[[i]]
      v_type   <- get_type(x)
      n_non_na <- sum(!is.na(x))
      n_unique <- length(unique(x[!is.na(x)]))
      miss_pct <- round(mean(is.na(x)) * 100, 1)
      summary  <- summarise_var(x, v_type)

      sprintf(
        "%-*s  type=%-*s  non-NA=%*d  unique=%*d  miss%%=%5.1f  %s",
        var_width,  var,
        type_width, v_type,
        int_width,  n_non_na,
        int_width,  n_unique,
        miss_pct,
        summary
      )
    }
  )

  # ------------------------------------------------------------------
  # variable labels
  # ------------------------------------------------------------------
  label_lines <- map_chr(
    names(dat),
    function(v) {
      lbl <- attr(dat[[v]], "label", exact = TRUE)
      if (is.null(lbl) || lbl == "") "" else sprintf("%s: %s", v, lbl)
    }
  )
  label_lines <- label_lines[label_lines != ""]

  # ------------------------------------------------------------------
  # header + assembly
  # ------------------------------------------------------------------
  header <- sprintf(
    "%s  |  n=%d  vars=%d  size=%.2f MB",
    data_name, nrow(dat), ncol(dat),
    round(as.numeric(object.size(dat)) / 1024^2, 2)
  )
  out <- c(header, strrep("-", nchar(header)), lines)
  if (length(label_lines) > 0)
    out <- c(out, "", "Variable labels:", label_lines)

  stringi::stri_join(out, collapse = "\n")
}


#' Build a privacy‑aware description of a data frame for LLM prompts
#'
#' The output is ready to paste under “Materials supplied to you” in your
#' privacy‑audit prompt.  It now *starts* with a one‑sentence disclaimer:
#'     “Sample values below are masked or truncated …”
#'
#' @param dat        A data.frame or tibble
#' @param data_name  Character; human‑readable name of the data set
#' @param max_sample Max number of example values per variable (default 3)
#' @return           A single character string
#'
#' Dependencies: stringi  (install.packages("stringi"))
#'
#' Build a privacy‑aware description of a data frame for LLM prompts
#'
#' *No global date range is reported.*  
#' Sample values are masked or truncated **only in this summary**
#' to avoid leaking PII; the underlying data remain unchanged.
#'
#' Masking rule  : show first 2 and last 1 characters → "Al***r"  
#' Truncation    : show first 3 characters, then "..." → "Ale..."  
#'
#' @param dat        data.frame or tibble to summarise
#' @param data_name  Character; human‑readable dataset name
#' @param max_sample Integer; examples per variable (default 3)
#' @param max_total  Integer; string length at which to truncate (default 12)
#' @return           Single character string, ready for LLM prompts
#'
#' Requires package stringi
#'
data_set_to_prompt_privacy = function(dat, data_name,
                                    max_sample = 3, max_total = 12) {

  if (!requireNamespace("stringi", quietly = TRUE))
    stop("Please install the 'stringi' package.")
  library(stringi)

  ## ------------------------------------------------------------------------
  ## Helper functions
  ## ------------------------------------------------------------------------

  get_type = function(x) {
    cls = class(x)[1]
    if (cls %in% c("numeric", "integer")) "num"
    else if (inherits(x, "Date"))         "date"
    else if (inherits(x, "POSIXt"))       "datetime"
    else if (is.factor(x))                "factor"
    else                                  "chr"
  }

  # Mask (*** between first 2 & last 1) or truncate (first 3 + ...)
  mask_string = function(s, max_total = 12) {
    s = as.character(s)
    if (is.na(s)) return("NA")
    n = nchar(s, type = "chars")

    if (n <= 3) {
      s  # keep as is (very short)
    } else if (n <= max_total) {
      paste0(substr(s, 1, 2), "***", substr(s, n, n))
    } else {
      paste0(substr(s, 1, 3), "...")
    }
  }

  # Build a single variable line
  build_var_line = function(var_name) {
    x            = dat[[var_name]]
    n_non_na     = sum(!is.na(x))
    n_unique     = length(unique(x[!is.na(x)]))
    na_pct       = if (nrow(dat) == 0) 0 else (1 - n_non_na / nrow(dat)) * 100
    var_type     = get_type(x)

    # Random sample of distinct non‑NA values
    if (n_non_na == 0) {
      sample_vals = "NA"
    } else {
      samp = sample(unique(x[!is.na(x)]), min(max_sample, n_unique))
      samp = switch(
        var_type,
        chr      = vapply(samp, mask_string, character(1), max_total = max_total),
        factor   = vapply(as.character(samp), mask_string, character(1),
                          max_total = max_total),
        num      = format(round(as.numeric(samp), 3), trim = TRUE),
        date     = format(as.Date(samp)),
        datetime = format(as.POSIXct(samp, tz = "UTC")),
        samp
      )
      sample_vals = stri_join(samp, collapse = ", ")
    }

    # Date range inside variable description (if applicable)
    range_str = ""
    if (var_type %in% c("date", "datetime") && n_non_na > 0) {
      rng = range(x, na.rm = TRUE)
      range_str = stri_paste("; range: ",
                             format(rng[1], "%Y-%m-%d"), " ... ",
                             format(rng[2], "%Y-%m-%d"))
    }

    stri_paste(
      var_name, " (", n_unique, "/", n_non_na,
      " unique, ", var_type, ", NA%=", sprintf("%.1f", na_pct), "): ",
      sample_vals, range_str
    )
  }

  ## ------------------------------------------------------------------------
  ## Header
  ## ------------------------------------------------------------------------
  n_obs   = nrow(dat)
  n_vars  = ncol(dat)
  size_mb = round(as.numeric(object.size(dat)) / 1024^2, 2)

  header = stri_paste(
    data_name, " (", n_obs, " obs, ", n_vars,
    " vars, ", size_mb, " MB)"
  )

  disclaimer = paste(
    "NOTE: Sample values below are randomly chosen and may be",
    "masked (*** pattern) or truncated (...) solely for this",
    "summary; the original dataset is unchanged."
  )

  ## ------------------------------------------------------------------------
  ## Variable lines and variable‑label appendix
  ## ------------------------------------------------------------------------
  var_lines = vapply(names(dat), build_var_line, character(1), USE.NAMES = FALSE)

  label_lines = vapply(
    names(dat),
    function(v) {
      lbl = attr(dat[[v]], "label", exact = TRUE)
      if (is.null(lbl) || lbl == "") "" else stri_paste(v, ": ", lbl)
    },
    character(1),
    USE.NAMES = FALSE
  )
  label_lines = label_lines[label_lines != ""]

  ## ------------------------------------------------------------------------
  ## Combine and return
  ## ------------------------------------------------------------------------
  descr_parts = c(header, disclaimer, var_lines)
  if (length(label_lines) > 0)
    descr_parts = c(descr_parts, "", "Variable labels:", label_lines)

  stringi::stri_join(descr_parts, collapse = "\n")
}
```
# END OF FILE: data_to_prompt.R

-----------------------------------------------------------


# FILE: dirs.R
```
rai_fp_dir = function(project_dir, doc_type) {
  paste0(project_dir, "/fp/prod_", doc_type)
}


rai_fp_dirs = function(project_dir) {
  dir(file.path(project_dir, "fp"), pattern=glob2rx("prod_*"), recursive = FALSE, full.names=TRUE)
}

rai_ver_dir_to_project_dir = function(ver_dir) {
  dirname(dirname(dirname(dirname(ver_dir))))
}

rai_ver_dir_to_doc_type = function(ver_dir) {
  fp_dir = basename(fp_ver_dir_to_fp_dir(ver_dir))
  stri_sub(fp_dir, 6)
}


project_dir_to_fp_dir = function(project_dir, doc_type = "art") {
  fp_dir = file.path(project_dir, "fp", paste0("prod_", doc_type))
  fp_dir
}

doc_dir_to_fp_dir = function(doc_dir) {
  if (!basename(dirname(doc_dir))=="doc") stop(paste0("Not a proper doc_dir: ", doc_dir))
  project_dir = dirname(dirname(doc_dir))
  doc_type = repboxDoc::rdoc_type(doc_dir)
  project_dir_to_fp_dir(project_dir, doc_type)
}

```
# END OF FILE: dirs.R

-----------------------------------------------------------


# FILE: ev_map.R
```
# Functions that evaluate mappings between regressions in tables and 
# regression outputs in Stata code.

# relevant products tab_main (table structure)
# map_reg_run, map_reg_inv_run, map_reg_static
```
# END OF FILE: ev_map.R

-----------------------------------------------------------


# FILE: ev_tab.R
```
# Rank tab versions and pick a standard

example = function() {
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
  doc_type = "art"
  doc_type = "app1"
  
  rstudioapi::filesPaneNavigate(project_dir)
}

repbox_ev_tab = function(project_dir, doc_type="art") {
  fp_dir = project_dir_to_fp_dir(project_dir, doc_type)
  ev_dir = file.path(project_dir, "fp", paste0("eval_", doc_type))
  if (!dir.exists(ev_dir)) dir.create(ev_dir)
  
  fp_all_ver_dirs(fp_dir, "cell_base")
  cell_base = fp_load_all_prod_df(fp_dir, "cell_base")
  cell_base = cell_base %>% group_by(ver_id, tabid) %>%
    filter(endsWith(ver_id, "v0"))
  
  deci_df = cell_base %>% filter(has_num & is.true(has_deci))
  id_li = list(
    deci_df %>%
      arrange(row, col) %>%
      summarize(ind = "by_row_deci", value = paste0(bracket,num_str, collapse="|")),
    deci_df %>%
      arrange(col,row) %>%
      summarize(ind = "by_col_deci", value = paste0(bracket,num_str, collapse="|")),
    deci_df %>%
      arrange(num_str, bracket) %>%
      summarize(ind = "set_deci", value =  paste0(bracket,num_str, collapse="|"))
  )
  id_df = bind_rows(id_li)    

  group_df = id_df %>%
    group_by(tabid, ind, value) %>%
    summarize(
      group = list(ver_id),
      group_size = n()
    )
  
  ev_df = id_df %>%
    group_by(tabid, ind) %>%
    summarize(
      num_ver = n(),
      num_group = n_distinct(value)
    )
  
} 

left_join_all = function(li, by=NULL) {
  df = li[[1]]
  for (i in setdiff(seq_along(li),1)) {
    df = left_join(df, li[[i]]) 
  }
  df
}
```
# END OF FILE: ev_tab.R

-----------------------------------------------------------


# FILE: load.R
```
rai_agg_all_prod_df = function(projects_dir, prod_id) {
  restore.point("rai_agg_all_prod_df")
  df = FuzzyProduction::fp_load_all_prod_df(projects_dir,prod_id, add_ver_dir=TRUE)
  df$project_dir = str.left.of(df$ver_dir,"/fp/")
  df$artid = basename(df$project_dir)
  df = df[, union("artid", names(df))]
  df
}


rai_file_cache = function(...) {
  new.env(parent=emptyenv())
}


rai_pick_tab_ver = function(fp_dir, prod_id = "tab_html", pref=NULL, proc_id=NULL) {
  restore.point("rai_pick_tab_ver")
  if (is.null(pref)) {
    if (prod_id == "tab_notes") {
      pref = fp_pref(proc_regex = c("html","^pdf-g2f.*"))
    } else {
      pref = fp_pref(proc_regex = c("html", "mocr","pdf-.*","pdf_txt"))
      
    }
  }
  fp_pick_prod_ver(fp_dir, prod_id=prod_id,proc_id=proc_id, pref=pref)
}

# Shorten file paths if files are unique
# to reduce length of output tokens
script_df_shorten_file = function(script_df) {
  if (!anyDuplicated(script_df$file_path)) {
    script_df$script_file = basename(script_df$file_path)
  } else {
    script_df$script_file = script_df$file_path
  }
  script_df
}

rai_load_do_source = function(project_dir, parcels=list()) {
  parcels = repdb_load_parcels(project_dir, "stata_source",parcels)
  script_df = parcels$stata_source$script_source
  script_df = script_df_shorten_file((script_df))
  script_df
}

rai_load_tab_prod_df = function(fp_dir, prod_id = "tab_html", pref=NULL, proc_id=NULL, cache=NULL) {
  restore.point("rai_load_tab_prod_df")
  ver_dir = rai_pick_tab_ver(fp_dir, prod_id, pref, proc_id)$ver_dir
  file = file.path(ver_dir, "prod_df.Rds")
  if (!is.null(cache)) {
    prod_df = cache[[file]]
    if (!is.null(prod_df)) return(prod_df)
  }
  prod_df = fp_load_prod_df(ver_dir)
  if (!is.null(cache)) {
    cache[[file]] = prod_df
  }
  prod_df
}

rai_pick_ref_li_doc_dir = function(project_dir, doc_type="art", tab_ref_pref = tab_ref_default_pref()) {
  restore.point("rai_load_ref_li")
  files = paste0(project_dir, "/doc/", doc_type, "_", tab_ref_pref, "/ref_li.Rds")
  files = files[file.exists(files)]
  if (length(files)==0) return(NULL)
  file = files[[1]]
  dirname(file)
}


rai_doc_file = function(project_dir, doc_type, pref = doc_file_form_default_pref(), doc_form=NULL) {
  repbox_doc_file_select(project_dir, doc_type = doc_type, doc_file_form_pref = pref, doc_form=doc_form)$doc_file
}
```
# END OF FILE: load.R

-----------------------------------------------------------


# FILE: media_run_do.R
```
# Create Stata log files for AI

example = function() {
  #chooseCRANmirror()
  #install.packages("dplyr")
  library(repboxDB)
  library(repboxTableTools)
  library(repboxAI)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_1_2_4"
  outfile = rai_media_run_do(project_dir)
  rstudioapi::filesPaneNavigate(dirname(outfile))
}


opts_media_run_do = function(do_max_runs=100000,line_max_runs=100,log_max_char=100000,...) {
  opts = copy_into_list()
  opts
}


rai_media_run_do = function(project_dir, parcels = list(), output_just_runid = NULL, opts=opts_media_run_do(), just_reg_log=TRUE, header=NULL) {
  restore.point("rai_media_run_do")
  parcels = repdb_load_parcels(project_dir, c("stata_source", "stata_run_cmd", "stata_run_log","stata_cmd"), parcels=parcels)
  script_df = parcels$stata_source$script_source
  
  cmd_df = parcels$stata_cmd$stata_cmd %>% left_join(script_df %>% select(file_path, script_num), by="file_path")
  log_df = parcels$stata_run_log$stata_run_log
  run_df = parcels$stata_run_cmd$stata_run_cmd
  run_df = left_join(run_df, cmd_df %>% select(artid, file_path, line, is_reg), by = c("artid", "file_path", "line"))
  
  if (just_reg_log) {
    library(repboxStata)
    reg_cmds = unique(c(stata_cmds_reg(), stata_cmds_postreg_comp(), stata_cmds_quasireg()))
    reg_cmd_df = cmd_df %>% filter(cmd %in% reg_cmds | is_reg) 
    
    script_nums = unique(reg_cmd_df$script_num)
    script_df = script_df[script_df$script_num %in% script_nums,]
    cmd_df = cmd_df[cmd_df$script_num %in% script_nums,]
    
    # add runid to regression commands, postregression commands that compute something and quasi regression commands
    
    cmd_runids = run_df$runid[run_df$is_reg | run_df$cmd %in% reg_cmds]
    
    if (is.null(output_just_runid)) {
      output_just_runid = cmd_runids
    } else {
      output_just_runid = intersect(output_just_runid, cmd_runids)
    }
    if (is.null(header)) {
      header="This file contains all Stata do scripts that contain at least one regression command. 

Line numbers for each script are shown.      
      
For regression commands the logged output is shown below the code line in a block like this example:
      
```output runid=25
. reg y x1 x2, robust
Linear regression                               Number of obs     =        749
                                                F(37, 711)        =       4.94

[Further output ommited in example]

```end_ouput runid=25

If the regression command was called multiple times inside a loop, multiple output blocks are shown for that code line.
            "

    }
    
  }
  
  if (!is.null(output_just_runid)) {
    log_df = log_df %>% filter(runid %in% output_just_runid)
    run_df = run_df %>% filter(runid %in% output_just_runid)
  }
  run_df = run_df  %>%
    left_join(log_df %>% select(runid, logtxt), by="runid") %>%
    left_join(script_df %>% select(file_path, script_num), by="file_path")
  
  run_df = run_df %>%
    adapt_too_big_run_df(opts=opts)
  
  ldf = script_df %>%
    mutate(
      txt = stri_split_fixed(script_df$text,"\n")
    ) %>%
    select(script_num, txt) %>%
    tidyr::unnest(txt) %>%
    group_by(script_num) %>%
    mutate(
      orgline = seq_along(txt),
      #txt = htmltools::htmlEscape(txt)
    )
  
  ldf = left_join(ldf,
    select(cmd_df, orgline_start, orgline_end, line, is_reg, cmd, script_num),
    join_by(script_num, between(orgline,orgline_start,orgline_end))
  )
  
  ldf = ldf %>%
    mutate(
      line_changes = !is.true(lag(line)==line | is.na(lag(line) & is.na(line))),
      line_group = cumsum(line_changes)
    ) %>%
    group_by(line_group) %>%
    mutate(
      orgline_start = min(orgline),
      orgline_end = max(orgline)
    )
  
  #pad_len = max(2, ceiling(log(max(ldf$orgline+1),10)))
  #stri_pad_left(ldf$orgline, pad_len)
  block_df = ldf %>%
    group_by(script_num, orgline_start, orgline_end, line) %>%
    summarize(
      code_txt =  paste0(orgline,": ",txt, collapse="\n")
    ) %>%
    ungroup()
      
  # Now we aggregate log on a line level
  loli_df = run_df %>%
    #left_join(run_df %>% select(runid, line, cmdline), by=c("runid")) %>%
    mutate(
      # correct weird log output
      logtxt = ifelse(endsWith(trimws(cmdline),"{"), "", logtxt),
      logtxt = paste0(". ", cmdline, "\n", logtxt),
      # reduce logtext that is too long
      len_logtxt = nchar(logtxt),
      logtxt = ifelse(is.true(len_logtxt > opts$log_max_char), paste0(substring(logtxt,1, opts$log_max_char), "\n... further output omitted ..."), logtxt)
    ) %>%
    left_join(block_df %>% select(line, script_num, orgline_start), by = c("line", "script_num")) %>%
    mutate(
      log_md = paste0('\n```output runid=', runid,'\n',logtxt,'\n```end_output runid=', runid,"\n")
    ) %>%
    group_by(script_num, line) %>%
    summarize(
      log_md = paste0(log_md, collapse="\n")
    )
  
  code_df = block_df %>%
    left_join(loli_df, by = c("script_num","line")) %>%
    mutate(
      log_md = na_val(log_md, ""),
      txt = paste0(code_txt, log_md)
    )
  
  
  all_df = code_df %>%
    left_join(script_df %>% select(file_path, script_num), by="script_num") %>%
    group_by(script_num) %>%
    summarize(
      script_html = paste0('\n\n#########################\nScript: ',  first(file_path), '\n#########################\n\n', paste0(txt, collapse="\n"))
    )

  text = paste0(header,paste0(all_df$script_html, collapse="\n"))
  outdir = paste0(project_dir, "/fp/prompt_files")
  if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)
  outfile = file.path(outdir, "do_run.txt")
  writeUtf8(text, outfile)
  return(outfile)
}




adapt_too_big_run_df = function(run_df, opts) {
  restore.point("adapt_too_big_run_df")
  run_df = run_df %>%
    group_by(script_num, line) %>%
    mutate(line.run.count = 1:n()) %>%
    ungroup() %>%
    filter(line.run.count <= opts$line_max_runs) %>%
    group_by(script_num) %>%
    mutate(do.run.count = 1:n()) %>%
    ungroup() %>%
    filter(do.run.count <= opts$do_max_runs)
  
  run_df
}



# rai_media_run_do_old = function(project_dir, parcels = list(), output_just_runid = NULL, opts=opts_do_run_html()) {
#   restore.point("rai_media_run_do")
#   parcels = repdb_load_parcels(project_dir, c("stata_source", "stata_run_cmd", "stata_run_log","stata_cmd"), parcels=parcels)
#   script_df = parcels$stata_source$script_source
#   
#   cmd_df = parcels$stata_cmd$stata_cmd %>% left_join(script_df %>% select(file_path, script_num), by="file_path")
#   log_df = parcels$stata_run_log$stata_run_log
#   run_df = parcels$stata_run_cmd$stata_run_cmd
#   
#   if (!is.null(output_just_runid)) {
#     log_df = log_df %>% filter(runid %in% output_just_runid)
#     run_df = run_df %>% filter(runid %in% output_just_runid)
#   }
#   run_df = run_df  %>%
#     left_join(log_df %>% select(runid, logtxt), by="runid") %>%
#     left_join(script_df %>% select(file_path, script_num), by="file_path") %>%
#     adapt_too_big_run_df(opts=opts)
#   
#   ldf = script_df %>%
#     mutate(
#       txt = stri_split_fixed(script_df$text,"\n")
#     ) %>%
#     select(script_num, txt) %>%
#     tidyr::unnest(txt) %>%
#     group_by(script_num) %>%
#     mutate(
#       orgline = seq_along(txt),
#       #txt = htmltools::htmlEscape(txt)
#     )
#   
#   ldf = left_join(ldf,
#     select(cmd_df, orgline_start, orgline_end, line, is_reg, cmd, script_num),
#     join_by(script_num, between(orgline,orgline_start,orgline_end))
#   )
#   
#   ldf = ldf %>%
#     mutate(
#       line_changes = !is.true(lag(line)==line | is.na(lag(line) & is.na(line))),
#       line_group = cumsum(line_changes)
#     ) %>%
#     group_by(line_group) %>%
#     mutate(
#       orgline_start = min(orgline),
#       orgline_end = max(orgline)
#     )
#   
#   block_df = ldf %>%
#     group_by(script_num, orgline_start, orgline_end, line) %>%
#     summarize(
#       code_html = paste0('<pre class="do_code" script_num=', first(script_num),' line=',first(orgline_start),' line_end = ', first(orgline_end),'>\n',htmltools::htmlEscape(paste0(txt, collapse="\n")),'\n</pre>')
#     ) %>%
#     ungroup()
#       
#   # Now we aggregate log on a line level
#   loli_df = run_df %>%
#     #left_join(run_df %>% select(runid, line, cmdline), by=c("runid")) %>%
#     mutate(
#       # correct weird log output
#       logtxt = ifelse(endsWith(trimws(cmdline),"{"), "", logtxt),
#       # reduce logtext that is too long
#       len_logtxt = nchar(logtxt),
#       logtxt = ifelse(is.true(len_logtxt > opts$log_max_char), paste0(substring(logtxt,1, opts$log_max_char), "\n... further output omitted ..."), logtxt)
#     ) %>%
#     #left_join(cmd_df %>% select(line, script_num), by = c("line", "script_num") ) %>%
#     left_join(block_df %>% select(line, script_num, orgline_start), by = c("line", "script_num")) %>%
#     mutate(
#       log_html = paste0('\n<pre class="do_output" runid=', runid,' line="', orgline_start,'">\n',htmltools::htmlEscape(logtxt),'\n</pre>')
#     ) %>%
#     group_by(script_num, line) %>%
#     summarize(
#       log_html = paste0(log_html, collapse="\n")
#     )
#   
#   code_df = block_df %>%
#     left_join(loli_df, by = c("script_num","line")) %>%
#     mutate(
#       log_html = na_val(log_html, ""),
#       html = paste0(code_html, log_html)
#     )
#   
#   
#   all_df = code_df %>%
#     left_join(script_df %>% select(file_path, script_num), by="script_num") %>%
#     group_by(script_num) %>%
#     summarize(
#       script_html = paste0('<h2>Script <span class="script_num">', first(script_num), '</span>: <span class="script_file">', first(file_path),'</span></h2><br><div class="code_and_output">', paste0(html, collapse="\n"),"</div>")
#     )
#   
#   head_html = paste0(
# '<!DOCTYPE html>
# <html lang="en">
# <head>
#   <meta charset="UTF-8">
#   <title>Stata do files including output of successfully run code lines</title>
# <style>
# /* Base styling for all pre blocks */
# pre {
#   margin: 0px;
#   padding: 1px;
#   /*border-radius: 8px;*/
#   font-family: Consolas, Monaco, monospace;
#   /*line-height: 1.5;*/
#   overflow-x: auto;
# }
# 
# /* Stata code blocks */
# .do_code {
#   margin: 1px;
#   background-color: #f0f8ff; /* soft blue */
#   position: relative;
# }
# 
# /* Stata log output blocks */
# .do_output {
#   margin-left: 3em;
#   background-color: #f9f9f9; /* light gray */
#   position: relative;
#   color: #444;
# }
# </style>
# ')
#   html = paste0(head_html, paste0(all_df$script_html, collapse="\n"), '</body></html>')
#   outdir = paste0(project_dir, "/fp/prompt_files")
#   if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)
#   outfile = file.path(outdir, "do_run.html")
#   writeUtf8(html, outfile)
#   return(outfile)
# }



```
# END OF FILE: media_run_do.R

-----------------------------------------------------------


# FILE: proc_all.R
```


repbox_default_fp_analysis = function(project_dir) {
  library(repboxAI)
  library(aikit)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_1_2_4"
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  if (FALSE)
    rstudioapi::filesPaneNavigate(project_dir)
  
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.5-flash")

  steps = repbox_fp_steps(map_reg_run = TRUE)
  repbox_run_fp(project_dir,steps, overwrite = FALSE)

    
  steps_all = repbox_fp_steps_from(tab_given=TRUE)
  steps_hx = repbox_fp_steps(tab_given=TRUE)
  steps2.0 = repbox_fp_steps_from(tab_notes_pdf = TRUE, ev_tab=FALSE)
  steps2.5 = repbox_fp_steps_from(ev_tab=TRUE)
  
  repbox_run_fp(project_dir,steps_hx, overwrite = FALSE)

  set_ai_opts(model = "gemini-2.5-flash-lite-preview-06-17")
  repbox_run_fp(project_dir,steps2.0, overwrite = FALSE)

  
  set_ai_opts(model = "gemini-2.0-flash")
  repbox_run_fp(project_dir,steps2.0, overwrite = FALSE)
  
  
  
  set_ai_opts(model = "gemini-2.5-flash")
  repbox_run_fp(project_dir,steps2.5, overwrite = FALSE)
  
  set_ai_opts(model = "gemini-2.5-flash-lite-preview-06-17")
  repbox_run_fp(project_dir,steps2.5, overwrite = FALSE)
    
  repbox_rerun_outages(project_dir,steps = steps_all,max_repeat = 5, sleep_sec = 30)
}


example = function() {
  #ai_clear_cache()
  library(repboxAI)
  library(aikit)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.5-flash")
  #project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  #project_dir = "~/repbox/projects_share/qje_3036349" 
  #project_dir = "~/repbox/projects_share/aeri_1_2_6"
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"

  steps = repbox_fp_steps(map_inv_reg_run = TRUE)
  steps = repbox_fp_steps(map_reg_run = TRUE)
  #steps = repbox_fp_steps_base()
  #set_ai_opts(model = "gemini-2.5-pro-exp-03-25")
  repbox_run_fp(project_dir, steps,overwrite = FALSE,doc_type = "art")

  rstudioapi::filesPaneNavigate(project_dir)

    
  set_ai_opts(model = "gemini-2.5-pro-exp-03-25")
  set_ai_opts(model = "gemini-2.0-flash")
  steps = repbox_fp_steps_advanced()
  repbox_run_fp(project_dir, steps,overwrite = FALSE)
  
  
  rstudioapi::filesPaneNavigate(project_dir)
  
  
  repbox_rerun_outages(project_dir, steps)
  
  library(repboxAI)
  library(aikit)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.0-flash-thinking-exp")
  set_ai_opts(model = "gemini-2.0-flash")
  set_ai_opts(model = "gemini-2.5-pro-exp-03-25")
  set_ai_opts(model = "gemini-2.5-flash")
  
  parent_dir = "~/repbox/projects_share"
  steps2.0 = repbox_fp_steps_from(tab_given=TRUE, ev_tab=FALSE)
  steps2.5 = repbox_fp_steps_from(ev_tab=TRUE)
  
  
  steps = repbox_fp_steps(map_reg_static = TRUE)
  steps = repbox_fp_steps(reg_classify = TRUE)
  steps = repbox_fp_steps(tab_given = TRUE)
  project_dirs = repboxExplore::get_project_dirs("~/repbox/projects_share")
  #project_dir = project_dirs[1]
  for (project_dir in project_dirs) {
    cat("\n", project_dir, "\n")
    repbox_run_fp(project_dir,steps, overwrite = FALSE)
  }
  
  repbox_rerun_outages(project_dirs,steps = steps,max_repeat = 5, sleep_sec = 30)

  
  repbox_error_ver_dirs(project_dirs, steps)
  repbox_outage_ver_dirs(project_dirs, steps)
  

  library(repboxAI)
  library(aikit)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.0-flash-thinking-exp")
  set_ai_opts(model = "gemini-2.5-pro-exp-03-25")
  set_ai_opts(model = "gemini-2.0-flash")
  parent_dir = "~/repbox/projects_share"
  project_dirs = repboxExplore::get_project_dirs("~/repbox/projects_share")
  repbox_rerun_outages(project_dirs,max_repeat = 5, sleep_sec = 30)
  
    
  repbox_run_fp(project_dir,steps)
  rstudioapi::filesPaneNavigate(project_dir)
  
  project_dirs = repboxExplore::get_project_dirs("~/repbox/projects_share")
  
}


# Implement default pipeline
# see pipeline.md
repbox_fp_steps_default_reproduced = function(tab_mocr=TRUE, tab_given=FALSE, tab_notes_pdf=FALSE, tab_html_pdf=FALSE, tab_main=FALSE,map_reg_run=FALSE, ev_tab=FALSE, tab_classify = FALSE,reg_classify=FALSE,
  by_tab_classify = FALSE, by_tab_classify_nodoc=FALSE, map_reg_static = FALSE,   map_inv_reg_run=FALSE) {
  # Need to implement
}

repbox_fp_join_steps = function(...) {
  step_li = list(...)
  restore.point("repbox_fp_join_steps")
  steps = do.call(c,step_li)
  all_steps = c(repbox_fp_steps_from(), repbox_fp_readme_steps_from())
  all_steps[names(steps)] = steps
  all_steps
}

repbox_fp_steps_base = function() {
  repbox_fp_steps_from(tab_given=TRUE,by_tab_classify = FALSE)
}

repbox_fp_steps_advanced = function() {
  repbox_fp_steps_from(map_reg_static = TRUE)
}

repbox_fp_readme_steps_from = function(readme_overview=FALSE, readme_var=readme_overview, readme_script_tab_fig = readme_var, readme_data=readme_script_tab_fig, readme_vs_guide=readme_data) {
  as.list(sys.frame(sys.parent(0)))  
}

repbox_fp_readme_steps = function(readme=FALSE, readme_overview=FALSE, readme_var=FALSE, readme_script_tab_fig = FALSE, readme_data=FALSE, readme_vs_guide=FALSE) {
  as.list(sys.frame(sys.parent(0)))  
}

repbox_fp_steps = function(tab_given=FALSE, tab_notes_pdf=FALSE, tab_html_pdf=FALSE, tab_main=FALSE, ev_tab=FALSE, tab_classify = FALSE, by_tab_classify = FALSE, by_tab_classify_nodoc=FALSE, map_reg_static = FALSE, reg_classify=FALSE, map_reg_run=FALSE, map_inv_reg_run=FALSE) {
  as.list(sys.frame(sys.parent(0)))
}


repbox_fp_steps_from = function(tab_given=FALSE, tab_notes_pdf=tab_given, tab_html_pdf=tab_notes_pdf, tab_main=tab_html_pdf, ev_tab=tab_main,  tab_classify = ev_tab,reg_classify= TRUE, map_reg_run = TRUE, by_tab_classify = FALSE, by_tab_classify_nodoc=FALSE, map_reg_static = FALSE,  map_inv_reg_run = FALSE) {
  as.list(sys.frame(sys.parent(0)))
}

repbox_run_fp = function(project_dir, steps = repbox_fp_steps_from(TRUE), overwrite=FALSE, overwrite_hx = overwrite, doc_type = NULL, to_v0 = TRUE) {
  restore.point("repbox_run_fp")
  org_steps = steps
  steps = repbox_fp_join_steps(steps)
  all_doc_types = repbox_doc_types(project_dir)
  if (!is.null(doc_type)) {
    doc_type = intersect(doc_type, all_doc_types)
  } else {
    doc_type = all_doc_types
  }
  if (isTRUE(steps$tab_given)) {
    proc_tab_given(project_dir, doc_type=doc_type)
  }
  pdf_doc_dirs = repbox_doc_dirs(project_dir, doc_form="pdf", doc_type=doc_type)
  if (isTRUE(steps$tab_notes_pdf)) {
    proc_tab_notes_from_pdf(project_dir, doc_type=doc_type, overwrite = overwrite)
  }
  if (isTRUE(steps$tab_html_pdf)) {
    proc_tab_html_from_pdf(project_dir,doc_type = doc_type, overwrite=overwrite)
  }
  if (isTRUE(steps$tab_main)) {
    proc_tab_main(project_dir, overwrite=overwrite)
  }
  
  if (isTRUE(steps$ev_tab)) {
    for (dt in doc_type) {
      repbox_ev_tab(project_dir, doc_type=dt)
    }
  }
  if (isTRUE(steps$readme_overview)) {
    proc_readme(project_dir,"readme_overview", overwrite=overwrite)
  }
  if (isTRUE(steps$readme_var)) {
    proc_readme(project_dir,"readme_var", overwrite=overwrite)
  }
  if (isTRUE(steps$readme_script_tab_fig)) {
    proc_readme(project_dir,"readme_script_tab_fig", overwrite=overwrite)
  }
  if (isTRUE(steps$readme_data)) {
    proc_readme(project_dir,"readme_data", overwrite=overwrite)
  }
  if (isTRUE(steps$readme_vs_guide)) {
    proc_readme(project_dir,"readme_vs_guide", overwrite=overwrite)
  }

  if (isTRUE(steps$tab_classify)) {
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "tab_classify",tpl_id = "tab_classify", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) %>%
        rai_pru_add_doc() %>%
        rai_pru_add_tab_df() #%>%
        #rai_pru_add_tab_media(by_tab = FALSE, add_ref = FALSE) 
      proc_rai_pru(pru)
    }
  }
  if (iTRUE(steps$by_tab_classify)) {
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "tab_classify",tpl_id = "by_tab_classify", doc_type=dt, overwrite=overwrite, proc_postfix = "_bytab", to_v0 = to_v0) %>%
        rai_pru_add_doc() %>%
        rai_pru_add_tab_df(by_tab = TRUE) %>%
        rai_pru_add_tab_media(by_tab = TRUE, add_ref = TRUE) 
      proc_rai_pru(pru)
    }
  }
  if (isTRUE(steps$by_tab_classify_nodoc)) {
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "tab_classify",tpl_id = "by_tab_classify_nodoc", proc_postfix = "_bytab_nodoc", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) %>%
        rai_pru_add_tab_df(by_tab = TRUE) %>%
        rai_pru_add_tab_media(by_tab = TRUE, add_ref = TRUE) 
      proc_rai_pru(pru)
    }
  }
  dt = first(doc_type)
  if (isTRUE(steps$map_reg_static)) {
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "map_reg_static", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) %>%
        rai_pru_add_doc() %>%
        rai_pru_add_tab_df() %>%
        rai_pru_add_tab_media(in_context=FALSE) %>%
        rai_pru_add_static_do()
      proc_rai_pru(pru)
    }
  }
  if (isTRUE(steps$reg_classify)) {
    for (dt in doc_type) {
      cat("\nreg_classify for", dt, "of", project_dir, "\n")
      pru = 
        rai_pru_base(project_dir, "reg_classify", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) %>%
        rai_pru_add_doc() %>%
        rai_pru_add_tab_df() %>%
        rai_pru_add_reg_list(static=FALSE, filter_tab_df = TRUE) %>%
        rai_pru_add_tab_media(in_context=FALSE)
      
      proc_rai_pru(pru)
    }
  }
  if (isTRUE(steps$map_reg_run)) {
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "map_reg_run", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) %>%
        rai_pru_add_doc() %>%
        rai_pru_add_tab_df() %>%
        rai_pru_add_tab_media(in_context=FALSE) %>%
        rai_pru_add_run_do(in_context = FALSE)
      proc_rai_pru(pru)
    }
  }
  if (isTRUE(steps$map_inv_reg_run)) {
    restore.point("proc_map_inv_reg_run")
    for (dt in doc_type) {
      pru = 
        rai_pru_base(project_dir, "map_inv_reg_run", doc_type=dt, overwrite=overwrite, to_v0 = to_v0) |>
        rai_pru_add_reg_df()
      # no regressions successfully run. we don't perform a mapping
      if (NROW(pru[["reg_df"]])==0) {
        next
      }
      pru = pru |>
        rai_pru_add_doc() |>
        rai_pru_add_tab_df() |>
        rai_pru_add_tab_media(in_context=FALSE) |>
        rai_pru_add_run_do(in_context = FALSE,only_reg_df_output = TRUE)
      proc_rai_pru(pru)
    }
  }

  
}

repbox_error_ver_dirs = function(project_dirs, steps = repbox_fp_steps_from(TRUE)) {
  restore.point("repbox_error_ver_dirs")
  parent_dirs = file.path(project_dirs, "fp") 
  ver_dirs = NULL
  if (steps$tab_notes_pdf) {
    ver_dirs = union(ver_dirs, fp_all_error_ver_dirs(parent_dirs, "tab_notes"))
  }
  if (steps$tab_html_pdf) {
    ver_dirs = union(ver_dirs, fp_all_error_ver_dirs(parent_dirs, "tab_html"))
  }
  if (steps$readme) {
    ver_dirs = union(ver_dirs, fp_all_error_ver_dirs(parent_dirs, "readme_overview"))
  }
  ver_dirs
} 

repbox_fp_steps_to_names = function(steps) {
  names(steps[unlist(steps)])
}

repbox_outage_ver_dirs = function(project_dirs, steps = repbox_fp_steps_from(TRUE)) {
  restore.point("repbox_outage_ver_dirs")
  parent_dirs = file.path(project_dirs, "fp") 
  step_names = repbox_fp_steps_to_names(steps)
  ver_dirs = NULL
  ver_dirs = union(ver_dirs, fp_all_outage_ver_dirs(parent_dirs, step_names))
  
  if (steps$tab_notes_pdf) {
    ver_dirs = union(ver_dirs, fp_all_outage_ver_dirs(parent_dirs, "tab_notes"))
  }
  if (steps$tab_html_pdf) {
    ver_dirs = union(ver_dirs, fp_all_outage_ver_dirs(parent_dirs, "tab_html"))
  }
  if (steps$tab_classify | steps$by_tab_classify | steps$by_tab_classify_nodoc) {
    ver_dirs = union(ver_dirs, fp_all_outage_ver_dirs(parent_dirs, "tab_classify"))
  }
  ver_dirs
}  

repbox_rerun_outages = function(project_dirs, steps = repbox_fp_steps_from(TRUE), max_repeat=0, sleep_sec = 30) {
  restore.point("repbox_rerun_outages")
  steps = repbox_fp_join_steps(steps)
  ver_dirs = repbox_outage_ver_dirs(project_dirs, steps)
  cat(paste0("\n", length(ver_dirs), " outages found.\n\n"))
  if (length(ver_dirs)==0) return(NULL)
  
  rem_ver_dirs = ver_dirs
  counter = 0
  while(counter <= max_repeat) {
    counter = counter+1
    fp_rerun_all_outage_ver(ver_dirs=rem_ver_dirs)
    rem_ver_dirs = repbox_outage_ver_dirs(project_dirs, steps)
    if (length(rem_ver_dirs)==0 | counter > max_repeat) break
    cat(paste0("\n\n", length(rem_ver_dirs), " remaining outages.\nSleep for ", sleep_sec, " sec...\n"))
    Sys.sleep(sleep_sec)
  }
  invisible(ver_dirs)
}

repbox_rerun_errors = function(project_dirs, steps = repbox_fp_steps_from(TRUE)) {
  restore.point("repbox_rerun_errors")
  ver_dirs = repbox_error_ver_dirs(project_dirs, steps)
  if (length(ver_dirs)==0) return(NULL)
  fp_rerun_all_error_ver(ver_dirs=ver_dirs)
  invisible(ver_dirs)
}

fp_count_prods = function(parent_dir) {
  ver_dirs = fp_all_ver_dirs(parent_dir)
  
  df = tibble(
    ver_dir = fp_all_ver_dirs(parent_dir),
    proc_dir = fp_ver_dir_to_proc_dir(ver_dir),
    prod_dir = fp_proc_dir_to_prod_dir(proc_dir),
    proc_id = fp_proc_dir_to_proc_id(proc_dir),
    prod_id = fp_prod_dir_to_prod_id(prod_dir),
    fp_dir = fp_prod_dir_to_fp_dir(prod_dir)
  )
  
  prod_df = df %>%
    group_by(prod_id) %>%
    summarize(num_fp = n_distinct(fp_dir))
  
  proc_df = df %>%
    group_by(prod_id, proc_id) %>%
    summarize(num_fp = n_distinct(fp_dir))
  
  
  prod_ids = fp_ver_dir_to_prod_id(ver_dirs) 
  
}

fp_find_missing_prod = function(parent_dir, prod_id=NULL) {
  parent_dir = "~/repbox/projects_share"
  restore.point("fp_find_missing_prod")
  ver_dirs = fp_all_ver_dirs(parent_dir)
  if (is.null(prod_id))
    prod_ids = fp_ver_dir_to_prod_id(ver_dirs) 
  
    
}
```
# END OF FILE: proc_all.R

-----------------------------------------------------------


# FILE: proc_cell.R
```
example = function() {
  
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  proc_all_tab_html_to_cell_list(project_dir, overwrite=TRUE)
  proc_all_cell_list_to_cell_base(project_dir, overwrite=TRUE)
  
  project_dirs = list.dirs("~/repbox/projects_share", full.names = TRUE,recursive = FALSE)
  proc_all_tab_html_to_cell_list(paste0(project_dirs,"fp"))
  proc_all_cell_list_to_cell_base(paste0(project_dirs,"fp"))
  
  ver_dir = "~/repbox/projects_share/aejapp_1_2_4/rai/prod_runs/tab_html/raw_tab_html-n-g2f-0/v0"
  
  parent_dir = "~/repbox/projects_share/aeri_1_2_6"
  proc_raw_tab_html_to_cell_base(ver_dir = ver_dir)
  
  rstudioapi::filesPaneNavigate(project_dir)
  
  
}

proc_all_tab_html_to_cell_list = function(project_dir, overwrite=FALSE) {
  ddp_derive_all_instances(project_dir, from_prod_id = "tab_html", to_prod_id = "cell_list",convert_fun = proc_tab_html_to_cell_list, overwrite=overwrite)
}

proc_all_tab_html_to_cell_list = function(project_dir, overwrite=FALSE) {
  ddp_derive_all_instances(project_dir, from_prod_id = "tab_html", to_prod_id = "cell_list",convert_fun = proc_tab_html_to_cell_list, overwrite=overwrite)
}

proc_all_raw_tab_html_to_cell_base = function(parent_dir) {
  restore.point("proc_all_raw_tab_html_to_cell_base")
  ver_dirs = fp_all_ver_dirs(parent_dir)
  proc_ids = fp_ver_dir_to_proc_id(ver_dirs)
  ver_dirs = ver_dirs[startsWith(proc_ids, "raw_tab_html")]
  for (ver_dir in ver_dirs) {
    proc_raw_tab_html_to_cell_base(ver_dir=ver_dir)
  }
}

proc_raw_tab_html_to_cell_base = function(org_pru=NULL, ver_dir=org_pru$ver_dir, tab_df = org_pru$prod_df) {
  restore.point("proc_raw_tab_html_to_cell_base")
  if (is.null(org_pru)) {
    org_pru = readRDS(file.path(ver_dir, "pru.Rds"))
  }
  prod_id = "cell_base"
  proc_id = str.right.of(org_pru$proc_id,"raw_")
  pru = ddp_init_pru(org_pru, ver_dir=ver_dir,ddp_prod_id=prod_id, ddp_proc_id = proc_id)
  if (is.null(tab_df)) {
    tab_df = fp_load_prod_df(ver_dir=org_pru$ver_dir)
  }
  
  i = 1
  cell_df = bind_rows(lapply(seq_len(NROW(tab_df)), function(i) {
    restore.point("shkfjhksfdo")
    cell_df = normalized_html_tab_to_cell_df(tab_df$tabhtml[[i]], tab_df$tabid[[i]])
    cell_df = cells_add_cell_base(cell_df,split_multi_num = TRUE)
    cell_df      
  }))
  prod = repbox_prod("cell_base")
  prod_df = df_to_prod_df(cell_df, prod)
  pru = pru_save(pru,prod_df)

  pru_backport_save(pru, repbox_prod("cell_list"),prod_df)
  cell_df$content = cell_df$text
  cell_li = split(cell_df, cell_df$tabid)
  tab_df$tabhtml =  sapply(cell_li, cell_df_to_simple_tabhtml)
  tab_pru = pru_backport_save(pru, repbox_prod("tab_html"), tab_df)
  # rstudioapi::filesPaneNavigate(tab_pru$ver_dir)
  rai_write_all_tables_html(tab_df, "tables.html", out_dir=tab_pru$ver_dir, info=tab_pru$proc_info)
  invisible(pru)
  
}


# cell_list is a derived prod of tab_html
proc_tab_html_to_cell_list = function(pru=NULL, ver_dir=pru$ver_dir, prods=repbox_prods(), prod_df=pru$prod_df, also_cell_base=FALSE) {
  restore.point("proc_tab_html_to_cell_list")
  prod = prods[["cell_list"]]
  df = fp_load_prod_df(ver_dir=ver_dir, prod_df=prod_df)

  pru = ddp_init_pru(pru, ver_dir=ver_dir,ddp_prod_id="cell_list")
  i = 1
  cell_df = bind_rows(lapply(seq_len(NROW(df)), function(i) {
    restore.point("shkfjhksfdo")
    cell_df = normalized_html_tab_to_cell_df(df$tabhtml[[i]], tabid=df$tabid[i])
    cell_df = add_col_left(cell_df,otabid = df$otabid[i])
    cell_df      
  }))
  prod_df = df_to_prod_df(cell_df, prod)
  pru = pru_save(pru,prod_df)
  
  if (also_cell_base) {
    pru = proc_cell_list_to_cell_base(pru)
  }
  #rstudioapi::filesPaneNavigate(pru$ver_dir)
  invisible(pru)
  
}


proc_all_cell_list_to_cell_base = function(project_dir, overwrite=FALSE) {
  restore.point("proc_all_cell_list_to_cell_base")
  ddp_derive_all_instances(project_dir, from_prod_id = "cell_list", to_prod_id = "cell_base",convert_fun = proc_cell_list_to_cell_base, overwrite=overwrite)
}


proc_cell_list_to_cell_base = function(pru=NULL, ver_dir=pru$ver_dir, prods=repbox_prods(), prod_df=pru$prod_df) {
  restore.point("proc_cell_list_to_cell_base")
  cell_df = fp_load_prod_df(ver_dir=ver_dir, prod_df=prod_df)
  prod = repbox_prod("cell_base", prods)
  pru = ddp_init_pru(pru, ver_dir=ver_dir,ddp_prod_id="cell_base")
  prod_df = cell_list_to_cell_base_prod(cell_df, prod=prod)
  pru = pru_save(pru,prod_df)
  #rstudioapi::filesPaneNavigate(pru$ver_dir)
  invisible(pru)  
}


cell_list_to_cell_base_prod = function(cell_list, prod=repbox_prods()[["cell_base"]]) {
  restore.point("cell_list_to_cell_base_prod")
  # From repboxTableTools
  cell_df = cells_add_cell_base(cell_list)
  df_to_prod_df(cell_df,  prod)
}
```
# END OF FILE: proc_cell.R

-----------------------------------------------------------


# FILE: proc_info.R
```
# Repbox AI process infos

rai_make_proc_info = function(prod_id, ai_opts,tpl_file=NA,json_mode=TRUE, use_schema=FALSE, raw=FALSE,tpl_id=NULL,doc_file_form=NULL, proc_prefix="",proc_postfix = "",proc_id=NULL,  ...) {
  schema_opt = case_when(
    use_schema ~ "s",
    json_mode ~ "j",
    TRUE ~ "n"
  )
  schema_opt_str = ifelse(schema_opt=="s", "s","")
  temp_str = ifelse(is.true(ai_opts$temperature > 0), paste0("t",round(10*ai_opts$temperature,0)),"")
  model_short = rai_model_short(ai_opts$model)
  if (is.null(proc_id)) {
    proc_id = paste0(proc_prefix,model_short,schema_opt_str, temp_str, proc_postfix)
  }
  if (raw) {
    proc_id = paste0("raw_", proc_id)
  }
  extra_args = list(...)
  restore.point("repbox_ai_version")
  if (length(extra_args)>0) {
    extra_str = do.call(paste, c(extra_args, list(sep="-")))
    proc_id = paste0(proc_id, "-", extra_str)
  }
  proc_info = c(list(proc_id=proc_id, prod_id=prod_id,model=ai_opts$model,model_short = model_short, temperature=ai_opts$temperature, tpl_id=null_to_na(tpl_id), tpl_file=tpl_file, tpl_base = basename(tpl_file), json_mode=json_mode, use_schema=use_schema, raw=raw), doc_file_form=null_to_na(doc_file_form), extra_args)
  proc_info = as_tibble(proc_info)
  proc_info
}

```
# END OF FILE: proc_info.R

-----------------------------------------------------------


# FILE: proc_patches.R
```

example = function(project_dir) {
  library(repboxAI)
  library(repboxRegmap)
  library(repboxReport)
  
  options(warn=1)
  #restore.point.options(display.restore.point = TRUE)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"

  if (FALSE)
    rstudioapi::filesPaneNavigate(project_dir)

  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.5-flash")

  rai_patch_rerun(project_dir, prod_id ="map_reg_run", patch_id = "patch")
  
  rai_apply_patches(project_dir, prod_id ="map_reg_run",doc_type = "art")
  
  # Redo mapping  
  rme = rme_init(project_dir)
  rme = rme_add_eval(rme, rme_steps_all())
  issue_df = rme_combine_ev_df(rme)
  # Create map reports
  opts = rr_map_report_opts(embed_data = FALSE)
  rep_file = rr_map_report(project_dir,opts = opts)
  browseURL(rep_file)

    
}

#' Rerun map_reg_run for specific tables with visual patch media
#' 
#' @param project_dir Project directory
#' @param patch_id Identifier for this patch run (e.g., "visual_fix")
#' @param auto_apply If TRUE, automatically applies the patch to create a revised version
rai_patch_rerun = function(project_dir, prod_id="map_reg_run", patch_id = "patch", doc_type = "art",issue_types = c("multicol_reg_plausibility"), to_v0 = TRUE, overwrite = FALSE, auto_apply = FALSE, ignore_map_versions=NULL, just_map_versions=NULL, ai_opts = get_ai_opts()) {
  restore.point("rai_patch_rerun")
  #stop()
  
  # 1. Load Evaluation Results to find problematic tables
  rme = repboxRegmap::rme_load(project_dir, doc_type)
  if (is.null(rme)) stop("No rme.Rds found. Run evaluation first.")
  
  # Identify tables with specific issues (customize filter as needed)
  # Here we look for issues in specific integrity/structure checks
  issues_df = repboxRegmap::rme_combine_ev_df(rme,eval_steps = issue_types)
  if (NROW(issues_df) == 0) {
    cat("\nNo issues found in evaluation. Nothing to rerun.")
    return(NULL)
  }
  
  if (length(ignore_map_versions)>0) {
    issues_df = issues_df %>% filter(!map_version %in% ignore_map_versions)
  }
  if (length(just_map_versions)>0) {
    issues_df = issues_df %>% filter(map_version %in% just_map_versions)
  }

  if (NROW(issues_df) == 0) {
    cat("\nNo issues found in evaluation. Nothing to rerun.")
    return(NULL)
  }
  
  # Get unique tabids that have issues
  target_tabids = unique(issues_df$tabid)
  cat(paste0("\nFound issues in tables: ", paste(target_tabids, collapse=", "), "\n"))
  
  
  # 2. Setup the Patch PRU
  # We base this on the existing map_reg_run logic but modify the input
  base_ver_id = issues_df$map_version[1] 
  
  # Create a patch process ID
  patch_proc_id = paste0(patch_id,"_", base_ver_id)
  patch_proc_id = stri_replace_all_fixed(patch_proc_id, "--","__")
  
  # Initialize pru
  pru = rai_pru_base(project_dir, prod_id = prod_id, doc_type = doc_type, tpl_id = paste0(patch_id,"_",prod_id),overwrite = overwrite, to_v0 = to_v0,proc_id = patch_proc_id, ai_opts = ai_opts) %>%
    rai_pru_add_doc() %>%
    rai_pru_add_run_do(in_context = FALSE) %>%
    rai_pru_add_tab_df() # Loads all tables initially
  
  # 3. FILTER: Only keep the problematic tables in the PRU
  pru$tab_df = pru$tab_df[pru$tab_df$tabid %in% target_tabids, ]
  
  if (nrow(pru$tab_df) == 0) stop("Target tables not found in tab_df.")
  
  # 4. Configure for Itemized Execution (One AI call per table)
  # This enables specific media attachment per chunk
  pru$itemize_by = "tab_df"
  pru$item_chunk_size = 1
  pru$by_tab_media_fun = "rai_patch_by_tab_media"
  
  # Store rme in pru so that rr_table does not need to reload it
  pru$rme = rme
  pru$patch_id = patch_id
  pru$issue_types = unique(issues_df$test_name)
  # 5. Run AI
  cat(paste0("\nRunning correction patch for ", length(target_tabids), " tables...\n"))
  proc_rai_pru(pru)
  
  restore.point("rai_patch_rerun_post_pru")
  
  
  invisible(pru)
}

rai_apply_patch = function(patch_pru) {
  cat("\nApplying patch to create new version...\n")
  # This uses the generic patch application logic
  # It assumes the patch ver_dir is the one we just created
  project_dir = pru$project_dir
  patch_ver_dir = pru$ver_dir
  
  base_verid = pru$
    
    # We need the base_ver_dir corresponding to base_ver_id
  fp_dir = file.path(project_dir, "fp", paste0("prod_", doc_type))
  base_ver_dir = repboxAI::fp_ver_id_to_ver_dir(fp_dir, prod_id, base_ver_id)
    
    # Define new revision directory
    base_proc_id = repboxAI::fp_ver_dir_to_proc_id(base_ver_dir)
    revised_proc_id = repboxAI::fp_get_revised_proc_id(base_proc_id)
    revised_proc_dir = file.path(fp_dir, prod_id, revised_proc_id)
    revised_ver_dir = repboxAI::fp_proc_dir_to_new_ver_dir(revised_proc_dir)
    
    repboxAI::fp_create_revised_version(
      base_ver_dir = base_ver_dir,
      patch_ver_dir = patch_ver_dir,
      revised_ver_dir = revised_ver_dir,
      key_col = "regid" # Assuming regid is the unique key for regressions
    )

}


rai_patch_by_tab_media = function(pru=NULL, tabid=NULL) {
  restore.point("rai_patch_by_tab_media")
  #stop()
  
  if (pru$prod_id != "map_reg_run") stop(paste0("Not yet implemented for ",pru$prod_id)) 
  
  library(repboxReport)
  project_dir = pru$project_dir
  
  # Ensure prompt_files directory exists (standard location for rai_pru media)
  prompt_files_dir = file.path(project_dir, "fp/prompt_files")
  if (!dir.exists(prompt_files_dir)) dir.create(prompt_files_dir, recursive = TRUE)
  
  # Generate report options (ensure we show the discrepancies)
  rr_opts = rr_map_report_opts(only_tests = pru$issue_types)
  res = rr_single_table(project_dir, rme=pru$rme, tabid = tabid, doc_type = pru$doc_type,  opts = rr_opts, table_png = TRUE, output_dir = file.path(prompt_files_dir))
  res_files = unlist(res[c(5,6)])
  res_files
}


rai_load_rme = function(project_dir, doc_type="art") {
  rme_file = file.path(project_dir, paste0("fp/eval_", doc_type,"/rme.Rds"))
  if (!file.exists(rme_file)) return(NULL)
  rme = readRDS(rme_file)
  rme
}


#' Apply all unapplied patches for a given product.
#'
#' Scans a product directory for all "patch" versions, checks if a corresponding
#' "revised" version already exists, and if not, creates one by applying the patch.
#'
#' @param project_dir The project directory.
#' @param prod_id The product ID to apply patches for (e.g., "map_reg_run").
#' @param doc_type The document type (e.g., "art").
#' @param key_col The key column for group-based replacement (e.g., "regid"). Defalt is tabid.
#' @param ... Additional arguments passed to `fp_create_revised_version_by_group_replace`.
rai_apply_patches = function(project_dir, prod_id, doc_type="art", key_col="tabid", patch_id="patch", ...) {
  restore.point("rai_apply_patches")
  #stop()
  fp_dir = file.path(project_dir, "fp", paste0("prod_", doc_type))
  prod_dir = file.path(fp_dir, prod_id)
  
  if (!dir.exists(prod_dir)) return(invisible())

  # 1. Find all patch process directories
  patch_proc_dirs = list.files(prod_dir, pattern = glob2rx(paste0(patch_id, "_*")), full.names = TRUE)
  patch_proc_dirs = patch_proc_dirs[dir.exists(patch_proc_dirs)]
  if (length(patch_proc_dirs) == 0) {
    cat("\nNo patch directories found for", prod_id, "in", doc_type)
    return(invisible())
  }

  
  patch_proc_dir = patch_proc_dirs[1]
  # 2. Iterate through each patch and apply if necessary
  for (patch_proc_dir in patch_proc_dirs) {
    patch_proc_id = basename(patch_proc_dir)

    # Extract the base proc_id that this patch is for
    base_ver_id = stringi::stri_replace_first_fixed(patch_proc_id, paste0(patch_id,"_"), "")
    
    base_ver_dir = fp_ver_id_to_ver_dir(fp_dir, prod_id, base_ver_id)
    patch_ver_dir = fp_pick_latest_ver_dir(patch_proc_dir)

    base_proc_id = fp_ver_dir_to_proc_id(base_ver_dir)
    # Determine the name of the revised version and check if it exists
    revised_proc_id = fp_get_revised_proc_id(base_proc_id)
    revised_proc_dir = file.path(prod_dir, revised_proc_id)

    if (dir.exists(revised_proc_dir) && fp_ver_dir_ok(fp_pick_latest_ver_dir(revised_proc_dir))) {
      cat("\nRevised version for patch '", patch_proc_id, "' already exists. Skipping.")
      next
    }

    # Apply the patch
    cat("\nApplying patch:", basename(patch_ver_dir), "to base:", basename(base_ver_dir))
    fp_create_revised_version(
      base_ver_dir = base_ver_dir,
      patch_ver_dir = patch_ver_dir,
      revised_ver_dir = fp_proc_dir_to_new_ver_dir(revised_proc_dir),
      key_col = key_col
    )
  }
}

fp_pick_latest_ver_dir = function(proc_dir) {
  dirs=list.dirs(proc_dir,full.names = TRUE,recursive = FALSE)
  fi = file.info(dirs)
  ind = which.max(fi$mtime)
  dirs[ind]
}
```
# END OF FILE: proc_patches.R

-----------------------------------------------------------


# FILE: proc_readme.R
```
# Parse tab_base

example = function() {
  library(repboxAI)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.0-flash-thinking-exp")
  set_ai_opts(model = "gemini-2.0-flash")
  get_ai_opts()
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
  
  res = proc_readme(project_dir,"readme_overview", overwrite=FALSE)
  res = proc_readme(project_dir,"readme_var", overwrite=FALSE)
  res = proc_readme(project_dir,"readme_script_tab_fig", overwrite=FALSE)
  res = proc_readme(project_dir,"readme_data", overwrite=FALSE)
  res = proc_readme(project_dir,"readme_data_descr", overwrite=!FALSE)
  
  rstudioapi::filesPaneNavigate(project_dir)
  rstudioapi::filesPaneNavigate(res$ver_dir)
  
  # Analyse readme overview
  project_dirs = repboxExplore::get_project_dirs("~/repbox/projects_share")
  readme_df = fp_pick_and_load_prod_df(project_dirs,prod_id = "readme_overview")
  df = fp_pick_and_load_prod_df(project_dirs,prod_id = "readme_data")
  df = fp_pick_and_load_prod_df(project_dirs,prod_id = "readme_data_descr")
  
}

#' Extracts tab_html from articles
proc_readme = function(project_dir, prod_id = c("readme_overview","readme_var","readme_tabfig")[1], ai_opts = get_ai_opts(), verbose=TRUE, to_v0=TRUE, use_schema = FALSE, overwrite=TRUE, max_readme_rank=1) {
  restore.point("proc_readme")
  library(repboxReadme)
  readme_df = repbox_load_or_make_readme_ranks(project_dir)
  if (NROW(readme_df)==0) {
    cat("\nNo readme files found for ", project_dir, ".\n")
    return(NULL)
  }
  readme_df = readme_df %>% filter(!ignore) %>% filter(rank <= max_readme_rank)
  fp_dir = file.path(project_dir, "fp", "readme") 
  
  tpl_file = file.path(rai_tpl_dir(), paste0(prod_id,".txt"))
  
  proc_info = rai_make_proc_info(prod_id=prod_id,ai_opts = ai_opts,tpl_file = tpl_file, json_mode=TRUE, use_schema = use_schema)
  
  pru = pru_init(fp_dir,prod_id,proc_info=proc_info,to_v0=to_v0)
  
  pru$max_readme_rank = max_readme_rank
  pru$readme_df = readme_df
  pru$project_dir = project_dir
  if (!overwrite) if (fp_ver_dir_ok(pru$ver_dir)) return(NULL)
  pru_next_stage(pru, "pru_readme_run")
}

pru_readme_run = function(pru) {
  restore.point("pru_readme_run")
  project_dir = pru$project_dir
  df = pru$readme_df
  row = 1
  pru = pru_make_items(pru, df=df, function(row, pru,...) {
    restore.point("pru_readme_overview_run_inner")
    readme_file = file.path(project_dir, "readme", "txt", df$txt_file[row])
    rai = rai_init(project_dir,proc_info = pru$proc_info,media_files = readme_file)
    values = list(readme_file = basename(df$org_file[row]))
    res = ai_run(rai, values=values)
    res
  })
  pru = pru_set_status(pru, pru$items)
  restore.point("pru_readme_run")
  if (!pru_is_ok(pru)) return(invisible(pru))
  
  prod = repbox_prod(pru$prod_id)
  res_df = ai_combine_content_df(pru$items, df %>% select(readme_file = org_file),schema = prod_to_schema(prod, "arr"))
  prod_df = df_to_prod_df(res_df, repbox_prod(pru$prod_id))
  pru_save(pru, prod_df)
  return(invisible(pru))
}
```
# END OF FILE: proc_readme.R

-----------------------------------------------------------


# FILE: proc_tab_from_pdf.R
```
# Parse tab_base

example = function() {
  library(repboxAI)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-1.5-flash-001")
  set_ai_opts(model = "gemini-2.0-flash-thinking-exp")
  set_ai_opts(model = "gemini-2.0-flash-lite")
  set_ai_opts(model = "gemini-2.0-flash")
  get_ai_opts()
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  project_dir = "~/repbox/projects_share/aejapp_1_2_7"
  project_dir = "~/repbox/projects_share/ecta_84_2_6"
  project_dir = "~/repbox/projects_share/jeea_12_1_11"
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
    
  res = proc_tab_html_from_pdf(project_dir, doc_type="app1", overwrite=FALSE)
  
  res = proc_tab_notes_from_pdf(project_dir, overwrite=FALSE)
  
  rstudioapi::filesPaneNavigate(project_dir)
  rstudioapi::filesPaneNavigate(res$ver_dir)
}

proc_tab_html_from_pdf = function(project_dir, doc_type = NULL, overwrite=FALSE, ...) {
  doc_dirs = repbox_doc_dirs(project_dir,doc_form = "pdf", doc_type=doc_type)
  
  
  for (doc_dir in doc_dirs) {
    cat(paste0("\nExtract tables from PDF in ", doc_dir,"\n"))
    proc_doc_tab_html_from_pdf(doc_dir,overwrite=overwrite, ...)
  }
}

#' Extracts tab_html from articles
proc_doc_tab_html_from_pdf = function(doc_dir, tpl_num=1,prods=repbox_prods(), ai_opts = get_ai_opts(), verbose=TRUE, to_v0=TRUE, overwrite=TRUE) {
  restore.point("proc_doc_tab_html_from_pdf")
  if (!rai_has_input(doc_dir, "pdf","tab_list")) return(NULL)
  
  doc_type = rdoc_type(doc_dir)
  fp_dir = doc_dir_to_fp_dir(doc_dir)
  project_dir = rai_fp_dir_to_project_dir(fp_dir)
  
  
  prod_id = "tab_html"
  doc_form = "pdf"
  tpl_file = file.path(rai_tpl_dir(), paste0(prod_id, "-", doc_form, "-", tpl_num, ".txt"))
  
  proc_info = rai_make_proc_info(prod_id=prod_id,ai_opts = ai_opts,tpl_file = tpl_file, json_mode=FALSE, use_schema = FALSE, raw=TRUE, proc_prefix = "pdf-")

  
  
  pru = pru_init(fp_dir,prod_id,proc_info=proc_info,to_v0=to_v0)
  
  if (!overwrite) if (fp_ver_dir_ok(pru$ver_dir)) return(NULL)

  # Prefer following table lists:
  # 1. extracted with same model
  # 2. mocr
  proc_postfix = str.right.of(proc_info$proc_id,"-") %>% str.right.of("-")
  proc_ids = fp_all_proc_id(fp_dir, "tab_list")
  input_pref = fp_pref(proc_id = c(
    proc_ids[endsWith(proc_ids, proc_postfix)],
    proc_ids[endsWith(proc_ids, "_mocr")]  
  ))
  pru = pru_pick_inputs(pru, "tab_list", input_pref)
  if (pru_has_input_err(pru)) return(pru)

  context = rai_context(project_dir,doc_type_pdf = doc_type)
  pru$rai = rai_init(fp_dir, context=context, proc_info=proc_info, ai_opts=ai_opts)
  pru_next_stage(pru, "pru_tab_html_pdf_run")
}

pru_tab_html_pdf_run = function(pru) {
  restore.point("pru_tab_html_pdf_ai_run")
  df = pru_get_input(pru, "tab_list")
  pru = pru_make_items(pru, df=df, function(row, pru,...) {
    values = as.list(df[row,])
    ai_run(pru$rai, values=values)
  })
  pru = pru_set_status(pru, pru$items)
  restore.point("pru_post_run")
  if (!pru_is_ok(pru)) return(invisible(pru))

  df$raw_html = ai_combine_content_str(pru$items,err_val = NA_character_)
  df$tabhtml = sapply(seq_len(NROW(df)), function(i) {
    html_table_add_cellnum_row_col(df$raw_html[i],tabid=df$tabid[i])
  })
  prod_df = df_to_prod_df(df, repbox_prod("tab_html"))
  pru_save(pru, prod_df)
  rai_write_all_tables_html(prod_df, "tables.html", out_dir=pru$ver_dir, info=pru$proc_info)
  
  proc_raw_tab_html_to_cell_base(pru, tab_df = prod_df)
  
  #rstudioapi::filesPaneNavigate(pru$ver_dir)
  
  return(invisible(pru))
  
}

proc_tab_notes_from_pdf = function(project_dir, doc_type=NULL, overwrite=FALSE,...) {

  doc_dirs = repbox_doc_dirs(project_dir,doc_form = "pdf", doc_type=doc_type)
  
  for (doc_dir in doc_dirs) {
    cat(paste0("\nExtract tables from PDF in ", doc_dir,"\n"))
    proc_doc_tab_notes_from_pdf(doc_dir,overwrite=overwrite, ...)
  }
  
}



#' Extracts tab_list and tab_notes from articles
proc_doc_tab_notes_from_pdf = function(doc_dir, tpl_num=1,use_schema=FALSE, to_v0=TRUE, ai_opts = get_ai_opts(), overwrite=TRUE) {
  if (!rai_has_input(doc_dir, "pdf")) return(NULL)
  
  fun_call = preserve_call("proc_doc_tab_notes_from_pdf")
  restore.point("proc_doc_tab_notes_from_pdf")
  
  doc_type = rdoc_type(doc_dir)
  prod_id = "tab_notes"
  art_source = "pdf"
  tpl_file = file.path(rai_tpl_dir(), paste0(prod_id, "-", art_source, "-", tpl_num, ".txt"))

  proc_info = rai_make_proc_info(prod_id=prod_id,ai_opts = ai_opts,tpl_file = tpl_file, json_mode=TRUE, use_schema = use_schema,proc_prefix = "pdf-")
  
  fp_dir = doc_dir_to_fp_dir(doc_dir)
  project_dir = rai_fp_dir_to_project_dir(fp_dir)
  pru = pru_init(fp_dir,prod_id,proc_info=proc_info,to_v0=to_v0, fun_call=fun_call)
  
  if (!overwrite) if (fp_ver_dir_ok(pru$ver_dir)) return(NULL)
  
  # In the prompt we name variables differently to make it easier for AI
  schema = NULL
  pru$prod_to_df_cols = c(table_number="tabid",table_title="tabtitle", table_notes="tabnotes")
  if (use_schema) {
    schema = prod_to_schema(prod, "arr") %>%
      schema_reduce(pru$prod_to_df_cols)
  }
  context = rai_context(project_dir,doc_type_pdf = doc_type)
  pru$rai = rai_init(project_dir, proc_info = proc_info, schema=schema, context=context)
  
  pru$rai = rai_run(pru$rai)
  restore.point("proc_doc_tab_notes_from_pdf_post_run")
  
  
  pru = pru_set_status(pru, pru$rai)
  if (!pru_is_ok(pru)) return(invisible(pru))
  prod = repbox_prod(prod_id)
  prod_df = df_to_prod_df(pru$rai$content, prod, prod_to_df_cols = pru$prod_to_df_cols)
  # Deal with continued tables
  prod_df = prod_df %>%
    mutate(
      merge_above = 
      is.true(lag(tabid)==tabid) |
      (is.true(has.substr(tabtitle, "ontinue") & is.na(tabid)))
    )
  rows = which(prod_df$merge_above)
  for (r in rows) {
    if (r==1) next
    prod_df$tabnotes[r-1] = paste0(prod_df$tabnotes[r-1],"\n",prod_df$tabnotes[r])
  }    
  prod_df = prod_df[!is.na(prod_df$tabid)&!prod_df$merge_above,]
  prod_df = prod_df[, setdiff(colnames(prod_df),"merge_above")]
  
  old_tabid = prod_df$tabid
  prod_df$tabid = tabid_normalize(prod_df$tabid)
  if (!all(old_tabid==prod_df$tabid)) {
    pru = pru_add_issue(pru, "tabid_was_standardized")
  }
  
  prod_df$otabid = tabid_to_otabid(prod_df$tabid)
  pru = pru_save(pru, prod_df)
  
  pru_backport_save(pru, repbox_prod("tab_list"), prod_df=prod_df)
  rstudioapi::filesPaneNavigate(pru$ver_dir)
  return(invisible(pru))
}


```
# END OF FILE: proc_tab_from_pdf.R

-----------------------------------------------------------


# FILE: proc_tab_given.R
```
# Use tables already (heuristically) extracted by repboxDoc and stored
# in corresponding table_df.Rds in doc_dir

example = function() {
  library(repboxAI)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  project_dir = "~/repbox/projects_share/aejapp_1_2_7"
  project_dir = "~/repbox/projects_share/ecta_84_2_6"
  project_dir = "~/repbox/projects_share/jeea_12_1_11"
  project_dir = "~/repbox/projects_share/jep_31_1_2"
  project_dir = "~/repbox/projects_share/jole_33_3_5"
  project_dir = "~/repbox/projects_share/jpe_123_4_4"
  project_dir = "~/repbox/projects_share/ms_65_10_17"
  project_dir = "~/repbox/projects_share/qje_3036349" # hx wrong col align
  project_dir = "~/repbox/projects_share/restat_2689366" 
  project_dir = "~/repbox/projects_share/restud_82_2_12" 
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
  
  project_dir = "~/repbox/projects_share/restud_82_2_12" 
  proc_tab_given(project_dir)
  rstudioapi::filesPaneNavigate(project_dir)
}

proc_tab_given = function(project_dir, to_v0=TRUE, doc_form=NULL, doc_type=NULL) {
  restore.point("proc_tab_given")
  doc_dirs = repboxDoc::repbox_doc_dirs(project_dir,doc_form = doc_form, doc_type=doc_type)
  doc_dir = first(doc_dirs)
  for (doc_dir in doc_dirs) {
    proc_doc_tab_given(doc_dir, to_v0=to_v0)
  }
}

proc_doc_tab_given = function(doc_dir, to_v0=TRUE) {
  restore.point("proc_doc_tab_given")
  prod_id = "tab_main"; prod = repbox_prod(prod_id)
  doc_form = rdoc_form(doc_dir)
  
  # Creates page_df.Rds, parts_df.Rds, tabs_df.Rds etc
  # if already exists skips
  rdoc_process(doc_dir,overwrite=FALSE)
  
  rdoc_is_processed(doc_dir)
  
  
  df = readRDS.or.null(file.path(doc_dir, "tab_df.Rds"))

  if (NROW(df)==0) return(NULL)
  
  just_raw=FALSE
  proc_id = doc_form
  if (doc_form=="html") {
    # Changes content of all cells to pure text
    df$raw_tabhtml = df$tabsource
    df$raw_tabhtml = sapply(df$raw_tabhtml, html_table_cells_to_text)
    just_raw = TRUE
    
  } else if (doc_form=="pdf") {
    df$raw_tabhtml = df$tab_html
    just_raw = TRUE
    proc_id = "pdf_txt"
  }
  if (just_raw) {
    df$raw_tabhtml <- stri_replace_all_fixed(df$raw_tabhtml, "\u2003", "")
    df$tabhtml = sapply(seq_len(NROW(df)), function(i) {
      html_table_add_cellnum_row_col(df$raw_tabhtml[i],tabid=df$tabid[i])
    })
  }
  #proc_id = paste0("doc_", doc_form)
  proc_info = data.frame(prod_id=prod_id, proc_id = proc_id, doc_form=doc_form)
  fp_dir = doc_dir_to_fp_dir(doc_dir)
  pru = pru_init(fp_dir,prod_id=prod_id, proc_info=proc_info, to_v0=to_v0)
  prod_df = df_to_prod_df(df, prod)
  old_tabid = prod_df$tabid
  prod_df$tabid = tabid_normalize(prod_df$tabid)
  if (!all(old_tabid==prod_df$tabid)) {
    pru = pru_add_issue(pru,"tabid_was_standardized")
  }
  
  prod_df$otabid = tabid_to_otabid(prod_df$tabid)
  
  # Don't save main: just save all backports...
  #pru = pru_save(pru, prod_df)
  pru_backport_save(pru, repbox_prod("tab_list"), prod_df)
  if (doc_form != "mocr") {
    pru_backport_save(pru, repbox_prod("tab_notes"), prod_df)
  }
  pru_backport_save(pru, repbox_prod("tab_html"), prod_df)
  proc_tab_html_to_cell_list(pru=pru, prod_df=prod_df, also_cell_base = TRUE)
  #rai_write_all_tables_html(prod_df, "tables.html",out_dir = pru$ver_dir)
  #rstudioapi::filesPaneNavigate(pru$ver_dir)
  invisible(pru)
}
```
# END OF FILE: proc_tab_given.R

-----------------------------------------------------------


# FILE: proc_tab_main.R
```
# tab_main combines tab_html and tab_notes
# not all our extraction methods generate tab_html and tab_notes
# in particular we have not yet included table notes extraction
# for mocr (mistral ocr)

# tab_main essentially just joins a tab_html and tab_notes
# per default we take notes from the same process than tab_html
# if it exists, otherwise we map the notes from our preferred AI 
# notes extraction


example = function() {
  library(repboxAI)
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  proc_tab_main(project_dir)
  rstudioapi::filesPaneNavigate(project_dir)
}

proc_tab_main = function(project_dir, ver_ind = 0, overwrite=TRUE) {
  restore.point("proc_tab_main")
  fp_dirs = rai_fp_dirs(project_dir)
  ver_dirs = fp_all_ver_dirs(fp_dirs, "tab_html", ver_ind=ver_ind)
  
  html_df = tibble(html_ver_dir = ver_dirs, proc_id = fp_ver_dir_to_proc_id(ver_dirs), fp_dir = fp_ver_dir_to_fp_dir(ver_dirs))

  ver_dirs = fp_all_ver_dirs(fp_dirs, "tab_notes", ver_ind=ver_ind)
  notes_df = tibble(notes_ver_dir = ver_dirs, proc_id = fp_ver_dir_to_proc_id(ver_dirs),  fp_dir = fp_ver_dir_to_fp_dir(ver_dirs))
  
  # 1. Create tab main for processes which are expected to use their own note extraction
  sel_df = html_df %>% 
    filter(startsWith(proc_id, "pdf-") | proc_id %in% c("html","pdf_txt")) %>%
    left_join(notes_df, by=c("fp_dir","proc_id"))
  for (i in seq_len(NROW(sel_df))) {
    proc_one_tab_main(sel_df$fp_dir[i],sel_df$html_ver_dir[i], sel_df$notes_ver_dir[i], overwrite=overwrite)
  }
  
  # 2. Create tab main for processes that use default tab notes
  sel_df = html_df %>% 
    filter(proc_id == "mocr")
  i = 1
  for (i in seq_len(NROW(sel_df))) {
    proc_one_tab_main(sel_df$fp_dir[i],sel_df$html_ver_dir[i], overwrite=overwrite)
  }

}

proc_one_tab_main = function(fp_dir, ver_dir_html=NULL, ver_dir_notes = NULL, to_v0=TRUE, overwrite=FALSE) {
  restore.point("proc_one_main_tab")
  add_notes_id=TRUE
  if (is.null(ver_dir_notes)) {
    add_notes_id = FALSE
    ver_dir_notes = rai_pick_tab_ver(fp_dir, "tab_notes")$ver_dir
  }
  if (is.null(ver_dir_html)) {
    ver_dir_html = rai_pick_tab_ver(fp_dir, "tab_html")$ver_dir
  }
  proc_id_notes = fp_ver_dir_to_proc_id(ver_dir_notes)
  proc_id_html = fp_ver_dir_to_proc_id(ver_dir_html)
  short_html = proc_id_html %>%
    stri_replace_all_regex("(tab_html_doc_)|(cell_base-n-)|(_tab_html-n-)","")
  short_notes = proc_id_notes %>%
    stri_replace_all_regex("(tab_html_doc_)|(cell_base)|(tab_notes-j-)","")

  proc_id = ifelse(short_html==short_notes | !add_notes_id, short_html, paste0(short_html, "-", short_notes))
  
  fp_dir = fp_ver_dir_to_fp_dir(ver_dir_html)
  proc_dir = file.path(fp_dir, "tab_main",proc_id)
  ver_dir = fp_proc_dir_to_new_ver_dir(proc_dir,to_v0 = to_v0)
  if (!overwrite) {
    if (fp_has_prod_df(ver_dir)) {
      return(invisible())
    }
  }
  
  
  tab_html = fp_load_prod_df(ver_dir_html)
  tab_notes = fp_load_prod_df(ver_dir_notes)
  df = left_join_overwrite(tab_html, tab_notes, by ="tabid")
  prod_df = df_to_prod_df(df, repbox_prod("tab_main"))
  fp_save_prod_df(prod_df, ver_dir)
  rai_write_all_tables_html(prod_df, "tables.html",out_dir = ver_dir)
  
  invisible(list(ver_dir=ver_dir, prod_df=prod_df))
}

```
# END OF FILE: proc_tab_main.R

-----------------------------------------------------------


# FILE: rai_internal_debug.R
```
# Correct all prod_df classes

example = function() {
  parent_dir = "~/repbox/projects_share"
  prod_id = proc_id = NULL
  rai_correct_prod_df_classes(parent_dir)
}

rai_correct_prod_df_classes = function(parent_dir, prod_id=NULL, proc_id=NULL) {
  restore.point("rai_correct_prod_df_classes")
  ver_dirs = fp_all_ver_dirs(parent_dir, prod_id=prod_id, proc_id = proc_id)
  #ver_dir = ver_dirs[1]
  prods = repbox_prods()
  for (ver_dir in ver_dirs) {
    prod_file = file.path(ver_dir, "prod_df.Rds")
    if (!file.exists(prod_file)) next
    pid = fp_ver_dir_to_prod_id(ver_dir)
    prod = prods[[pid]]
    if (is.null(prod)) next
    prod_df = readRDS(prod_file)
    new_prod_df = prod_set_df_col_class(prod_df, prod)
    if (!identical(prod_df, new_prod_df)) {
      cat("\nCorrected classes for ", ver_dir)
      saveRDS(new_prod_df, prod_file)
    }
  }
}
```
# END OF FILE: rai_internal_debug.R

-----------------------------------------------------------


# FILE: rai_media.R
```
# Generate typical media and context used in rebox analyses
example = function() {
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  outfile = rai_media_all_static_do(project_dir)
  tab_main = rai_pick_tab_main(project_dir,"art")$tab_main
  outfile = rai_media_all_tab_html(project_dir, tab_main)
  rstudioapi::filesPaneNavigate(dirname(outfile))
}

rai_media_all_static_do = function(project_dir, script_df = rai_load_do_source(project_dir)) {
  restore.point("rai_make_all_static_do")
  if (!has_col(script_df,"script_file")) {
    script_df = script_df_shorten_file(script_df)
  }
  
  txt = paste0(
    paste(rep("*", 60), collapse=""),"\n",
    script_df$script_file,"\n",
    paste(rep("*", 60), collapse=""),"\n\n",
    "Below is the code of ", script_df$file_path, " with line numbers\n\n",
    add_line_numbers_to_code(script_df$text),
    "\n"
  )
  outfile = rai_save_prompt_media_file(txt, project_dir, base="static_do.txt")
  outfile
  
}

add_line_numbers_to_code = function(code, pad_len) {
  if (length(code)>1) {
    return(sapply(code, add_line_numbers_to_code))
  }
  code = sep.lines(code)
  pad_len = max(2, ceiling(log(length(code)+1,10)))
  paste0(
    stri_pad_right(seq_along(code),pad_len),
    ": ",
    code,
    collapse="\n"
  )
}

rai_media_all_tab_html = function(project_dir, tab_main, doc_type="art", base=NULL) {
  restore.point("rai_make_all_tab_html")
  fp_dir = rai_fp_dir(project_dir, doc_type)
  
  tab_df = tab_main
  title_col = "tabtitle"; notes_col = "tabnots"
  tabtitles =   
  html = paste0(paste0("<h2>",tab_df$tabtitle, "</h2>", tab_df$tabhtml, "<p>", na_val(tab_df$tabnotes,""),"</p>"))
  
  html = paste0(html, collapse="\n")
  
  if (is.null(base)) {
    base = paste0(doc_type,"_tabs.html")
  }
  outfile = rai_save_prompt_media_file(html, project_dir, base)
  outfile
}

rai_media_tab_html = function(project_dir, tabid, tab_main, all_ref_li=NULL, all_part_df = NULL, outfile=NULL, doc_type="art") {
  restore.point("rai_make_tab_prompt_html")
  fp_dir = rai_fp_dir(project_dir, doc_type)
  
  tab_main = tab_df = tab_main[tab_main$tabid == tabid,]
  title_col = "tabtitle"; notes_col = "tabnots"
  tabtitles =   
    tab_html = paste0(paste0("<h2>",tab_df$tabtitle, "</h2>", tab_df$tabhtml, "<p>", na_val(tab_df$tabnotes,""),"</p>"))
  
  ref_txt = NULL
  if (!is.null(all_ref_li)) {
    ref_txt = sapply(seq_along(all_ref_li), function(i) {
      paste0(repboxDoc::rdoc_tab_ref_text(tabid = tabid, ref_li = all_ref_li[[i]],part_df = all_part_df[[i]],sep_str = "<p>[...]</p>")$text, collapse="\n")
    })
    ref_txt = ref_txt[nchar(ref_txt)>0]
    ref_txt = paste0(ref_txt, collapse = "<p>[...]</p>")
  }
  if (length(ref_txt)>0) {
    ref_html = paste0("<p>", ref_txt, "</p>")
    html = paste0(tab_html, "\n<h2>Parts in the text that reference to the table </h2>", ref_html)
  } else {
    html = tab_html
  }
  html = paste0(html, collapse="\n")
  
  if (!is.null(outfile)) {
    outdir = dirname(outfile)
    if (!dir.exists(outdir)) dir.create(outdir,recursive = TRUE)
    writeUtf8(html, outfile)
    
  }
  invisible(html)
}


rai_save_prompt_media_file = function(txt, project_dir, base, ind=NULL) {
  restore.point("rai_save_prompt_media_file")
  dir = file.path(project_dir,"fp","prompt_files")
  if (!dir.exists(dir)) dir.create(dir)
  if (is.null(ind)) {
    outfile = paste0(dir, "/", base)
  } else {
    outfile = paste0(dir, "/", tools::file_path_sans_ext(base),"--", ind, tools::file_ext(base))
  }
  writeUtf8(paste0(txt, collapse="\n"), outfile)
  outfile
}
```
# END OF FILE: rai_media.R

-----------------------------------------------------------


# FILE: rai_pick.R
```

doc_file_form_default_pref = function() {
  c("html","pdf","mocr_md", "pdf_txt")
}

tab_df_default_pref = function(prod_id = "tab_main") {
  glob2rx(c("html","pdf-g*","mocr","pdf_txt"))
}


map_reg_static_default_pref = function() {
  glob2rx(c("gp25*mocr*","gp25*"))
}


tab_ref_default_pref = function() {
  c("html", "mocr","pdf")
}


rai_pick_tab_df = function(project_dir, prod_id = "tab_main", doc_type, pref =  tab_df_default_pref(prod_id), pru = NULL) {
  if (is.null(pru)) pru = list()
  
  fp_dir = file.path(project_dir, "fp", paste0("prod_",doc_type))
  pru$tab_df_info = fp_pick_prod_ver(fp_dir, prod_id, pref=pref)
  pru$tab_df = fp_load_prod_df(pru$tab_df_info$ver_dir)
  pru
}
```
# END OF FILE: rai_pick.R

-----------------------------------------------------------


# FILE: rai_prompt.R
```
# Generate typical media and context used in rebox analyses
example = function() {
  library(repboxAI)
  library(restorepoint)
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  outfile = rai_media_all_static_do(project_dir)
  outfile = rai_media_all_tab_html(project_dir)
  rstudioapi::filesPaneNavigate(dirname(outfile))
  
  ver_dir = "~/repbox/projects_share/aeri_1_2_6/fp/prod_art/map_reg_static/g25pe-pdf-g2f/v0"
  map_df = fp_load_prod_df(ver_dir)
  
}

merge_unique_comma_str = function(str) {
  paste0(unique(unlist(stri_split_fixed(str, ",") 
  )), collapse=",")
}

rai_prompt_value_reg_list_static = function(map_df, values=list()) {
  # Transform map_df to reg_df
  map_df = rename.cols(map_df, "do_file","script_file")
  reg_df = map_df %>%
    group_by(tabid, regid) %>%
    summarize(
      cell_ids = merge_unique_comma_str(cell_ids),
      script_files = merge_unique_comma_str(script_file),
      code_lines = merge_unique_comma_str(code_line)
    ) %>%
    ungroup()
  values$reg_list_static = text_table(reg_df)
  values
}


rai_prompt_value_reg_run_list = function(reg_df, values=list()) {
  values$reg_run_list = text_table(reg_df[c("runid")])
  values
}


rai_prompt_value_tab_list = function(tab_df, values = list(), header = c("tabid", "tab_title")) {
  values$tab_list = text_table(tab_df[c("tabid", "tabtitle")], header=header)
  values
}

rai_prompt_value_script_list = function(script_df, values = list(), header = c("script_file")) {
  restore.point("rai_prompt_value_script_list")
  values$script_list = text_table(script_df[c("script_file")], header=header)
  values
}
```
# END OF FILE: rai_prompt.R

-----------------------------------------------------------


# FILE: rai_pru.R
```
# Create an rai_pru object that can be used for an ai-based analysis
example = function() {
  library(repboxAI)
  rgemini::set_gemini_api_key(file = "~/repbox/gemini/gemini_api_key.txt")
  set_ai_opts(model = "gemini-2.0-flash")
  set_ai_opts(model = "gemini-2.5-pro-exp-03-25")
  
  project_dir = "~/repbox/projects_share/aeri_1_2_6"
  pru = 
    rai_pru_base(project_dir, "reg_classify", doc_type="art", overwrite=TRUE) %>%
    rai_pru_add_doc() %>%
    rai_pru_add_tab_df() %>%
    rai_pru_add_reg_list_static() %>%
    rai_pru_add_tab_media(in_context=FALSE)

  proc_rai_pru(pru)
  
  rstudioapi::filesPaneNavigate(project_dir)
}  


rai_pru_base = function(project_dir, prod_id, doc_type="art", ai_opts = get_ai_opts(), verbose=TRUE, to_v0=TRUE, tpl_id = paste0(prod_id), json_mode=TRUE, use_schema = FALSE, overwrite=FALSE, tpl_dir = rai_tpl_dir(), tpl_file = NULL, proc_prefix = "", proc_postfix="", proc_id=NULL, parcels=list()) {
  fp_dir = file.path(project_dir, "fp", paste0("prod_",doc_type))
  prefix = postfix = ""
  pru = copy_into_list()
  restore.point("rai_pru_base")
  
  pru$doc_types = repbox_doc_types(project_dir)
  pru$media_files = NULL
  pru$context_media_files = NULL
  return(pru)
}  

rai_pru_add_doc = function(pru, add_all_doc=TRUE, doc_file_form_pref = doc_file_form_default_pref(), in_context=TRUE) {
  if (is.null(pru)) return(pru)
  pru = copy_into_list(dest=pru, exclude = "pru")
  restore.point("rai_pru_add_doc")
  
  pru$doc_files = rai_doc_file(pru$project_dir,doc_type = NULL, doc_file_form_pref)
  if (length(pru$doc_files)==0) {
    cat("\nNo doc_files found.")
    return(NULL)
  }
  
  pru$doc_file = rai_doc_file(pru$project_dir, pru$doc_type, doc_file_form_pref)
  
  if (add_all_doc) {
    media_files = pru$doc_files
  } else {
    media_files = pru$doc_file
  }
  rai_pru_add_media(pru, media_files, in_context)
}

# rai_pru_adapt_proc_id = function(pru, prefix="", postfix="", new_proc_id=NULL) {
#   if (is.null(pru)) return(pru)
#   restore.point("rai_pru_adapt_proc_id")
#   if (is.null(new_proc_id)) {
#     new_proc_id = paste0(prefix, pru$proc_id, postfix)
#   }
#   
#   new_proc_dir = file.path(dirname(pru$proc_dir),new_proc_id)
#   new_ver_dir = file.path(new_proc_dir, "v0")
#   new_ver_id = fp_ver_dir_to_ids(new_ver_dir)$ver_id
#   pru$proc_info$proc_id = new_proc_id
#   pru$proc_id = new_proc_id
#   pru$ver_dir = new_ver_dir
#   pru$ver_id = new_ver_id
#   pru
# }

rai_pru_add_tab_df = function(pru, tab_prod_id = "tab_main", tab_df_pref = tab_df_default_pref(tab_prod_id), by_tab = FALSE, tab_chunk_size = if (by_tab) 1 else NULL, tab_df_id = tab_prod_id, just_tabid = NULL) {
  if (is.null(pru)) return(pru)
  pru = copy_into_list(dest=pru, exclude = "pru")
  restore.point("rai_pru_add_tab_df")
  
  pru$tab_df_info = fp_pick_prod_ver(pru$fp_dir, tab_prod_id, pref=tab_df_pref)
  if (NROW(pru$tab_df_info)==0 | fp_is_err_obj(pru$tab_df_info)) {
    cat(paste0("\nPlease first successfully create ", tab_prod_id,".\n"))
    return(NULL)
  }
  
  
  pru$tab_df = fp_load_prod_df(pru$tab_df_info$ver_dir)
  
  if (!is.null(just_tabid)) {
    pru$tab_df = pru$tab_df[pru$tab_df$tabid %in% just_tabid,]
  }
  
  postfix=paste0("-", pru$tab_df_info$proc_id)
  if (isTRUE(tab_chunk_size < NROW(pru$tab_df)) & !by_tab) {
    postfix = paste0(postfix, "_c",tab_chunk_size)     
  }
  pru$postfix = paste0(postfix, pru$postfix)
  
  if (!is.null(tab_chunk_size) & is.null(pru$itemize_by)) {
    pru$itemize_by = "tab_df"
    pru$item_chunk_size = tab_chunk_size
  }
  
  pru
}

rai_pru_add_tab_media = function(pru, tab_df = pru$tab_df, by_tab = FALSE, add_ref = FALSE, in_context=FALSE, tab_ref_pref = tab_ref_default_pref()) {
  if (is.null(pru)) return(pru)
  restore.point("rai_pru_add_tab_media")
  # most overwrite every time: not a unique name
  base = paste0(pru$doc_type, "_tabs.html")
  
  if (add_ref & !by_tab) stop("Currently table references can only be added (add_ref=TRUE) if by_tab=TRUE")
  
  if (!by_tab) {
    outfile = rai_media_all_tab_html(pru$project_dir,tab_df,doc_type=pru$doc_type, base)
    pru = rai_pru_add_media(pru, outfile, in_context)
  } else {
    if (!isTRUE(pru$item_chunk_size==1)) {
      stop("by_tab=TRUE only works if yo have set by_tab=TRUE in rai_pru_add_tab_df.")
    }
    pru$add_by_tab_media = TRUE
    pru$tab_media_add_ref = add_ref
    
    if (add_ref) {
      pru$ref_li_doc_dirs = sapply(pru$doc_types, function(dt) {
        rai_pick_ref_li_doc_dir(project_dir,doc_type=dt,tab_ref_pref)
      })
      if (length(pru$ref_li_doc_dir)>0) {
        pru$ref_li_form = rdoc_form(pru$ref_li_doc_dirs[[1]])
        pru$all_ref_li = lapply(pru$ref_li_doc_dirs, rdoc_load_ref_li)
        pru$all_part_df = lapply(pru$ref_li_doc_dirs,rdoc_load_part_df)
      }
      
    }
    
    
    
  }
  pru
}

rai_pru_add_reg_list_static = function(pru, map_reg_static_pref = map_reg_static_default_pref(), filter_tab_df = TRUE) {
  if (is.null(pru)) return(pru)
  pru = copy_into_list(dest=pru, exclude = "pru")
  restore.point("rai_pru_add_reg_list_static")
  
  pru$map_ver_info = fp_pick_prod_ver(pru$fp_dir,"map_reg_static", pref = map_reg_static_pref)
  
  if (NROW(pru$map_ver_info)==0) {
    cat("\nPlease first successfully create map_reg_static.\n")
    return(NULL)
  }
  
  pru$map_df = fp_load_prod_df(pru$map_ver_info$ver_dir)
  if (filter_tab_df) {
    if (is.null(pru$tab_df)) stop("Please make sure to first call rai_pru_add_tab_df.")
    pru$tab_df = pru$tab_df[pru$tab_df$tabid %in% pru$map_df$tabid,]
  }
  pru$postfix = paste0("-", str.left.of(pru$map_ver_info$proc_id,"-"),pru$postfix)
  pru
}

rai_pru_add_media = function(pru, media_files, in_context = TRUE) {
  if (is.null(pru)) return(pru)
  if (in_context) {
    pru$context_media_files = unique(c(pru$context_media_file, media_files))
  } else {
    pru$media_files = unique(c(pru$media_file, media_files))
  }
  pru
}

rai_pru_load_parcels = function(pru, parcel_names, parcels=pru[["parcels"]]) {
  if (is.null(parcels)) parcels = list()
  pru$parcels = repdb_load_parcels(pru$project_dir, parcel_names, parcels)
  pru
}

rai_pru_add_static_do = function(pru, in_context=FALSE) {
  restore.point("rai_pru_add_static_do")
  pru = rai_pru_load_parcels(pru, "stata_source")
  pru$do_df = rai_load_do_source(project_dir,pru$parcels) 
  outfile = rai_media_all_static_do(project_dir,script_df = pru$do_df)
  pru = rai_pru_add_media(pru,outfile, in_context=in_context)
  pru
}

rai_pru_add_reg_df = function(pru, parcels = pru$parcels) {
  restore.point("rai_pru_add_reg_run_df")
  pru = rai_pru_load_parcels(pru, "reg_core")
  pru$reg_df = pru$parcels$reg_core$reg
  pru
}

rai_pru_add_run_do = function(pru, in_context=FALSE, only_reg_df_output = FALSE) {
  restore.point("rai_pru_add_run_do")
  pru = rai_pru_load_parcels(pru, "stata_source")
  pru$do_df = rai_load_do_source(project_dir,pru$parcels)
  output_just_runid = NULL
  if (only_reg_df_output) {
    output_just_runid = pru$reg_df$runid
  }
  
  outfile = rai_media_run_do(project_dir,parcels=pru$parcels, output_just_runid = output_just_runid)
  pru = rai_pru_add_media(pru,outfile, in_context=in_context)
  pru
}



rai_pru_set_tpl = function(pru, tpl_id=pru$tpl_id, tpl_dir=pru$tpl_dir, tpl_file = pru[["tpl_file"]], tpl=pru[["tpl"]]) {
  if (is.null(pru)) return(pru)
  pru = copy_into_list(dest=pru, exclude = "pru")
  restore.point("rai_pru_set_tpl")
  
  if (is.null(tpl_file) & is.null(tpl)) {
    tpl_file = file.path(tpl_dir, paste0(tpl_id,".txt"))
  }
  pru$tpl_file = tpl_file
  if (is.null(tpl)) {
    tpl = paste0(readLines(tpl_file), collapse="\n")
  }
  pru$tpl = tpl
  pru$tpl_var = ai_tpl_vars(pru$tpl)
  pru
}

proc_rai_pru = function(pru) {
  restore.point("proc_rai_pru")
  if (is.null(pru)) return(NULL)
  
  if (is.null(pru[["tpl"]]))
    pru = rai_pru_set_tpl(pru)
  
  prefix = paste0(pru$proc_prefix, pru$prefix)
  postfix = paste0(pru$postfix, pru$proc_postfix)
  
  pru$proc_info = rai_make_proc_info(prod_id=pru$prod_id,ai_opts = pru$ai_opts,tpl_file = pru$tpl_file, json_mode=pru$json_mode, use_schema = pru$use_schema, tpl_id=pru$tpl_id,proc_prefix = prefix, proc_postfix = postfix, proc_id = pru[["proc_id"]])
  
  pru$proc_id  = pru$proc_info$proc_id
  pru = pru_init_dirs(pru=pru)
  
  # We check overwrite a bit late, but I don't know a better approach
  # since ver_dir may depend on the different steps
  # when building pru
  if (!pru$overwrite) if (fp_ver_dir_ok(pru$ver_dir)) return(NULL)

  
  # Some values for prompt can be set in advance
  values = list()
  if ("doc_type_descr" %in% pru$tpl_var) {
    if (length(pru$doc_types)<=1) {
      values$doc_type_descr = "scientific article"
    } else if (length(pru$doc_types)==2) {
      values$doc_type_descr = "scientific article and an online appendix"
    } else {
      values$doc_type_descr = paste0("scientific article and ", length(pru$doc_types)-1," online appendices")
    }
  }
  
  if ("cur_doc" %in% pru$tpl_var) {
    if (pru$doc_type=="art") {
      values$cur_doc = "the article"
    } else if (length(pru$doc_types)<=2) {
      values$cur_doc = "the online appendix"
    }  else {
      values$cur_doc = "an online appendix"
    }
  }
  if ("script_list" %in% pru$tpl_var) {
    values = rai_prompt_value_script_list(pru$do_df, values)
  }
  if (!is.null(pru[["values"]])) {
    values[[names(pru$values)]] = pru$values
  }
  
  pru$values = values
  
  pru_next_stage(pru, "proc_rai_pru_run")
}

proc_rai_pru_run = function(pru) {
  restore.point("proc_rai_pru_run")
  
  if (isTRUE(pru$verbose)) {
    cat("\nRun ", pru$ver_dir,"\n")
  }
  
  project_dir = pru$project_dir
  
  if (length(pru$content_media_files)>0) {
    pru$context = rai_context(pru$project_dir,model = pru$ai_opts$model,media_files = pru$context_media_files)
  }
  
  values = pru$values
  if ("script_list" %in% pru$tpl_var) {
    values = rai_prompt_value_script_list(pru$do_df, values)
  }
  
  if ("tab_list" %in% pru$tpl_var & !isTRUE(pru$itemize_by=="tab_df")) {
    values = rai_prompt_value_tab_list(pru$tab_df, values)
  }
  if ("reg_run_list" %in% pru$tpl_var) {
    values = rai_prompt_value_reg_run_list(pru[["reg_df"]], values)
  }
  if ("reg_list_static" %in% pru$tpl_var) {
    values = rai_prompt_value_reg_list_static(pru$map_df, values)
  }
  
  rai = rai_init(pru$project_dir,proc_info = pru$proc_info,media_files = pru[["media_files"]],context = pru[["context"]])
  
  
  if (!is.null(pru$itemize_by)) {
    org_media_files = pru$media_files
    item_df = pru[[pru$itemize_by]]
    inds = seq_len(NROW(item_df))
    chunks  = split(inds, ceiling(seq_along(inds) / pru$item_chunk_size))
    num_items = length(chunks)
    
    row = 1
    pru = pru_make_items(pru, num_items = num_items, function(row, pru,...) {
      restore.point("proc_rai_pru_run_item_fun")
      tab_df = pru$tab_df[chunks[[row]],]
      media_files = org_media_files

      if ("tabtitle" %in% pru$tpl_var & isTRUE(pru$itemize_by=="tab_df")) {
        values$tabtitle = tab_df$tabtitle
      }
      
      if ("tabid" %in% pru$tpl_var & isTRUE(pru$itemize_by=="tab_df")) {
        values$tabid = tab_df$tabid
      }

      if ("tab_list" %in% pru$tpl_var & isTRUE(pru$itemize_by=="tab_df")) {
        values = rai_prompt_value_tab_list(tab_df, values)
      }
      
      if (!is.null(pru$by_tab_media_fun)) {
        by_tab_media_files = do.call(pru$by_tab_media_fun, list(pru=pru, tabid = tab_df$tabid))
        media_files = c(media_files, by_tab_media_files)
        rai$media_files = media_files

      }
      
      if (isTRUE(pru$add_by_tab_media)) {
        tabid = tab_df$tabid
        outfile = file.path(pru$project_dir,"fp","prompt_files",paste0("tab--",tabid,".html"))
        html = rai_media_tab_html(pru$project_dir,tabid = tabid, tab_main = tab_df, all_ref_li=pru[["all_ref_li"]], all_part_df = pru[["all_part_df"]], outfile=outfile)
        if (file.exists(outfile)) {
          media_files = c(media_files, outfile)
        }
        rai$media_files = media_files
      }
      
      res = ai_run(rai, values=values)
    })
  } else {
    res = ai_run(rai, values=values)
    pru$items = list(res)
  }
  pru = pru_set_status(pru, pru$items)
  
  restore.point("proc_rai_run_post")
  temp_pru_save(pru); #pru = temp_pru
  if (!pru_is_ok(pru)) return(invisible(pru))
  prod = repbox_prod(pru$prod_id)
  # Check: obj or arr? -> Need obj
  schema = prod_to_schema(prod, "obj")
  
  res_df = ai_combine_content_df(pru$items,schema=schema)
  prod_df = df_to_prod_df(res_df, repbox_prod(pru$prod_id))
  pru_save(pru, prod_df)
  writeLines(pru$items[[1]]$prompt, file.path(pru$ver_dir,"prompt.txt"))
  return(invisible(pru))
}
```
# END OF FILE: rai_pru.R

-----------------------------------------------------------


# FILE: rai.R
```
rai_tpl_dir = function() {
  system.file("prompts", package="repboxAI")
  "~/repbox/gemini/repboxAI/inst/prompts"
}


rai_model_short = function(model) {
  ai_model_short(model)
}

rai_init = function(project_dir,json_mode=first_nn(proc_info$json_mode,FALSE), schema=NULL, values = NULL, context=NULL, media_files=NULL, tpl=NULL, tpl_file = proc_info$tpl_file,   model=ai_opts$model, temperature=ai_opts$temperature, proc_info=NULL, ai_opts=get_ai_opts() ) {
  
  ai_init(project_dir, json_mode=json_mode, schema=schema, values=values, context=context, media_files=media_files, tpl=tpl, tpl_file=tpl_file, model=model, temperature=temperature, ai_opts=ai_opts)
}

# Simple wrapper to ai_context
#
# Allow to easily add typical repbox files like art_pdf
rai_context = function(project_dir, model=ai_opts$model, media_files = NULL, prompt=NULL, ttl_sec=60*5, doc_type_pdf = NULL, add_all_pdf = FALSE, cache_context = isTRUE(ai_opts$cache_context), api_key = getOption("gemini_api_key"), ai_opts = get_ai_opts()) {
  restore.point("rai_context")
  pdf_files = NULL
  if (add_all_pdf) {
    pdf_files = repbox_all_pdf_file(project_dir)
  } else if (!is.null(doc_type_pdf)) {
    pdf_files = repbox_pdf_file(project_dir, doc_type = doc_type_pdf)
  }
  media_files = sort(union(pdf_files, media_files))
  ai_context(project_dir,model = model, media_files=media_files, prompt=prompt, ttl_sec=ttl_sec, cache_context = cache_context, api_key=api_key, ai_opts=ai_opts)
}

rai_run = function(rai,values=rai$values, verbose=FALSE) {
  restore.point("rai_run")
  rai = ai_run(rai, values, verbose)
  rai
}

rai_doc_dir_to_project_dir = function(doc_dir) {
  dirname(dirname(dirname(dirname(doc_dir))))
}

rai_fp_dir_to_project_dir = function(fp_dir) {
  dirname(dirname(fp_dir))
}

rai_fp_dir_to_doc_type = function(fp_dir) {
  str.right.of(basename(fp_dir),"prod_")
}


rai_has_input = function(doc_dir, ...) {
  inputs = list(...)
  restore.point("rai_has_input")
  fp_dir = doc_dir_to_fp_dir(doc_dir)
  doc_form = rdoc_form(doc_dir)
  for (inp in inputs) {
    if (isTRUE(inp=="pdf")) {
      if (doc_form != "pdf") return(FALSE)
    } else if (is.character(inp)) {
      inp = list(prod_id = inp)
    }
    if (is.list(inp)) {
      ok = fp_has_prod(fp_dir, inp$prod_id, inp$proc_id)
      if (!ok) {
        cat("\nRequired input ", inp$prod_id, " not found for ", doc_dir,"\n")
        return(FALSE)
      }
    }
  }
  return(TRUE)
}
```
# END OF FILE: rai.R

-----------------------------------------------------------


# FILE: repair.R
```
# Functions to repir stuff, e.g.
# remove products that are renamed etc

example = function() {
  parent_dir = "~/repbox/projects_share"
  project_dirs = repboxExplore::get_project_dirs(parent_dir)
  prod_id = "readme_data_descr"
  #prod_id = "tab_tino"
  proc_id = NULL
}

# previously some cellid had form cell-2_4
# others just 2_4
# now common format c2_4
repair_cell_id = function() {
  parent_dir = "~/repbox/projects_share"
  project_dirs = repboxExplore::get_project_dirs(parent_dir)

  ver_dirs = fp_all_ver_dirs(project_dirs, "tab_html")
  ver_dirs = fp_all_ver_dirs(project_dirs, "tab_main")
  
  ver_dir = ver_dirs[33]
  for (ver_dir in ver_dirs) {
    prod_df = fp_load_prod_df(ver_dir)
    html = prod_df$tabhtml
    html = html %>% stri_replace_all_fixed('id = "cell-cell-', 'id="')
    html = html %>% stri_replace_all_regex('id[ ]*=[ ]*"', 'id="')
    html = html %>% stri_replace_all_fixed('id="cell-', 'id="')

    html = sapply(seq_along(html), function(i) {
      str = html[i]
      tabid = prod_df$tabid[i]
      str = stri_replace_all_regex(str, 'id="(\\d+)"', paste0('id="c', tabid, '_$1"'))[[1]]
      str = stri_replace_all_regex(str, 'id="(\\d+)-(\\d+)"','id="c$1_$2"')[[1]]
      str = stri_replace_all_regex(str, 'id="(\\d+)_(\\d+)"','id="c$1_$2"')[[1]]
      str
    })
    prod_df$tabhtml = html
    fp_save_prod_df(prod_df, ver_dir)
  }

  ver_dirs = fp_all_ver_dirs(project_dirs, "cell_base")
  ver_dirs = fp_all_ver_dirs(project_dirs, "cell_list")
  ver_dir = ver_dirs[1]
  
  for (ver_dir in ver_dirs) {
    prod_df = fp_load_prod_df(ver_dir)
    cellid = prod_df$cellid
    cellid = stri_replace_first_fixed(cellid, "cell-","")
    cellid = stri_replace_all_fixed(cellid,  '-',"_")
    
    rows = stri_detect_regex(cellid,"^\\d+_\\d+$" )
    cellid[rows] = paste0("c", cellid[rows])
    rows = stri_detect_regex(cellid,"^\\d+$" )
    cellid[rows] = paste0("c", prod_df$tabid[rows], "_", cellid[rows])
    prod_df$cellid = cellid
    fp_save_prod_df(prod_df, ver_dir, overwrite=TRUE)
  }

  
  ver_dirs = fp_all_ver_dirs(project_dirs, "map_reg_static")
  ver_dir = ver_dirs[1]
  
  for (ver_dir in ver_dirs) {
    prod_df = fp_load_prod_df(ver_dir)
    cellids = prod_df$cell_ids
    i = 1
    cellids = sapply(seq_along(cellids), function(i) {
      str = cellids[i]
      tabid = prod_df$tabid[i]
      str = stri_replace_all_fixed(str, "cell-","")
      str = stri_replace_all_regex(str,  '(?<=^|,)(\\d+)(?=,|$)', paste0(tabid,"_$1"))
      str = stri_replace_all_fixed(str,  '-',"_")
      str = stri_replace_all_regex(str,  '(?<=^|,)(\\d+_\\d+)(?=,|$)', paste0("c$1"))
      str
    })
    prod_df$cell_ids = cellids
    prod_df = rename.col(prod_df, "do_file", "script_file")
    fp_save_prod_df(prod_df, ver_dir, overwrite=TRUE)
  }
  

  ver_dirs = fp_all_ver_dirs(project_dirs, "reg_classify")
  ver_dir = ver_dirs[1]
  
  for (ver_dir in ver_dirs) {
    prod_df = fp_load_prod_df(ver_dir)
    cellids = prod_df$cell_id_coef_of_interest
    i = 1
    cellids = sapply(seq_along(cellids), function(i) {
      str = cellids[i]
      tabid = prod_df$tabid[i]
      str = stri_replace_all_fixed(str, "cell-","")
      str = stri_replace_all_regex(str,  '(?<=^|,)(\\d+)(?=,|$)', paste0(tabid,"_$1"))
      str = stri_replace_all_fixed(str,  '-',"_")
      str = stri_replace_all_regex(str,  '(?<=^|,)(\\d+_\\d+)(?=,|$)', paste0("c$1"))
      str
    })
    prod_df$cell_id_coef_of_interest = cellids
    fp_save_prod_df(prod_df, ver_dir, overwrite=TRUE)
  }
  
}

rename_map_reg_static_proc_id = function() {
  library(repboxAI)
  parent_dir = "~/repbox/projects_share"
  project_dirs = repboxExplore::get_project_dirs(parent_dir)
  proc_dirs = list.files(paste0(project_dirs,"/fp"), glob2rx("g25pe"),recursive = TRUE,include.dirs = TRUE, full.names = TRUE) 
  proc_dirs = proc_dirs[has.substr(proc_dirs, "/map_reg_static/") ]
  proc_dirs = proc_dirs[dir.exists(proc_dirs)]
  
  for (proc_dir in proc_dirs) {
    ver_dir = file.path(proc_dir, "v0")
    pru_files = file.path(ver_dir, c("pru.Rds", "outage_pru.Rds", "error_pru.Rds"))
    pru_files = pru_files[file.exists(pru_files)]
    
    
    proc_id = basename(proc_dir)
    new_procid = NULL
    for (pru_file in pru_files) {
      pru = readRDS(pru_files)
      if (pru$proc_id != "g25pe") next
      tabmain_procid = pru$tab_main_info$proc_id
      new_procid = paste0(pru$proc_id, "-", tabmain_procid)
      new_proc_dir = file.path(dirname(proc_dir),new_procid)
      new_ver_dir = file.path(new_proc_dir, "v0")
      new_ver_id = fp_ver_dir_to_ids(new_ver_dir)$ver_id
      pru$proc_info$proc_id = new_procid
      pru$proc_id = new_procid
      pru$ver_dir = new_ver_dir
      pru$ver_id = new_ver_id
      saveRDS(pru, pru_file)
    }
    if (!is.null(new_procid)) {
      file.rename(proc_dir, new_proc_dir)
    }
  }
}

fp_prod_dir_to_trash = function(parent_dir, prod_id) {
  rem_ver_dirs = unique(c(
    fp_all_ver_dirs(parent_dir, prod_id),
    fp_all_error_ver_dirs(parent_dir,prod_id),
    fp_all_outage_ver_dirs(parent_dir, prod_id)
  ))
  if (NROW(rem_ver_dirs)==0) {
    cat("\nNo prod_dir found for ", prod_id,"\n")
    return(NULL)
  }
  rem_prod_dirs = unique(fp_ver_dir_to_prod_dir(rem_ver_dirs))  
}

remove_old_rai_dirs = function(project_dirs) {
  rai_dirs = file.path(project_dirs,"rai")
  rai_dirs = rai_dirs[dir.exists(rai_dirs)]
  dir_to_trash(rai_dirs)
}
```
# END OF FILE: repair.R

-----------------------------------------------------------


# FILE: repbox_prods.R
```
example = function() {
  prods = repbox_prods()
  names(prods)
  prod = repbox_prod("reg_classify")
  prod = repbox_prod("map_reg_run")
  prod = repbox_prod("map_inv_reg_run")
  
  prod = repbox_prod("readme_overview")
  prod = repbox_prod("readme_vs_guide")
  prod_to_json_schema(prod, "obj",allow_null_def = TRUE)
  prod = repbox_prod("readme_data")
  prod_to_json_schema(prod, "arr",allow_null_def = FALSE)
  
  
  # write all json schemas so they 
  # can be used in AI prompts
  prods = repbox_prods()
  i = 1
  for (i in seq_along(prods)) {
    prod_name = names(prods[i])
    file = paste0("~/repbox/gemini/repboxAI/inst/prod_schemas/", prod_name, ".json")
    json = prod_to_json_schema(prods[[i]], "obj",, allow_null_def = FALSE)
    writeLines(json, file)  
  }
}

repbox_prod = function(pid, prods = repbox_prods()) {
  prods[[pid]]
}

repbox_prods = function() {
  c(
    repbox_tab_prods(),
    repbox_readme_prods(),
    repbox_map_prods(),
    repbox_other_prods()
  )
}


repbox_tab_prods = function() {
  prods_define(
    prod_define("tab_list",
      descr = "List of article's tables",
      list(
        tabid = schema_str(is_key=TRUE,maxLength = 10),
        otabid = schema_str(), # ordered tabid by augmenting numbers with 0s from left
        tabtitle = schema_str(maxLength=400)
      ),
      keys = "tabid",
      order_by = "otabid"
    ),
    prod_define("tab_notes",
      descr = "List of article's tables with extracted title and table notes",
      widens = "tab_list",
      list(
        tabnotes = schema_str(maxLength=2000)
      )
    ),
    prod_define("tab_html",
      descr = "Contains normalized HTML of every extracted article table",
      widens = "tab_list",
      list(
        tabhtml = schema_html_tab()
      )
    ),
    prod_define("tab_main",
      descr = "Article tables with html, title and notes",
      widens = c("tab_html","tab_notes")          
    ),
    prod_define(
      "cell_list",
      # means 1 parent row can have multiple children rows
      parent = "tab_html",
      from_parent = c("tabid","otabid"),
      fields = list(
        cellid = schema_str(),
        row = schema_int(),
        col = schema_int(),
        inner_html = schema_str(),
        text = schema_str(),
        colspan = schema_int(),
        rowspan = schema_int()
      ),
      keys = c("cellid"),
      order_by = c("otabid","cellid"),
      test_group_by = c("tabid")
    ),
    prod_define(
      "cell_base",
      widens = "cell_list",
      fields = list(
        has_num = schema_bool(),
        num_str = schema_str(),
        num = schema_num(),
        has_deci = schema_bool(descr = "Did the original string has a decimal point?"),
        num_deci = schema_int(descr = "Number of digits after decimal point in original string"),
        bracket = schema_str(enum=c("", "()","[]","{}")),
        has_sig_star = schema_bool(),
        sig_star_str = schema_str(),
        other_num_str = schema_str(descr = "Not empty if we found another number string looking from the right"),
        nchar = schema_int(),
        nchar_letters = schema_int(),
        
        # Tests that can be quickly computed and will be added       
        flag_two_num = schema_bool(descr="Are there two numbers in the cell? Can suggests incorrect cell splits."),
        flag_two_deci = schema_bool(descr="Are there two decimal numbers in the cell? More strongly suggests wrong cell split."),
        flag_miss_bracket_below = schema_bool(descr="Do we miss a cell like (3.42) below, because such cells are below other numbers in the row?"),
        flag_miss_num_above_bracket = schema_bool(descr="Complements flag_miss_bracket_below, do we miss a cell with a normal number like 1.32 above a cell like (3.42)?")
      ),
      descr ="Will be generated with heuristics from cell_list. We have so many fields because they may facilitate consistency checks of the extracted tables."
    )
  )
}


repbox_readme_prods = function() {
  prods_define(
    prod_define(
      "readme_overview",
      fields = list(
        readme_file = schema_str("The name of the readme file."),
        is_reproduction_package_readme = schema_bool("Does the file roughly look like a typical readme file for a reproduction package? I.e. does it describe the code file and possibly the data files?"),
        describes_data = schema_bool("Does the README file describes the data used in the analysis?"),
        listed_data_set_files = schema_str("Comma separated string with names of data set files described in the README. "),
        describes_variables = schema_bool("Does the README describe at least some variables contained in the data set?"),
        missing_data = schema_bool("Does the README state that some data sets are missing in the reproduction package, e.g. because the data is confidential or proprietary?"),
        missing_confidential_data = schema_bool("Does the README explicitly state that some data sets are missing due to confidentiality reasons?"),
        missing_proprietary_data = schema_bool("Does the README explicitly state that some data sets are missing because they are propietary?"),
        data_country = schema_str("Is there information that the data sets are from a particular country? If yes, state the countries as comma separated string. If the data is from a larger region state it, e.g. EU or world."),
        data_year_start = schema_int("If the README provides information on the first year the data is from, state it. Otherwise return NA"),
        data_year_end = schema_int("If the README provides information on the last year the data is from, state it. Otherwise return NA"),
        missing_data_set_files = schema_str("Comma separated string with names of those data set files described flagged in the README as not included in the reproduction package, e.g. because they are proprietary or confidental."),
        included_data_set_files = schema_str("Comma separated string with names of those data set files described as being included in the reproduction package."),
        generated_data_set_files = schema_str("Comma separated string with names of those data set files described as being generated by the code in the reproduction package. Precomputed versions of the generated data sets may sometimes be already included in the reproduction package."),
        analyst_notes = schema_str("Any other relevant information, context, or ambiguities noted in the README related to the fields above. Be brief. Analyst notes can often remain empty.")
      ),
      keys = c("readme_file")
    ),
    prod_define(
      "readme_var",
      fields = list(
        readme_file = schema_str("The name of the readme file."),
        varname = schema_str("Name of the variable"),
        vardescr = schema_str("Description or label of the variable as given in the readme file."),
        dataset_files = schema_str("If the readme file mentions in which data set file(s) the variable occurs, list those data set file names as a comma separated string.")
      ),
      keys = c("readme_file")
    ),
    prod_define(
      "readme_script_tab_fig",
      fields = list(
        readme_file = schema_str("The name of the readme file."),
        script_file = schema_str("Filename of the script"),
        table_names = schema_str("Name of the table or tables that according to the README file are wholly or partially created by the script given in script_file. If the script creates multiple tables, write a comma separated list, e.g. 'Table 1, Table 3, Table A2.'"),
        figure_names = schema_str("Name of the figure or figures that according to the README file are wholly or partially created by the script. If the script creates multiple figures, write a comma separated list, e.g. 'Figure 2, Figure 5'")
      ),
      keys = c("readme_file")
    ),
    prod_define(
      "readme_data",
      fields = list(
        readme_file = schema_str("The name of the readme file."),
        dataset_file = schema_str("Name of the data set. If a concrete file name is mentioned use that file name with extension. Sometimes the rows of a larger data set are distrbuted over several files with similar naming convetions. E.g. files like pop_de.csv, pop_fr.csv, pop_uk.csv, ... which all contain population data for a different country. In that case list up to three data set files from the larger data set as a comma separated list and add the glob pattern that matches all files in the next field dataset_file_glob."),
        dataset_file_glob = schema_str("Only relevant if the data set rows are distributed over several files with similar naming convention. Then write down the glob pattern that matches all files. E.g. if the data set is in a set of files like pop_de.csv, pop_fr.csv, pop_uk.csv, ... which all contain population data for a different country. Write pop_*.csv. If not relevant, write just an empty string."),
        dataset_descr = schema_str("Based on the information in the README a short description of the data set in 1 to 4 sentences."),
        dataset_source = schema_str("If the README provides any information on the data set source, please state the source here."),
        is_included = schema_bool("TRUE if the README says that the data set is included in the reproduction package. FALSE if the README states that the data set is not included, e.g. because it is proprietary. If not info is given, set to null.",allow_null = TRUE),
        instructions_how_to_obtain_data = schema_bool("Only relevant for data sets that are not included. Does the README contain instructions of how to obtain the data set?"),
        is_intermediate_data = schema_bool("Does the README state that it is an intermediate data set, generated from other raw data sets?"),
        table_names = schema_str("If the README describes that the data set is used to generate certain tables in the article, please list all thoise tables as a comma separated list, e.g. 'Table 2, Table 3, Table A1.' If nothing is explicitly stated just write an empty string. "),
        figure_names = schema_str("If the README describes that the data set is used to generate certain figures in the article, please list all thoise tables as a comma separated list, e.g. 'Figure 1, Figure 5'. If nothing is explicitly stated just write an empty string."),
        data_country = schema_str("Is there information that the data is from one or multiple countries? If yes, state the countries as comma separated string. If the data is from a larger region state it, e.g. EU or world."),
        explicitly_stated_data_country = schema_bool("TRUE if the information about 'data_country' explicitly stated in the README file, FALSE if you guessed the information."),
        data_year_start = schema_int("If the README provides information on the first year of observatons in the data is from, state it."),
        data_year_end = schema_int("If the README provides information on the last year of observations in the data, state it."),
        explicitly_stated_data_year_start = schema_bool("TRUE if the information about 'data_year_start' explicitly stated in the README file, FALSE if you guessed the information."),
        explicitly_stated_data_year_end = schema_bool("TRUE if the information about 'data_year_end' explicitly stated in the README file, FALSE if you guessed the information."), 
        dataset_type = schema_str("Can one infer from the readme whether it is a 'panel', 'cross-section' or 'time series' data set? A 'panel' data set has a time dimensions and at least one cross sectional dimension. Example 1: A panel data set that has observations for multiple industries (cross-section dimension 1) in multiple countries (cross-section dimension 2) for multiple years (time dimension). Example 2: A panel data set with observations for multiple subjects (cross-section dimension 1) for multiple experimental rounds (time dimension). A cross-section data set has no explicit time dimension i.e. no multiple periods of observations for one cross section unit. A time-series data set only has a time dimension and just a single cross-section unit (e.g. a time series for a single country).",enum = c("panel", "cross-section","time series")),
        explicitly_stated_data_set_type = schema_bool("TRUE if the information in the README really explicitly allows to infer the data set type, FALSE if you rather guessed the information."),
        num_cross_section_dimensions = schema_int("For panel and cross-section data, can you infer the number of cross section dimensions of the data set from the README?"),
        names_cross_section_dimensions = schema_str("For cross section or panel data sets can you infer suitable names for the cross section dimensions from the README? If there are multiple cross-section dimensions return a comma separated list."),
        explicitly_stated_names_cross_section_dimensions = schema_bool("TRUE if the information in the README really explicitly allows to infer the cross section dimensions, FALSE if you rather guessed the dimesions."),
        id_cross_section_dimensions = schema_str("For cross section or panel data sets can you infer from the README which variables are the ID variables contained in the data set for each cross section dimensions (e.g. subject id, sector id, country name, etc)? If there are multiple cross-section dimensions return a comma separated list."),
        explicitly_stated_id_cross_section_dimensions = schema_bool("TRUE if the information in the README really explicitly allows to infer the cross section dimensions, FALSE if you rather guessed the dimenisons."),
        name_time_dimension = schema_str("For time series or panel data sets can you infer from the README a suitable description of the time dimenions / frequency (e.g. 'year', 'year-month', 'day', 'experimental round') and write it down?"),
        explicitly_stated_name_time_dimension = schema_bool("TRUE if the information in the README really explicitly allows to infer the type of time dimension of the data set, FALSE if you rather guessed the dimension."),
        id_time_dimension = schema_str("For time series or panel data sets can you infer from the README which variables are ID variables for the time dimension (e.g. year, month, t, period) and write it down?"),
        analyst_notes = schema_str("Any other relevant information, context, or ambiguities noted in the README file regarding this dataset. Be brief. Analyst notes can often remain empty.")
      ),
      keys = c("readme_file")
    ),
    prod_define(
      "readme_data_descr",
      fields = list(
        readme_file = schema_str("The name of the readme file."),
        dataset_file = schema_str("Name of the data set."),
        dataset_descr = schema_str("Based on the information in the README a short description of the data set in 1 to 4 sentences."),
        dataset_source = schema_str("If the README provides any information on the data set source, please state the source here.")
      ),
      keys = c("readme_file")
    ),
    prod_define(
      "readme_vs_guide",
      fields = list(
        readme_file = schema_str("The name of the readme file being analyzed."),
        overview_is_present = schema_bool("TRUE if an overview section is present at the beginning of the README."),
        overview_mentions_runtime = schema_bool("TRUE if the overview gives an estimate of the total runtime for the replication."),
        data_provenance_is_present = schema_bool("TRUE if a section discussing data availability and provenance is present."),
        data_provenance_no_external_data_is_stated = schema_bool("TRUE if the README explicitly states that no external data is used."),
        data_provenance_rights_statement_is_present = schema_bool("TRUE if any statement about rights to use or redistribute data is included."),
        data_provenance_rights_claims_permission_to_use = schema_bool("TRUE if the author certifies legitimate access and permission to use the data."),
        data_provenance_rights_claims_permission_to_redistribute = schema_bool("TRUE if the author certifies permission to redistribute the data in the package."),
        data_provenance_license_is_present = schema_bool("TRUE if a subsection on data licensing is present."),
        data_provenance_license_summary = schema_str("A brief summary of the data license mentioned (e.g., 'CC-BY-NC', 'Public Domain')."),
        data_provenance_availability_summary = schema_str("The stated summary of data availability."),
        data_provenance_sources_details_is_present = schema_bool("TRUE if detailed descriptions for individual data sources are provided."),
        data_provenance_sources_details_uses_table_format = schema_bool("TRUE if the recommended tabular format (Data.Name, Location, Provided, Citation) is used to list data sources."),
        dataset_list_is_present = schema_bool("TRUE if a section describing each data file in the package is present."),
        dataset_list_uses_table_format = schema_bool("TRUE if a tabular format (Data file, Source, Notes, Provided) is used to list the files."),
        computational_reqs_is_present = schema_bool("TRUE if a section on computational requirements is present."),
        computational_reqs_software_is_present = schema_bool("TRUE if software requirements are listed."),
        computational_reqs_software_mentions_setup_script = schema_bool("TRUE if the README mentions a program/script to install dependencies (e.g., '0_setup.do', 'requirements.txt')."),
        computational_reqs_software_listed = schema_str("A comma-separated list of major software mentioned (e.g., 'Stata, Python, R, Matlab')."),
        computational_reqs_randomness_is_present = schema_bool("TRUE if the README mentions controlled randomness or pseudo-random number generator seeds."),
        computational_reqs_randomness_seed_is_set = schema_bool("TRUE if the README states that a random seed is set and ideally where."),
        computational_reqs_performance_is_present = schema_bool("TRUE if performance requirements (runtime, storage, hardware) are described."),
        computational_reqs_performance_estimated_runtime = schema_str("The approximate time stated to run the analysis (e.g., '1-2 hours', '> 14 days')."),
        computational_reqs_performance_estimated_storage = schema_str("The approximate storage space needed (e.g., '250 MB - 2 GB')."),
        computational_reqs_performance_mentions_hardware = schema_bool("TRUE if specific hardware (e.g., CPU cores, RAM, OS) used for the analysis is described."),
        code_description_is_present = schema_bool("TRUE if a high-level overview of the program files and their purpose is given."),
        code_description_license_is_present = schema_bool("TRUE if a subsection on code licensing is present."),
        code_description_license_summary = schema_str("A brief summary of the code license mentioned (e.g., 'MIT', 'GPL')."),
        replication_instructions_is_present = schema_bool("TRUE if a section with step-by-step instructions for the replicator is present."),
        replication_instructions_is_linear_sequence = schema_bool("TRUE if the instructions are presented as a simple, clear, step-by-step list as recommended by the template."),
        output_mapping_is_present = schema_bool("TRUE if a list or table mapping outputs (tables, figures) to the programs that generate them is present."),
        output_mapping_reproducibility_claim = schema_str("The claim made about which outputs can be reproduced."),
        output_mapping_uses_table_format = schema_bool("TRUE if the recommended tabular format (Figure/Table #, Program, Output file) is used for the mapping."),
        references_section_is_present = schema_bool("TRUE if a 'References' section is present at the end of the README."),
        analyst_summary = schema_str("A brief, one-to-three sentence summary of how well the README conforms to the template, noting any major omissions.")
      ),
      keys = c("readme_file")
    )
  )  
}


repbox_map_prods = function() {
  prods_define(
    prod_define("map_reg_static",
      list(
        tabid = schema_str("The table ID as stated in the list of tables above."),
        regid = schema_str("A unique id you assign to each regression that you have identified. The format shall be `r{tabid}_{counter}`. E.g. the regid of 4th regression identified in Table 2 would be r2_4"),
        script_file = schema_str("The name of the script file, as listed in the list of script files above. I.e. include file paths if they are stated in the list above."),
        code_line = schema_int("The code line of the regression command in the script file, corresponding to the particular regression shown in the table. If the command extends over more than one line, write down the first line. If certain cells"),
        cell_ids = schema_str("A comma separated list of all cell ids of those cells in the HTML version of the table that correspond to this specific regression and whose value was computed by the specified code line. E.g. for a table with tabid='2', this comma separated string of cell ids might look like 'c2_10,c2-12,c2-14'. Each cell id can be found as the 'id' tag of the corresponding <td> or <th> element of the HTML version of the article's table. Only add cells that show numeric results, e.g. estimated coefficient, or number of observations, but no title cells or cells showing variable labels. Some tables in articles are structured such that some descriptive statistics, like the number of observations are shown on the bottom of a column and apply to multiple regressions shown in that column. Add the corresponding cell id for every regression, that they apply to.")
      )
    ),
    prod_define("map_reg_run", list(
      tabid = schema_str("The table ID as stated in the list of tables above."),
      regid = schema_str("A unique id you assign to each regression that you have identified. The format shall be `r{{tabid}}_{{counter}}`. E.g. the regid of 4th regression identified in Table 2 would be r2_4"),
      cell_ids = schema_str("A comma separated list of all cell ids of those cells in the HTML version of the table that correspond to this specific regression and whose value was computed by the specified code line and correspond to the specified output. E.g. for a table with tabid='2', this comma separated string of cell ids might look like 'c2_10,c2_12,c2_14'. Each cell id can be found as the 'id' tag of the corresponding <td> or <th> element of the HTML version of the article's table. Only add cells that show numeric results, e.g. estimated coefficient, or number of observations, but no title cells or cells showing variable labels. Don't add here cells that belong to the regression but are computed in another (post-)regression command. For those cells a separate entry shall be generated. Some tables in articles are structured such that some descriptive statistics, like the number of observations are shown on the bottom of a column and apply to multiple regressions shown in that column. Add the cell ids of such statistics to every regression, that they apply to."),
      runid = schema_int("The unique runid identifying the regression output in the script file. If no output chunk is provided for the regression or post regression command, set null.", allow_null=TRUE),
      script_file = schema_str("The corresponding name of the Stata script file.", allow_null=TRUE),
      script_num = schema_int("The number of the script file, map the script file name to the script_num shown in the table of Stata scripts in the prompt.", allow_null=TRUE),
      code_line = schema_int("The code line of the regression command in the script file.", allow_null=TRUE),
      wrong_number_cases = schema_arr(descr = "Sometimes, but quite rarely, there was a transcription error when the article was written such that one or multiple numbers shown in the regression table don't correspond to the actual numbers computed by the Stata command in the replication package. If you find such cases for the current regression, note them in this array. Don't note cases where numbers between the Stata output and table only differ because they are rounded to a different number of digits or formatted differently, which is very common and no problem. Usually all numbers are transcribed correctly and this array will be empty. Also don't note here those cells whose number is computed by another (post-)regression command. As explained in the instructions for numbers belonging to this regression but compted by another post regression command, a complete new main entry shall be generated.", items=schema_obj(
        properties = list(
          cell_id = schema_str("The cell_id of the table cell showing the wrong number, e.g. 'c2_10'"),
          wrong_number_in_cell = schema_num("The wrong number shown in the cell. Ignore special formating and just write down the numeric value."),
          number_in_stata_output = schema_num("The actual number shown in the output of the Stata regression.")
        )
      )),
      problem = schema_str("Did you encounter a problem related to this mapping? If yes describe it here. Typically this field will be empty. Only write something if there is an important problem that substantially hampers this mapping task.", allow_null = TRUE)
    )),
    prod_define("map_inv_reg_run",list(
      runid = schema_int("The unique runid of the regression as stated in the table above. It is also shown in title of the corresponding output chunk of the text file containing all Stata scripts with regression outputs."),
      script_file = schema_str("The name of the Stata script file that contains the regression."),
      script_num = schema_int("The corresponding number of the script file as shown in the table of all Stata scripts in the prompt."),

      code_line = schema_int("The code line of the regression command in its Stata script. For regression commands spaning more than one line, use the first code line."),
      tabid = schema_str("If you can map the regression to a particular table shown in the list above, state here the corresponding tabid. Otherwise set null.", allow_null = TRUE),
      figid = schema_str("If the regression is used to generate a particular figure in article or appendix, please note here the figure id. For example, if a figure is called 'Figure 5' the figid would be just '5', if a figure is called 'Fugure A.1' the figid would be 'A.1'.", allow_null = TRUE),
      cell_ids = schema_str("Relevant if the run regression can be mapped to particular table. A comma separated list of all cell ids of those cells in the HTML version of the table that correspond to the specific run regression. E.g. for a table with tabid='2', this comma separated string of cell ids might look like 'c2_10,c2-12,c2-14'. Each cell id can be found as the 'id' tag of the corresponding <td> or <th> element of the HTML version of the article's tables. Only add cells that show numeric results, e.g. estimated coefficient, or number of observations, but no title cells or cells showing variable labels. Some tables in articles are structured such that some descriptive statistics, like the number of observations are shown on the bottom of a column and apply to multiple regressions shown in that column. Add the corresponding cell id for every regression, that they apply to."),
      problem = schema_str("Did you encounter a problem related to this mapping? If yes describe it here. Typically this field will be empty. Only write something if there is an important problem that substantially hampers this mapping task.", allow_null = TRUE)
    ))
  )
}

repbox_classify_prods = function() {
  prods_define(
    prod_define("tab_classify",
      list(
        tabid = schema_str("The table id"),
        tab_title = schema_str("The table title"),
        panels = schema_str("Some tables exist of different panels shown above each other. If that is the case return a comma separated string with short panel IDs e.g. 'A,B,C' if it has a panel A, panel B and panel C. If no separate panels are marked just return null."),
        num_panels = schema_int("The number of explicit panels in the table. If the table does not distinguish panels, write 0."),
        shows_descriptive = schema_bool("true if the table shows descriptive statistics"),
        is_balancing_table = schema_bool("true if the table is a balancing table that shows whether certain characteristics are similarily distributed between control and treatment groups."),
        shows_regression = schema_bool("true if results of one or several regressions are shown in the table."),
        shows_did = schema_bool("true if results of a difference-in-difference regression are shown in the table."),
        shows_rdd = schema_bool("true if results of regression discontinuity design are shown in the table."),
        shows_iv_results =  schema_bool("true if results of an instrumental variable regression are shown in the table"),
        shows_iv_first_stage = schema_bool("true if results of a first stage instrumental variable regression are shown in the table."),
        shows_placebo_test = schema_bool("true if results of a placebo test are shown in the table"),
        num_regression = schema_int("The results of how many separate regressions are shown in the table?"),
        uses_panel_data =  schema_bool("true if the data set underlying the table is a panel data set."),
        short_descr =  schema_str("A short description of what is shown in the table.", allow_null = FALSE)
      )
    ),
    prod_define("reg_classify_static", list(
      tabid = schema_str("The table ID as stated in the list of regressions above."),
      regid = schema_int("The regression ID as stated in the field 'regid' in the list of regressions shown above."),
      short_descr =  schema_str("A short description of what the regression analyzes based on the information in the article.", allow_null = FALSE),
      is_did_reg = schema_bool("true if the regression performs a difference-in-difference (DID) analysis."),
      is_rdd_reg = schema_bool("true if the regression performs a regression discontinuity design (RDD) analysis."),
      is_iv_reg =  schema_bool("true if it is an instrumental varibale regression"),
      is_iv_first_stage_reg = schema_bool("true if the shown regression results correspond to the first stage regression of an instrumental variable regression (in the script the command could be an iv regression with the option to show first stage results or it could be a separate OLS first stage regression)"),
      is_placebo_test = schema_bool("true if the regression performs a placebo test, or a similar permutation test."),
      is_pref_spec_in_tab = schema_bool("Often tables show multiple regression specifications and sometimes the authors state which specification is their preferred specification. Set true if this regression is the preferred specification  among the specifications shown in the table."),
      is_main_result = schema_bool("true if the regression results are described as main results of the article (compared to robustness checks or additional results)"),
      is_additional_result = schema_bool("true if the regression shows additional results that are not described as main results of the article"),
      is_robustness_check = schema_bool("true if the regression is mainly a robustness check for other results."),
      label_dep_var = schema_str("Based on the information in the article and table find a suitable label for the dependent variable in the regression."),
      labels_coef_of_interest = schema_str("Often regression tables show both coefficients of primary interest for the analysis and coefficients for control variables that are not of primary interest. Sometimes only coefficient of primary interest are shown. Please state the variable lables of the coefficients of primary interest for this regression as shown in the table. If there are multiple variables of primary interest your string shall be a comma separted list, e.g. 'age,gender'."),
      cell_id_coef_of_interest = schema_str("Please state the cell ids of all cells for this regression that show the numeric value of the  coefficient of interests or their standard error / p-value / t-value. Return a comma separated list of all those cell_ids, like 'c2_10,c2-12'."),
      analyses_heterogeneity = schema_bool("true if the  cofficient of interest of the regressions analyze heterogenous effects, e.g. if the regression provided information on how treatment effect sizes differ between subgroups."),
      error_in_prompt_or_media = schema_str("Is there some inconsistency in the prompt or the attached media files, e.g. the media don't show the tables listed in the prompt etc. This could be an indicator for some error in my pipeline. If such an inconsistency exists, briefly describe it only for the first regression. If all seems ok, set to an empty string. In later regressions always set to an empty string.")
    )),
    prod_define("reg_classify", list(
      tabid = schema_str("The table ID as stated in the list of regressions above."),
      regid = schema_int("The regression ID as stated in the field 'regid' in the list of regressions shown above."),
      short_descr =  schema_str("A short description of what the regression analyzes based on the information in the article.", allow_null = FALSE),
      is_did_reg = schema_bool("true if the regression performs a difference-in-difference (DID) analysis."),
      is_rdd_reg = schema_bool("true if the regression performs a regression discontinuity design (RDD) analysis."),
      is_iv_reg =  schema_bool("true if it is an instrumental varibale regression"),
      is_iv_first_stage_reg = schema_bool("true if the shown regression results correspond to the first stage regression of an instrumental variable regression (in the script the command could be an iv regression with the option to show first stage results or it could be a separate OLS first stage regression)"),
      is_placebo_test = schema_bool("true if the regression performs a placebo test, or a similar permutation test."),
      is_pref_spec_in_tab = schema_bool("Often tables show multiple regression specifications and sometimes the authors state which specification is their preferred specification. Set true if this regression is the preferred specification  among the specifications shown in the table."),
      is_main_result = schema_bool("true if the regression results are described as main results of the article (compared to robustness checks or additional results)"),
      is_additional_result = schema_bool("true if the regression shows additional results that are not described as main results of the article"),
      is_robustness_check = schema_bool("true if the regression is mainly a robustness check for other results."),
      label_dep_var = schema_str("Based on the information in the article and table find a suitable label for the dependent variable in the regression."),
      labels_coef_of_interest = schema_str("Often regression tables show both coefficients of primary interest for the analysis and coefficients for control variables that are not of primary interest. Sometimes only coefficient of primary interest are shown. Please state the variable lables of the coefficients of primary interest for this regression as shown in the table. If there are multiple variables of primary interest your string shall be a comma separted list, e.g. 'age,gender'."),
      cell_id_coef_of_interest = schema_str("Please state the cell ids of all cells for this regression that show the numeric value of the  coefficient of interests or their standard error / p-value / t-value. Return a comma separated list of all those cell_ids, like 'c2_10,c2-12'."),
      analyses_heterogeneity = schema_bool("true if the  cofficient of interest of the regressions analyze heterogenous effects, e.g. if the regression provided information on how treatment effect sizes differ between subgroups."),
      map_label_to_var = schema_arr(
        descr = "For the current regression only: map labels in the document (table/text/notes) for the dependent variable and for every explanatory variable shown in the table (and fixed effects / clustering vars if described) to the variable names used in the code and data.",
        items = schema_obj(
          properties = list(
            label_in_doc = schema_str("The label of the variable as shown in the document: in the table or used in the text of the article."),
            var_in_code = schema_str("The variable name as used in the code. If the regressor is an interaction effect between to variables, e.g. 'x' and 'z' use the notation 'x#z'."),
            var_type = schema_str("A short characteriztion of the variable type: 'd' for dependented variable, 'x' for explanatory variable, 'fe' for a fixed effect, 'clu' for a variable that determines the cluster robust standard errors.")
          )
        )
      ),
      error_in_prompt_or_media = schema_str(
        "Is there some inconsistency in the prompt or the attached media files, e.g. the media don't show the tables listed in the prompt etc. This could be an indicator for some error in my pipeline. If such an inconsistency exists, briefly describe it only for the first regression. If all seems ok return null. In other regressions than the first regressions, always set this field to null.",
        allow_null = TRUE
      )
    )  
  )

}



repbox_other_prods = function() {
  prods_define(
    prod_define(
      "privacy_breach",
      fields = list(
        dataset     = schema_str("Exact name of the dataset (file or table)."),
        variables   = schema_str("Comma‑separated list of variable names that, alone or jointly, create the privacy breach."),
        explanation = schema_str("Brief justification of why these variables constitute a breach."),
        risk_level  = schema_str("Severity of the breach: must be one of 'low', 'moderate', or 'high'.", enum=c("low","moderate","high"))
      )
    )
  )

}


schema_html_tab = function(...) {
  x = schema_str(...)
  class(x) = union(c("schema_html_tab", "schema_html"), class(x))
  x
}
```
# END OF FILE: repbox_prods.R

-----------------------------------------------------------


# FILE: repbox_tools.R
```
# Will probably be moved to repboxUtils
# but keep them here during development cycle

repbox_art_pdf_file = function(project_dir) {
  pdf_dir = file.path(project_dir, "art", "pdf")
  pdf_files = list.files(pdf_dir, glob2rx("*.pdf"), full.names = TRUE)
  pdf_files
}

```
# END OF FILE: repbox_tools.R

-----------------------------------------------------------


# FILE: tab_with_ref.R
```
rai_tab_with_ref_html = function(doc_dir) {
  
}
```
# END OF FILE: tab_with_ref.R

-----------------------------------------------------------


# FILE: td_background.R
```

```
# END OF FILE: td_background.R

-----------------------------------------------------------


# FILE: temp_source.R
```
# To better develop FuzzyProduction and repboxAI simultaneously
# we just source FuzzyProduction each time we re-load repboxAI

source_dir = function(dir, verbose=TRUE) {
  files = list.files(dir, pattern=glob2rx("*.R"),full.names = TRUE)
  for (f in files) {
    if (verbose) cat("\nsource ", f)
    source(f)
  }
}
cat("\nSource FuzzyProduction and repboxTableTools")
source_dir("~/repbox/gemini/FuzzyProduction/R")
source_dir("~/repbox/gemini/repboxTableTools/R")
source_dir("~/repbox/gemini/aikit/R")
source_dir("~/repbox/repboxDoc/R")
```
# END OF FILE: temp_source.R

-----------------------------------------------------------


# FILE: test_cell.R
```

example = function() {
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"

  ver_dir = "~/repbox/projects_share/aejapp_1_2_4/rai/prod_runs/cell_base/tab_html-n-g2flp-0-pdf-1/r0"
  ver_dir = "~/repbox/projects_share/aejapp_1_2_4/rai/prod_runs/cell_base/tab_html_hx_pdf/r0"
  prod_df = ver_load_prod_df(ver_dir=ver_dir)
  
  test_cell_base(ver_dir)
  
  ver_dirs = test_all_cell_base(project_dir)

}

test_all_cell_base = function(project_dir, overwrite=FALSE) {
  tests = tests = define_tests_cell_base()
  prod = repbox_prod("cell_base")
  fp_dir = project_dir_to_fp_dir(project_dir)
  ver_dirs = fp_all_ver_dirs(fp_dir, "cell_base")
  for (ver_dir in ver_dirs) {
    test_cell_base(ver_dir,prod=prod, tests=tests)
  }
  return(ver_dirs)
}

test_cell_base = function(ver_dir, prod_df=fp_load_prod_df(ver_dir=ver_dir), prod = repbox_prod("cell_base"), tests = define_tests_cell_base()) {
  restore.point("test_cell_base")
  tests = define_tests_cell_base()
  test_df = prod_run_tests(tests,prod_df=prod_df, prod=prod, return_details = TRUE)
  
  # create HTML output
  tab_df = cells_to_tabhtml(cell_df, add_flags=TRUE)
  rai_write_all_tables_html(tab_df, "table_test.html", ver_dir=ver_dir, title="Tests: cell_base")
  rstudioapi::filesPaneNavigate(ver_dir)
}

define_tests_cell_base = function() {
  tests = prod_tests_define(
    # flags are already part of cell_base
    flag_test_funs(
      function(df, ...) return(df)
    ),
    descr = list(),
    keys = "cellid"
  )
    
}
```
# END OF FILE: test_cell.R

-----------------------------------------------------------


# FILE: tools.R
```
# Will probably be moved to repboxUtils
# but keep them here during development cycle

example = function() {
  str = c("a,b,c","b,d","b")
}

null_to_na = function(x, na_val = NA_character_) {
  if (is.null(x)) return(na_val)
  x
}

has_col = function(x, col) {
  col %in% names(x)
}

first_nn = first.non.null = function (...) 
{
  args = list(...)
  for (val in args) {
    if (!is.null(val)) 
      return(val)
  }
  return(NULL)
}

invert_names_values = function(x) {
  y = names(x)
  names(y) = x
  y
}

add_col_left = function(df, ...) {
  args = list(...)
  restore.point("add_col_left")

  len = NROW(df)
  args = lapply(args, function(x) rep(x, length=len))
  bind_cols(as_tibble(args), df)
}
```
# END OF FILE: tools.R

-----------------------------------------------------------


# R code (project repboxDoc)


# FILE: patch_prompt.R
```
example = function() {
  library(repboxRegmap)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  base_ver_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6/fp/prod_art/map_reg_run/g25f-mocr/v0"
  rstudioapi::filesPaneNavigate(project_dir)
  options(warn=1)
  rme = rme_load(project_dir)
  txt = patch_prompt_map_reg_run_base_results(base_ver_dir, rme)
  outfile = file.path(project_dir,"fp/eval_art/patch_map_reg_run.md")
  writeLines(txt, outfile)
  rstudioapi::filesPaneNavigate(outfile)

  rme = rme_init(project_dir)

}


#' Create a string with base results and issues for a patch prompt
#'
#' This function generates a markdown-formatted string that summarizes
#' the previous mapping results and flagged issues for specific tables.
#' This string is intended to be used as part of a larger prompt for an
#' AI to "patch" or correct the initial mappings. The function is
#' structured with explicit blocks for each check to allow for easy
#' manual customization (e.g., changing descriptions or filtering checks).
#'
#' @param base_ver_dir The directory of the base mapping version to report on.
#' @param rme The rme object containing all data and evaluation results.
#' @return A character string with the formatted summary.
patch_prompt_map_reg_run_base_results = function(base_ver_dir, rme) {
  restore.point("patch_prompt_map_reg_run_base_results")

  map_version = fp_ver_dir_to_ver_id(base_ver_dir)

  # Filter for all issues related to the specified base version

  ignore_tests = c("runids_differ")

  all_issues = rme_combine_ev_df(rme) %>%
    dplyr::filter(map_version == .env$map_version, !test_name %in% ignore_tests)

  if (NROW(all_issues) == 0) {
    return("No issues found.")
  }

  # Consider only tabids that have issues
  tabids = repboxTableTools::sort_tabids(unique(all_issues$tabid))
  tab_str = sapply(tabids,patch_prompt_base_results_for_tab, rme=rme, map_version=map_version)
  main_str = paste0(tab_str, collapse="\n")

  return(main_str)
}

#' Generates a markdown summary of issues for a single table
patch_prompt_base_results_for_tab = function(tabid, rme, map_version) {
  tid = tabid; ver_id = map_version
  restore.point("patch_prompt_base_results_for_tab")

  table_str = paste0("\n\n## Table ", tid)

  # 1. Show the previous mapping result for this table
  prev_map = rme$map_reg_run %>%
    dplyr::filter(ver_id == .env$map_version, tabid == tid) %>%
    dplyr::select(any_of(c("regid", "runid", "script_file", "code_line", "cell_ids", "problem", "wrong_number_cases")))

  map_json = jsonlite::toJSON(prev_map, pretty = TRUE, auto_unbox = TRUE)

  table_str = paste0(table_str,
    "\n\n### Previous Mapping Result\n\n",
    "```json\n",
    map_json,
    "\n```\n",
    "\n\n### Flagged Potential Issues"
  )

  # Helper to get the issues for a specific test, filtered for the current table/version
  get_test_df = function(test_name) {
    if (!test_name %in% names(rme$evals)) return(tibble::tibble())
    df = rme$evals[[test_name]]
    if (is.null(df) || NROW(df) == 0) return(tibble::tibble())

    df = dplyr::ungroup(df)
    if ("map_version" %in% names(df)) df = dplyr::filter(df, map_version == ver_id)
    if ("tabid" %in% names(df)) df = dplyr::filter(df, tabid == tid)
    df
  }

  # Helper to append a formatted test result to the main string
  num_tests = 0

  note_test = function(test_name, descr, df) {
    cat("\nNoted: ", test_name, " for Table ", tid, " with ", NROW(df), " flagged rows.\n")
    num_tests <<- num_tests+1
    table_str <<- paste0(table_str,
      "\n\n#### Flagged Issue: `", test_name, "`\n\n",
      "", descr, "\n\n")
    if (is.null(df) || NROW(df)==0) return(invisible(table_str))

    table_str <<- paste0(table_str,
      df_to_markdown(df %>% dplyr::select(-any_of(c("map_version", "tabid"))))
    )
    invisible(table_str)
  }

  # --- Manual blocks for each check ---

  # Ignore runids_differ, non_reg_cmd

  # Check: invalid_cellids
  test_name = "invalid_cellids"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Invalid `cellid` Mapping.** This test flags mappings that reference a `cellid` that does not exist in the parsed table data (`cell_df`). This is a critical integrity error, indicating a hallucinated or malformed cell reference from the AI.
    descr = "**Invalid `cell_id` Mapping.** This test flags mappings that reference a `cell_id` that does not exist in the parsed table data. This indicates a hallucinated or malformed cell reference from the AI."
    note_test(test_name, descr, df)
  }

  # Check: coef_se_match
  test_name = "coef_se_match"
  df = get_test_df(test_name) # df from heuristic

  # Get wrong numbers identified by the previous AI run for this table
  wn_df = rme$map_reg_run %>%
    dplyr::filter(ver_id == .env$ver_id, tabid == tid, !sapply(wrong_number_cases, is.null))

  has_ai_wn = NROW(wn_df) > 0
  if (has_ai_wn) {
    wn_df = wn_df %>%
      dplyr::select(regid, wrong_number_cases) %>%
      tidyr::unnest(wrong_number_cases)
    if (NROW(wn_df)==0) {
      wn_df$cellid = character(0)
    } else {
      wn_df = wn_df %>%
      dplyr::rename(cellid = cell_id) # Rename for consistent joining
    }
  }

  # Only create a section if either heuristic or AI found something.
  if (NROW(df) > 0 || has_ai_wn) {
    # Heuristics found issues that AI missed
    ai_miss_df = if (has_ai_wn) df %>% dplyr::anti_join(wn_df, by = "cellid") else df
    # AI found issues that heuristics missed
    ai_extra_df = if (has_ai_wn) wn_df %>% dplyr::anti_join(df, by = "cellid") else tibble::tibble()
    # Issues found by both
    common_df = if (has_ai_wn) df %>% dplyr::inner_join(wn_df, by = "cellid") else tibble::tibble()

    common_str = ""
    if (NROW(common_df) > 0) {
      common_str =  paste0(
        "\n\n- **Agreed Mismatches:** Both our heuristic and the previous AI run identified potential transcription errors for the following cells. Please verify and include them in `wrong_number_cases`:\n\n",
        df_to_markdown(common_df %>% dplyr::select(cellid, table_val, true_val_cand, issue, details))
      )
    }

    miss_str = ""
    if (NROW(ai_miss_df) > 0) {
      miss_str =  paste0("\n\n- **Mismatches Missed by AI:** Our heuristic found the following potential errors that the previous AI run did *not* report. Please review them. If they are genuine errors, add them to `wrong_number_cases` in your new mapping:\n\n",
        df_to_markdown(ai_miss_df %>% dplyr::select(cellid, table_val, true_val_cand, issue, details)))
    }

    extra_str = ""
    if (NROW(ai_extra_df) > 0) {
      extra_str =  paste0("\n\n- **Mismatches Only Found by AI:** The previous AI reported these errors, but our heuristic did *not* find them. Please double-check if these are real errors. If they are not, do not include them in `wrong_number_cases` this time:\n\n",
        df_to_markdown(ai_extra_df %>% dplyr::select(any_of(c("cellid", "wrong_number_in_cell", "number_in_stata_output")))))
    }

    # Only report if there is something to report
    if (nchar(common_str)>0 | nchar(miss_str)>0 | nchar(extra_str)>0) {
      descr = paste0("**Review of Potential Transcription Errors.** We have compared the numeric mismatches found by our heuristic checks against those reported in the previous AI run. Please review the lists below.", common_str, miss_str, extra_str)
      note_test(test_name, descr, NULL)
    } else if (NROW(df) > 0) {
       # Perfect agreement and errors were found
       descr = "**Mismatch Detection Agreement.** Our heuristic checks confirm all the potential transcription errors reported by the previous AI run. Please review them and include them in `wrong_number_cases`:"
       note_test(test_name, descr, df)
    }
  }

  # Check: single_col_reg
  test_name = "single_col_reg"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Regression Spans Multiple Columns.** Regressions are typically presented in a single column. This test flags regressions whose mapped cells span multiple columns without a clear structural reason (like having standard errors in an adjacent column). This often indicates that cells from different regressions have been incorrectly grouped together.
    descr = "**Regression Spans Multiple Columns.** A single regression has been mapped to cells in multiple columns, which is unusual but can happen in some cases. E.g.

  - SEs are in an adjacent column instead of below the coefficient.

  - If a balancing table is generated using regressions sometimes a regression spans one or two rows instead of a column.

  But sometimes having mapped cells from multiple columns for a regression can indicated an mistake in the mapping, so please check again."
    note_test(test_name, descr, df)
  }

  # Check: multicol_reg_plausibility
  test_name = "multicol_reg_plausibility"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Implausible Multi-Column Structure.** For a regression that legitimately spans multiple columns, we expect to find rows with numbers in more than one of those columns. This test flags multi-column regressions where every row *only* has a value in one column, suggesting a 'slip' where different rows of the same conceptual regression were incorrectly assigned to different columns.
    descr = "**Implausible Multi-Column Structure.** A regression spans multiple columns, but each row only has a value in one of those columns. This typcially suggests an inconsistent mapping across rows, but there might be exceptions."
    note_test(test_name, descr, df)
  }

  # Check: overlapping_regs
  test_name = "overlapping_regs"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    descr = "**Overlapping Regression Mappings.** A single cell (identified as a coefficient) has been mapped to more than one regression. This can well indicate a mapping error."
    note_test(test_name, descr, df)
  }

  # Check: missing_se_mapping
  test_name = "missing_se_mapping"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Unmapped Standard Error.** This test flags cases where a mapped coefficient cell has an associated standard error (a value in parentheses, typically below the coefficient) that was *not* included in the regression mapping. It also reports whether the numeric value of that unmapped SE would have been a correct match for the regression's output, helping to distinguish simple mapping omissions from more complex issues.
    descr = "**Unmapped Standard Error.** A coefficient was mapped, but its associated standard error (or t-value, p-value ..., we mean the value in parentheses) was not included in the mapping. Check if it should be added."
    note_test(test_name, descr, df)
  }

  # Check: consistent_regid_in_col
  test_name = "consistent_regid_in_col"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Inconsistent `regid` in Column.** In a column that maps to a single main regression, all mapped cells (including post-estimation stats) should share the same `regid`. This check flags columns where cells are assigned to multiple different `regid`s, suggesting a mapping inconsistency.
    descr = "**Inconsistent `regid` in Column.** Within a single table column that seems to represent one regression, cells have been mapped to different regression indices (`regid`). This can be correct if a column contains results from multiple regressions, but often indicates a mapping error. Please review."
    note_test(test_name, descr, df)
  }

  # Check: post_est_reg_match
  test_name = "post_est_reg_match"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Mismatched Post-Estimation Command.** In a column corresponding to a single regression, any post-estimation commands (e.g., `test`, `summarize`) should be associated with that main regression. This check flags cases where a post-estimation command's associated `reg_runid` does not match the main regression's `runid` for that column.
    descr = "**Mismatched Post-Estimation Command.** A post-estimation command (like a test statistic) in a regression column appears to be linked to the wrong regression run. Our heuristic check which post-regression runid is closest to the runid of the mapped regression in the table column. But in particular if there are multiple regressions in a table column, the heuristic may be wrong. But take a look it."
    note_test(test_name, descr, df)
  }

  # Check: value_match_other_cmd
  test_name = "value_match_other_cmd"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Value Mismatch for Other Commands.** This check verifies if numeric values in cells mapped to non-regression commands (e.g., `summarize`, `display`) can be found in the raw output of that command. It checks for a match after rounding the output values to the same number of decimal places as the table value.
    descr = "**Value Mismatch for Other Commands.** The value in a cell mapped to a non-regression command (like `summarize`) was not found in the command's Stata output. This could be a mapping error (the cell is linked to the wrong command) or a transcription error in the table. If it is a transcription error, please add it to the `wrong_number_cases` array for the correct mapping entry."
    note_test(test_name, descr, df)
  }

  # Check: mapped_to_line_not_run
  test_name = "mapped_to_line_not_run"
  df = get_test_df(test_name)
  if (NROW(df) > 0) {
    # long_descr = **Mapped to Code Line but not a `runid`.** This flags cells that are mapped to a specific line in a script but are not associated with a `runid` (i.e., a specific execution output). While sometimes intentional for un-executed code, numeric cells in a results table should be mapped to a `runid`. If a numeric cell cannot be mapped to a specific `runid`, both `runid` and `code_line` should ideally be to indicate it belongs to the conceptual regression without a direct code link.
    descr = "**Mapped to Code Line but not a `runid`.** Cells are mapped to a line of code, but not to a specific output run (`runid`). Numeric results should generally be linked to a `runid` if the outut is shown. The previous AI run might have not yet seen the Stata output and runid, but now we have added every Stata output from the code lines listed below. Make sure to add for those cases the runid now, if you think the mapping is still valid after seeing the output. Otherwise modify the mapping."
    note_test(test_name, descr, df)
  }


  if (num_tests == 0) table_str = "-- No issues found --"

  # The check consistent_vertical_structure is omitted as it is less about a single
  # mapping's correctness and more about overall table consistency.

  return(table_str)
}
```
# END OF FILE: patch_prompt.R

-----------------------------------------------------------


# FILE: rme_cell_hx.R
```
first_or_na = function(x, na_val=NA_character_) {
  if (NROW(x)==0) return(na_val)
  return(first(x))
}

#' Add cell type info (coef, se) to cell_df
#'
#' This function applies heuristics to identify which cells in a table
#' likely represent coefficients and which represent their standard errors,
#' t-stats, or p-values, which are typically in parentheses.
#' @param rme The rme object.
#' @return The rme object with an augmented cell_df.
rme_add_cell_reg_info = function(rme) {
  restore.point("rme_add_cell_reg_info")
  cell_df = rme$cell_df

  # Initialize columns
  cell_df$reg_role = NA_character_
  cell_df$partner_cellid = NA_character_
  cell_df$se_position = NA_character_
  cell_df$is_paren = stringi::stri_detect_fixed(cell_df$text, "(") | stringi::stri_detect_fixed(cell_df$text, "[")

  # Heuristic 1: Value in parenthesis is below a non-parenthesis value
  # This suggests a coef-se pair stacked vertically.
  df = cell_df %>%
    filter(has_num) %>%
    select(tabid, cellid, row, col, is_paren, text) %>%
    group_by(tabid, col) %>%
    arrange(row) %>%
    mutate(
      is_se_below = is_paren & lag(row) == row - 1 & !lag(is_paren, default=FALSE),
      coef_cellid_below = if_else(is_se_below, lag(cellid), NA_character_)
    ) %>%
    ungroup()

  # Heuristic 2: Value in parenthesis is to the right of a non-parenthesis value
  # This suggests a coef-se pair side-by-side.
  df = df %>%
    group_by(tabid, row) %>%
    arrange(col) %>%
    mutate(
      is_se_right = is_paren & lag(col) == col - 1 & !lag(is_paren, default=FALSE),
      coef_cellid_right = if_else(is_se_right, lag(cellid), NA_character_)
    ) %>%
    ungroup()

  # Combine heuristics
  # Priority to "below" as it's a more common table format.
  res_df = df %>%
    mutate(
      partner_cellid = dplyr::coalesce(coef_cellid_below, coef_cellid_right),
      se_position = dplyr::case_when(
        !is.na(coef_cellid_below) ~ "below",
        !is.na(coef_cellid_right) ~ "right",
        TRUE ~ NA_character_
      ),
      reg_role = if_else(!is.na(partner_cellid), "se", NA_character_)
    ) %>%
    select(cellid, reg_role, partner_cellid, se_position) %>%
    filter(!is.na(reg_role))

  # Mark the corresponding coefficients
  coef_df = res_df %>%
    filter(!is.na(partner_cellid)) %>%
    select(.cellid = partner_cellid, partner_cellid=cellid) %>%
    rename(cellid=.cellid) %>%
    distinct() %>%
    mutate(reg_role = "coef")

  # Join the identified roles back to the main cell_df
  roles_df = dplyr::bind_rows(res_df, coef_df) %>%
    # In case a cell is both a coef and an se (unlikely but possible), prioritize coef
    group_by(cellid) %>%
    summarise(
      reg_role = first_or_na(if_else("coef" %in% reg_role, "coef", "se")),
      partner_cellid = first_or_na(stats::na.omit(partner_cellid)),
      se_position = first_or_na(stats::na.omit(se_position))
    )

  cell_df = cell_df %>%
    select(-any_of(c("reg_role", "partner_cellid", "se_position"))) %>%
    left_join(roles_df, by = "cellid")

  # Fill other numeric cells
  cell_df = cell_df %>%
    mutate(reg_role = ifelse(has_num & is.na(reg_role), "other_num", reg_role))

  rme$cell_df = cell_df
  return(rme)
}
```
# END OF FILE: rme_cell_hx.R

-----------------------------------------------------------


# FILE: rme_cmd_type.R
```
# R/rme_cmd_type.R

#' Get the type of a Stata command
#' @param cmd A character vector of Stata command names.
#' @return A character vector of command types.
rme_get_cmd_type = function(cmd) {
  type = rep("unknown", length(cmd))
  # Standard regression commands
  reg_cmds = c("reg", "areg", "ivreg", "ivreg2", "xtreg", "xtivreg", "probit", "logit", "ols")
  type[cmd %in% reg_cmds] = "reg"
  # Other estimation commands
  altreg_cmds = c("rd", "gmm")
  type[cmd %in% altreg_cmds] = "altreg"
  # Post-estimation / test commands
  test_cmds = c("test", "testparm", "lincom", "margins", "estat")
  type[cmd %in% test_cmds] = "test"
  # Descriptive statistics
  descr_cmds = c("sum", "summarize", "tab", "tabulate")
  type[cmd %in% descr_cmds] = "descr"
  # Output commands
  output_cmds = c("display", "putexcel")
  type[cmd %in% output_cmds] = "output"
  return(type)
}


#' Adds command type and associated regression runid to run_df
#' @param rme The rme object.
#' @return The rme object with augmented run_df and cmd_df.
rme_add_cmd_info = function(rme) {
  restore.point("rme_add_cmd_info")

  run_df = rme$run_df %>%
    arrange(runid)

  # Get command type
  run_df$cmd_type = rme_get_cmd_type(run_df$cmd)

  # For non-regression commands, find the runid of the last preceding regression
  run_df = run_df %>%
    mutate(
      prev_reg_runid = if_else(is_reg, runid, NA_integer_)
    ) %>%
    tidyr::fill(prev_reg_runid, .direction = "down") %>%
    mutate(
      # The associated regression is the previous one, but not the command itself if it's a regression
      reg_runid = if_else(is_reg, NA_integer_, prev_reg_runid)
    ) %>%
    select(-prev_reg_runid)

  rme$run_df = run_df

  # Also add info to cmd_df
  if (!is.null(rme$cmd_df)) {
    rme$cmd_df = rme$cmd_df %>%
      mutate(cmd_type = rme_get_cmd_type(cmd))
  }

  return(rme)
}
```
# END OF FILE: rme_cmd_type.R

-----------------------------------------------------------


# FILE: rme_eval_coef_se.R
```
#' A function to group all value-based checks
#' @export
rme_steps_value = function() {
  c("coef_se_match")
}

#' Check for consistency between table coefficients/SEs and regcoef output
#'
#' This evaluation compares numeric values from cells identified as coefficients
#' and their associated standard errors (in parentheses) with the detailed
#' output from the `regcoef` parcel.
#'
#' It first finds the best combined match for each (coefficient, se) pair from the
#' table with a result from the corresponding regression output (`regcoef` parcel).
#' A match is considered "perfect" if the regression output value, when rounded to the
#' number of decimal places shown in the table, is identical to the table value.
#'
#' Based on the best match, it reports several types of issues:
#' - `no_coef_match`: The coefficient from the table does not match any coefficient
#'   in the corresponding `regcoef` output for that `runid` within a given tolerance.
#' - `no_match_perhaps_wrong_sign`: A coefficient match is only found if the sign of the
#'   table value is flipped. This suggests a potential transcription error (e.g., a missing minus sign).
#' - `no_paren_match`: The coefficient matched perfectly, but the value in parentheses
#'   does not match the corresponding standard error, t-statistic, or p-value.
#' - `rounding_error`: A match was found (i.e., not a perfect match after rounding), but the relative difference
#'   is smaller than a mismatch tolerance. This can point to minor transcription errors or unusual rounding.
#'
#' @param rme The rme object.
#' @param rel_tol_rounding The relative tolerance for what is considered a rounding error. (This is now superseded by decimal-based rounding but kept for legacy reasons).
#' @param rel_tol_mismatch The relative tolerance for what is considered a mismatch.
#' @return A data frame of identified issues.
rme_ev_coef_se_match = function(rme) {
  restore.point("rme_ev_coef_se_match")

  long_descr = "**Value Mismatch between Table and Code.** This is a core value-based check. It compares numeric values from the table (identified as coefficient/standard error pairs) against the results from the mapped regression's `regcoef` output. A match is considered perfect if the code output, rounded to the number of decimal places shown in the table, equals the table value."

  # 1. Get all coef-se pairs from the tables, including tabid and decimal places
  cell_info_df = rme$cell_df %>% select(cellid, num, num_deci)

  pairs_df = rme$cell_df %>%
    filter(reg_role == "se", !is.na(partner_cellid)) %>%
    select(tabid, se_cellid = cellid, coef_cellid = partner_cellid) %>%
    # Join for SE info (paren value)
    left_join(cell_info_df, by = c("se_cellid" = "cellid")) %>%
    rename(paren_val = num, paren_deci = num_deci) %>%
    # Join for Coef info
    left_join(cell_info_df, by = c("coef_cellid" = "cellid")) %>%
    rename(coef_val = num, coef_deci = num_deci)

  # 2. Join with mappings to get runids
  mc_coef_df = rme$mc_df %>%
    filter(cellid %in% pairs_df$coef_cellid) %>%
    select(map_version, coef_cellid = cellid, runid) %>%
    distinct()

  # 3. Combine pairs with their mappings
  mapped_pairs = pairs_df %>%
    inner_join(mc_coef_df, by = "coef_cellid", relationship = "many-to-many") %>%
    filter(!is.na(coef_val), !is.na(paren_val))

  if (NROW(mapped_pairs) == 0) {
    return(rme_df_descr(tibble::tibble(), "No coef/se pairs found to check.", test_type = "flag", long_descr = long_descr))
  }

  # 4. Get all relevant regcoef data
  if (is.null(rme$parcels$regcoef$regcoef)) {
     warning("`regcoef` parcel not found in rme object. Cannot run `rme_ev_coef_se_match`.")
     return(NULL)
  }
  regcoef_df = rme$parcels$regcoef$regcoef %>%
    select(runid, cterm, reg_coef = coef, reg_se = se, reg_t = t, reg_p = p) %>%
    filter(!is.na(runid))

  # 5. Create all potential matches between table pairs and regcoef entries
  all_potential_matches = mapped_pairs %>%
    left_join(regcoef_df, by = "runid", relationship = "many-to-many") %>%
    filter(!is.na(reg_coef))

  if (NROW(all_potential_matches) == 0) {
    return(rme_df_descr(tibble::tibble(), "No regcoef entries for mapped runids.", test_type = "flag", long_descr = long_descr))
  }

  # 6. Score each potential match
  matches_with_scores = all_potential_matches %>%
    mutate(
      # Default NA decimal places to 0 (for integers)
      coef_deci = tidyr::replace_na(coef_deci, 0),
      paren_deci = tidyr::replace_na(paren_deci, 0),

      # Check for perfect matches based on rounding to table's decimal places
      is_perfect_coef_match = (round(reg_coef, coef_deci) == coef_val),
      is_perfect_sign_flip_match = (round(reg_coef, coef_deci) == -coef_val),
      is_perfect_se_match = (round(reg_se, paren_deci) == paren_val),
      is_perfect_t_match = (round(reg_t, paren_deci) == paren_val),
      is_perfect_p_match = (round(reg_p, paren_deci) == paren_val),

      # Relative distances are still needed for reporting on misses
      rel_dist_coef = abs(reg_coef - coef_val) / pmax(abs(reg_coef), abs(coef_val), 1e-9),
      rel_dist_se = abs(reg_se - paren_val) / pmax(abs(reg_se), abs(paren_val), 1e-9),
      rel_dist_t = abs(reg_t - paren_val) / pmax(abs(reg_t), abs(paren_val), 1e-9),
      rel_dist_p = abs(reg_p - paren_val) / pmax(abs(reg_p), abs(paren_val), 1e-9)
    ) %>%
    rowwise() %>%
    mutate(
      min_rel_dist_paren = min(c(rel_dist_se, rel_dist_t, rel_dist_p), na.rm = TRUE),
      is_perfect_paren_match = any(c(is_perfect_se_match, is_perfect_t_match, is_perfect_p_match), na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      min_rel_dist_paren = ifelse(is.infinite(min_rel_dist_paren), NA, min_rel_dist_paren),
      # Score: 2 for perfect match, 1 for close match (rounding error), 0 for mismatch.
      # A "rounding error" is a non-perfect match where the absolute difference
      # is within 1.01 units of the last decimal place.
      is_rounding_coef = !is_perfect_coef_match & (abs(reg_coef - coef_val) < (1.01 * 10^(-coef_deci))),

      coef_match_quality = case_when(
        is_perfect_coef_match ~ 2,
        is_rounding_coef ~ 1,
        TRUE ~ 0
      ),

      # For parenthesis value, check if any candidate (se, t, p) qualifies as a rounding error.
      is_rounding_paren = !is_perfect_paren_match & any(
          (abs(reg_se - paren_val) < (1.01 * 10^(-paren_deci))),
          (abs(reg_t - paren_val) < (1.01 * 10^(-paren_deci))),
          (abs(reg_p - paren_val) < (1.01 * 10^(-paren_deci))),
          na.rm = TRUE
      ),

      paren_match_quality = case_when(
        is.na(min_rel_dist_paren) ~ 0,
        is_perfect_paren_match ~ 2,
        is_rounding_paren ~ 1,
        TRUE ~ 0
      ),
      match_score = coef_match_quality + paren_match_quality
    )

  # 7. For each table pair, find the best `regcoef` match
  best_matches = matches_with_scores %>%
    group_by(map_version, coef_cellid) %>%
    filter(match_score == max(match_score)) %>%
    mutate(combined_dist = rel_dist_coef + min_rel_dist_paren) %>%
    filter(combined_dist == min(combined_dist, na.rm=TRUE)) %>%
    slice(1) %>%
    ungroup()

  # Added 7b find best paren type over each table
  best_paren = best_matches %>%
    group_by(tabid) %>%
    summarize(
      score_se = sum(is_perfect_se_match),
      score_t = sum(is_perfect_t_match),
      score_p = sum(is_perfect_p_match)
    ) %>%
    mutate(
      paren_type = case_when(
        score_se >= pmax(score_t, score_p) ~ "se",
        score_t >= score_p ~ "t",
        TRUE ~ "p"
      )
    )
  best_matches = left_join(best_matches, select(best_paren, tabid, paren_type), by="tabid")
  # recompute match score and type for best_matches using actual paren_type

  best_matches = best_matches %>%
    mutate(
      is_perfect_paren_match = case_when(
        paren_type == "se" ~ is_perfect_se_match,
        paren_type == "t" ~ is_perfect_t_match,
        paren_type == "p" ~ is_perfect_p_match,
        TRUE ~ FALSE
      ),
      is_rounding_paren = !is_perfect_paren_match &  case_when(
        paren_type == "se" ~ abs(reg_se - paren_val) < (1.01 * 10^(-paren_deci)),
        paren_type == "t" ~  abs(reg_t - paren_val) < (1.01 * 10^(-paren_deci)),
        paren_type == "p" ~ abs(reg_p - paren_val) < (1.01 * 10^(-paren_deci)),
        TRUE ~ FALSE
      ),
      paren_match_quality = case_when(
        is.na(min_rel_dist_paren) ~ 0,
        is_perfect_paren_match ~ 2,
        is_rounding_paren ~ 1,
        TRUE ~ 0
      ),
      match_score = coef_match_quality + paren_match_quality
    )

  # temp = best_matches %>%
  #   filter(!is_perfect_paren_match) %>%
  #   select(tabid, se_cellid, coef_cellid, paren_val, paren_deci, reg_se, is_rounding_paren, is_perfect_paren_match, everything())
  #
  #
  # abs(temp$reg_se[1] - temp$paren_val[1])[1] < (1.01 * 10^(-temp$paren_deci))[1]

  # 8. Generate issues based on the quality of the best match
  issues_from_best = best_matches %>%
    filter(match_score < 4 & match_score > 0) %>% # Imperfect but not total failure
    rowwise() %>%
    mutate(
      best_paren_type = c("se", "t", "p")[which.min(c(rel_dist_se, rel_dist_t, rel_dist_p))],
      best_paren_val = get(paste0("reg_", best_paren_type))
    ) %>%
    ungroup() %>%
    mutate(
      is_sign_flip_match = (coef_match_quality == 0) & is_perfect_sign_flip_match,
      issue = case_when(
        coef_match_quality == 0 & paren_match_quality == 0 ~ "no_coef_paren_match",
        is_sign_flip_match ~ "no_coef_match_perhaps_wrong_sign",
        coef_match_quality == 0 ~ "no_coef_match",
        paren_match_quality == 0 ~ "no_paren_match",
        coef_match_quality == 1 & paren_match_quality == 1 ~ "coef_paren_rounding_error",
        coef_match_quality == 1 ~ "coef_rounding_error",
        paren_match_quality == 1 ~ "paren_rounding_error",
        TRUE ~ NA_character_
      ),
      issue_cellid = if_else(coef_match_quality < 2, coef_cellid, se_cellid),
      partner_cellid = if_else(issue_cellid == coef_cellid, se_cellid, coef_cellid),
      table_val = if_else(issue_cellid == coef_cellid, coef_val, paren_val),
      true_val_cand = if_else(issue_cellid == coef_cellid, reg_coef, best_paren_val),
      details = case_when(
        is_sign_flip_match ~ paste0("Perfect match if sign is flipped. Paren match quality: ", paren_match_quality),
        issue_cellid == coef_cellid ~ paste0("Coef rel diff ", round(rel_dist_coef,3), ". Paren match quality: ", paren_match_quality),
        TRUE ~ paste0("Paren rel diff ", round(min_rel_dist_paren,3)," to ", best_paren_type, " ", round(best_paren_val, 4))
      )
    ) %>%
    select(map_version, tabid, runid, cellid = issue_cellid, partner_cellid, issue, table_val, true_val_cand, details)

  # 9. Identify pairs that had no match at all (max score was 0)
  all_pairs_to_check = mapped_pairs %>% select(map_version, coef_cellid) %>% distinct()
  pairs_with_any_match = best_matches %>% filter(match_score > 0) %>% select(map_version, coef_cellid) %>% distinct()
  no_match_pairs = all_pairs_to_check %>% anti_join(pairs_with_any_match, by = c("map_version", "coef_cellid"))

  no_match_issues = tibble::tibble()
  if(NROW(no_match_pairs) > 0) {
    closest_misses = no_match_pairs %>%
      inner_join(matches_with_scores, by = c("map_version", "coef_cellid")) %>%
      mutate(combined_dist = rel_dist_coef + min_rel_dist_paren) %>%
      group_by(map_version, coef_cellid) %>%
      filter(combined_dist == min(combined_dist, na.rm=TRUE)) %>%
      slice(1) %>%
      ungroup()

    no_match_issues = closest_misses %>%
      mutate(
        is_sign_flip_match = is_perfect_sign_flip_match,
        issue = if_else(is_sign_flip_match, "no_match_perhaps_wrong_sign", "no_coef_match"),
        partner_cellid = se_cellid,
        table_val = coef_val,
        true_val_cand = reg_coef,
        details = if_else(is_sign_flip_match,
                         paste0("Perfect match if sign is flipped. Paren rel diff ", round(min_rel_dist_paren,3)),
                         paste0("No match found. Closest coef rel diff ", round(rel_dist_coef,3),
                                ", closest paren rel diff ", round(min_rel_dist_paren,3)))
      ) %>%
      select(map_version, tabid, runid, cellid = coef_cellid, partner_cellid, issue, table_val, true_val_cand, details)
  }

  issues = dplyr::bind_rows(issues_from_best, no_match_issues) %>%
    left_join(rme$mc_df %>% select(map_version, regid, cellid, runid) %>% unique(), by= c("map_version","cellid","runid"))
  return(rme_df_descr(issues, "Issues from comparing table coef/se values with regcoef output.", test_type = "flag", long_descr = long_descr))
}
```
# END OF FILE: rme_eval_coef_se.R

-----------------------------------------------------------


# FILE: rme_eval_other.R
```
# R/rme_eval_other.R

# This file implements additional checks that are more specific
# or cross-cutting than the standard integrity, value, and structure checks.

#' A function to group all "other" checks
#' @export
rme_steps_other = function() {
  c("consistent_regid_in_col", "post_est_reg_match", "value_match_other_cmd", "mapped_to_line_not_run")
}

#' Check for consistent regid in single-regression columns.
#'
#' In a column that maps to a single main regression, all mapped cells
#' (including post-estimation stats) should share the same `regid`. This check
#' flags columns where cells are assigned to multiple different `regid`s,
#' suggesting a mapping inconsistency.
#'
#' @param rme The rme object.
#' @return A data frame of identified issues.
rme_ev_consistent_regid_in_col = function(rme) {
  restore.point("rme_ev_consistent_regid_in_col")

  long_descr = "**Inconsistent `regid` in Column.** In a column that maps to a single main regression, all mapped cells (including post-estimation stats) should share the same `regid`. This check flags columns where cells are assigned to multiple different `regid`s, suggesting a mapping inconsistency."

  mc_df = rme$mc_df

  # 1. Identify single-regression columns and their "correct" regid
  single_reg_cols = mc_df %>%
    filter(is_reg == TRUE) %>%
    group_by(map_version, tabid, col) %>%
    summarise(
      main_regids = list(sort(unique(regid))),
      .groups = "drop"
    ) %>%
    filter(lengths(main_regids) == 1) %>%
    mutate(correct_regid = unlist(main_regids)) %>%
    select(map_version, tabid, col, correct_regid)

  if (NROW(single_reg_cols) == 0) {
    return(rme_df_descr(tibble::tibble(), "No single-regression columns found to check.", test_type = "flag", long_descr = long_descr))
  }

  # 2. Find all cells in these columns that have an inconsistent regid
  issues = mc_df %>%
    inner_join(single_reg_cols, by = c("map_version", "tabid", "col")) %>%
    filter(regid != correct_regid) %>%
    group_by(map_version, tabid, col, correct_regid, regid) %>%
    summarise(
        cellids = paste(sort(unique(cellid)), collapse=","),
        .groups = "drop"
    ) %>%
    mutate(
        details = paste0("Has regid ", regid, " but should be ", correct_regid)
    ) %>%
    select(map_version, tabid, col, regid, cellids, details) %>%
    rme_df_descr("Inconsistent `regid` in single-regression columns.", test_type = "flag", long_descr = long_descr)

  return(issues)
}

#' Check for consistent reg_runid for post-estimation commands.
#'
#' In a column corresponding to a single regression, any post-estimation
#' commands should be associated with that main regression. This check flags
#' cases where a post-estimation command's associated `reg_runid` does not
#' match the main regression `runid` for that column.
#'
#' @param rme The rme object.
#' @return A data frame of identified issues.
rme_ev_post_est_reg_match = function(rme) {
  restore.point("rme_ev_post_est_reg_match")

  long_descr = "**Mismatched Post-Estimation Command.** In a column corresponding to a single regression, any post-estimation commands (e.g., `test`, `summarize`) should be associated with that main regression. This check flags cases where a post-estimation command's associated `reg_runid` does not match the main regression's `runid` for that column."

  mc_df = rme$mc_df

  # 1. Identify single-regression columns and their main runid
  single_reg_cols = mc_df %>%
    filter(is_reg == TRUE) %>%
    group_by(map_version, tabid, col) %>%
    summarise(main_reg_runids = list(sort(unique(runid))), .groups = "drop") %>%
    filter(lengths(main_reg_runids) == 1) %>%
    mutate(expected_reg_runid = unlist(main_reg_runids)) %>%
    select(map_version, tabid, col, expected_reg_runid)

  if (NROW(single_reg_cols) == 0) {
    return(rme_df_descr(tibble::tibble(), "No single-regression columns found to check.", test_type = "flag", long_descr = long_descr))
  }

  # 2. Find non-regression commands in these columns and check their associated reg_runid
  issues = mc_df %>%
    filter(is_reg == FALSE, !is.na(reg_runid)) %>%
    inner_join(single_reg_cols, by = c("map_version", "tabid", "col")) %>%
    filter(reg_runid != expected_reg_runid) %>%
    select(
      map_version, tabid, col, cellid, runid, cmd,
      found_reg_runid = reg_runid,
      expected_reg_runid
    ) %>%
    rme_df_merge_cellids() %>%
    rme_df_descr("Post-estimation command associated with the wrong regression.", test_type = "flag", long_descr = long_descr)

  return(issues)
}

#' Helper to extract all numbers from a string
extract_numbers = function(text) {
  restore.point("extract_numbers")
  text_no_comma = stringi::stri_replace_all_fixed(text, ",", "")
  num_pattern = "[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?"
  matches = stringi::stri_extract_all_regex(text_no_comma, num_pattern, omit_no_match = TRUE)
  lapply(matches, function(m) as.numeric(m))
}

#' Check if values from non-regression commands match Stata output.
#'
#' Verifies if numeric values in cells mapped to non-regression commands
#' (e.g., `summarize`, `display`) can be found in the raw output of that command.
#' It rounds output values to the same number of decimal places as the table value.
#'
#' @param rme The rme object.
#' @return A data frame of identified issues.
rme_ev_value_match_other_cmd = function(rme) {
  restore.point("rme_ev_value_match_other_cmd")

  long_descr = "**Value Mismatch for Other Commands.** This check verifies if numeric values in cells mapped to non-regression commands (e.g., `summarize`, `display`) can be found in the raw output of that command. It checks for a match after rounding the output values to the same number of decimal places as the table value."

  if (is.null(rme$parcels$stata_run_log$stata_run_log)) {
    warning("`stata_run_log` parcel not found. Cannot run `rme_ev_value_match_other_cmd`.")
    return(rme_df_descr(tibble::tibble(), "`stata_run_log` parcel not found.", test_type = "flag", long_descr = long_descr))
  }
  output_df = rme$parcels$stata_run_log$stata_run_log %>% select(runid, output = logtxt)

  cells_to_check = rme$mc_df %>%
    filter(!cmd_type %in% c("reg","altreg")) %>%
    select(map_version, tabid, cellid, runid, cmd, table_val = num, table_deci = num_deci) %>%
    distinct()

  if (NROW(cells_to_check) == 0) {
    return(rme_df_descr(tibble::tibble(), "No numeric cells mapped to non-regression commands.", test_type = "flag", long_descr = long_descr))
  }

  cells_with_output = cells_to_check %>%
    left_join(output_df, by = "runid") %>%
    filter(!is.na(output) & output != "")

  results = cells_with_output %>%
    mutate(
      table_deci = tidyr::replace_na(table_deci, 0),
      output_numbers = extract_numbers(output)
    ) %>%
    rowwise() %>%
    mutate(
      #out_num = list(round(unlist(output_numbers),table_deci)),
      is_match = any(round(unlist(output_numbers), table_deci) == table_val, na.rm = TRUE)
    ) %>%
    ungroup()

  mismatch_issues = results %>%
    filter(!is_match) %>%
    rowwise() %>%
    mutate(
      diffs = abs(output_numbers[[1]] - table_val),
      closest_val = if (length(diffs) > 0 && any(!is.na(diffs))) output_numbers[[1]][which.min(diffs)] else NA_real_,
      details = if (!is.na(closest_val)) {
        paste0("No match. Closest value in output: ", closest_val)
      } else {
        "No numeric values found in output."
      }
    ) %>%
    ungroup() %>%
    select(map_version, tabid, cellid, runid, cmd, table_val, details) %>%
    mutate(issue = "no_value_match")

  no_output_issues = cells_to_check %>%
    anti_join(cells_with_output, by = c("map_version", "tabid", "cellid", "runid")) %>%
     mutate(details = paste0("No Stata output found for runid ", runid), issue="no_output_found") %>%
     select(map_version, tabid, cellid, runid, cmd, table_val, details, issue)

  issues = dplyr::bind_rows(mismatch_issues, no_output_issues) %>%
     rme_df_descr("Numeric value in table not found in command output.", test_type = "flag", long_descr = long_descr)

  return(issues)
}

#' Check for cells mapped to a code line but not a runid.
#'
#' This check flags cells that are mapped to a specific line in a script but
#' are not associated with a `runid` (i.e., a specific execution output).
#'
#' @param rme The rme object.
#' @return A data frame of identified issues.
rme_ev_mapped_to_line_not_run = function(rme) {
  restore.point("rme_ev_mapped_to_line_not_run")

  long_descr = "**Mapped to Code Line but not a `runid`.** This flags cells that are mapped to a specific line in a script but are not associated with a `runid` (i.e., a specific execution output). While sometimes intentional for un-executed code, numeric cells in a results table should be mapped to a `runid`. If a numeric cell cannot be mapped to a specific `runid`, both `runid` and `code_line` should ideally be to indicate it belongs to the conceptual regression without a direct code link."
  #map_reg_run = rme$map_reg_run

  issues = rme$map_reg_run %>%
    filter(is.na(runid) & !is.na(code_line)) %>%
    select(map_version = ver_id, tabid, regid, code_line, script_num, cell_ids) %>%
    filter(!is.na(cell_ids) & cell_ids != "") %>%
    unnest_comma_string_col("cell_ids") %>%
    rename(cellid = cell_ids) %>%
    group_by(map_version, tabid, regid, code_line, script_num) %>%
    summarise(cellids = paste(sort(unique(cellid)), collapse=","), .groups="drop") %>%
    rename(mapped_code_line = code_line, mapped_script_num=script_num) %>%
    rme_df_descr("Cells mapped to a code line but not a `runid`.", test_type = "flag", long_descr = long_descr)

  return(issues)
}
```
# END OF FILE: rme_eval_other.R

-----------------------------------------------------------


# FILE: rme_eval.R
```
# This file implements Module A: Integrity and Sanity Checks,
# as described in the project's plan.md.

example = function() {
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  #rme = rme_load(project_dir)
  rme = rme_init(project_dir)
  names(rme)
  rme$map_versions

  rme = rme_add_eval(rme, rme_steps_all())

  rme_print_ev_report(rme)
  df = rme_combine_ev_df(rme)
}



rme_all_ev_funs = function() {
  funs = getNamespaceExports("repboxRegmap")
  funs[startsWith(funs,"rme_ev_")]
}


#' Get all evaluation step names
#' @export
rme_steps_all = function() {
  all_steps = c(
    rme_steps_integrity(),
    rme_steps_value(),
    rme_steps_structure(),
    rme_steps_other()
  )
  return(all_steps)

  funs = rme_all_ev_funs()
  all_steps2 = stringi::stri_sub(funs, 8)
  list(
    missing = setdiff(all_steps2, all_steps),
    extra = setdiff(all_steps, all_steps2)
  )
  all_steps
}

#' Run all evaluation steps
#'
#' A convenience wrapper to run all available evaluation checks.
#' @param rme The rme object.
#' @param ... Arguments passed on to the individual evaluation functions.
#' @return The rme object with all evaluation results.
#' @export
rme_eval_all = function(rme, ...) {
  restore.point("rme_eval_all")
  rme = rme_add_eval(rme, rme_steps_all(), ...)
  rme
}


rme_steps_integrity = function() {
  c("runids_differ", "invalid_runids", "invalid_cellids", "non_reg_cmd")
}


# Find cellids where different map versions point to different runids
rme_ev_runids_differ = function(rme) {
  mc_df = rme$mc_df

  # we store a string of all corresponding runid for a cell-map_version
  df_runs = mc_df %>%
    dplyr::group_by(map_version, tabid, cellid) %>%
    dplyr::summarize(
      runids = paste0(sort(unique(runid)), collapse=","),
      .groups = "drop"
    ) %>%
    dplyr::arrange(tabid, cellid, map_version)

  # Find cellids where there is more than one unique runids string
  discrepancy_df = df_runs %>%
    dplyr::group_by(tabid, cellid) %>%
    dplyr::filter(dplyr::n_distinct(runids) > 1) %>%
    dplyr::ungroup()

  # The returned df has columns: map_version, tabid, cellid, runids
  # and contains all map versions for cells that have discrepancies.
  # The reporting function will use this to generate per-version reports.
  discrepancy_df %>%
    rme_df_descr("cellids where map versions map to different runids",
                 test_type = "discrepancy",
                 long_descr = "**Discrepancy Across Map Versions.** This test identifies cells that are mapped to *different* regression `runid`s by different AI mapping versions. This is a key indicator of disagreement between models and points to areas of uncertainty.")
}

rme_ev_invalid_cellids = function(rme) {
  restore.point("eval_check_invalid_cellids")
  mc_df = rme$mc_df
  cell_df = rme$cell_df

  inv_df = mc_df %>%
    anti_join(cell_df, by="cellid") %>%
    select(map_version, tabid, regid, cellid)  %>%
    rme_df_merge_cellids() %>%
    rme_df_descr("map versions with invalid cellids (cellids not in cell_df)", test_type = "flag",
                 long_descr = "**Invalid `cellid` Mapping.** This test flags mappings that reference a `cellid` that does not exist in the parsed table data (`cell_df`). This is a critical integrity error, indicating a hallucinated or malformed cell reference from the AI.")
  return(inv_df)
}

# Check for mappings to runids that do not exist
rme_ev_invalid_runids = function(rme) {
  restore.point("eval_check_invalid_runids")
  df = rme$mc_df %>%
    filter(!is.na(runid)) %>%
    filter(!runid %in% rme$run_df$runid) %>%
    select(map_version, tabid,regid, cellid, runid=runid) %>%
    rme_df_merge_cellids() %>%
    rme_df_descr("mapped to non-existent runid", test_type = "flag",
                 long_descr = "**Invalid `runid` Mapping.** This test flags mappings that point to a `runid` that does not exist in the project's execution log (`run_df`). This is a critical integrity error, as the mapped regression output cannot be found.")
}

# Check for mappings to commands that are not regressions
rme_ev_non_reg_cmd = function(rme) {
  restore.point("eval_check_non_reg_cmd")
  df = rme$mc_df %>%
    # is_reg is pre-computed and joined into mc_df
    filter(!is.true(is_reg)) %>%
    select(map_version, tabid, regid, cellid, runid, cmd, cmd_type, reg_runid) %>%
    rme_df_merge_cellids() %>%
    rme_df_descr("Mapped to a cmd that is not a regcmd. This is not neccessarily a wrong mapping but useful information, e.g. for post-estimation commands.",
                 test_type = "note_flag",
                 long_descr = "**Mapping to Non-Regression Command.** This test identifies cells mapped to a Stata command that is not a primary regression command (e.g., `test`, `margins`, `summarize`). This is not necessarily an error—post-estimation results are often included in tables—but serves as an important note. The report shows the command type and the `runid` of the last preceding regression.")
  df
}
```
# END OF FILE: rme_eval.R

-----------------------------------------------------------


# FILE: rme_eval_structure.R
```
# R/rme_eval_structure.R

# This file implements structural checks on the table mappings.

#' A function to group all structural checks
#' @export
rme_steps_structure = function() {
  c("single_col_reg", "multicol_reg_plausibility", "overlapping_regs", "consistent_vertical_structure", "missing_se_mapping")
}

#' Check if regressions span multiple columns without justification.
#'
#' A regression should ideally be in a single column. An exception is when
#' standard errors (or t-stats/p-values) are placed in the column to the
#' right of the coefficient. This check flags regressions that span multiple
#' columns without such a horizontal "coef-se" structure.
#'
#' @param rme The rme object.
#' @return A data frame of regressions violating the single-column principle,
#'   including a `cellids` column with all cells of the regression.
rme_ev_single_col_reg = function(rme) {
  restore.point("rme_ev_single_col_reg")

  long_descr = "**Regression Spans Multiple Columns.** Regressions are typically presented in a single column. This test flags regressions whose mapped cells span multiple columns without a clear structural reason (like having standard errors in an adjacent column). This often indicates that cells from different regressions have been incorrectly grouped together."

  df = rme$mc_df %>%
    group_by(map_version, tabid, regid) %>%
    summarise(
      cellids = paste(sort(unique(cellid)), collapse=","),
      num_cols = n_distinct(col),
      has_right_se = any(se_position == "right", na.rm = TRUE),
      .groups = "drop"
    ) %>%
    # allow multi-column regressions
    # if all regressions in table are multi-column
    # e.g. the case in balancing tables
    group_by(map_version, tabid) %>%
    mutate(min_tab_num_cols = max(1, min(num_cols))) %>%
    filter(num_cols > min_tab_num_cols & !has_right_se) %>%
    rme_df_descr("Regressions spanning multiple columns without horizontal coef-se pairs.", test_type = "flag", long_descr = long_descr)

  return(df)
}

#' Plausibility check for multi-column regressions.
#'
#' If a regression is correctly mapped to multiple columns, it's expected that
#' for at least one row, there are values from that regression in multiple columns.
#' This check flags cases where a regression spans multiple columns, but each row
#' only contains values in a single column, suggesting a "slip" in column
#' assignment across rows.
#'
#' @param rme The rme object.
#' @return A data frame of implausible multi-column regressions, including a
#'   `cellids` column.
rme_ev_multicol_reg_plausibility = function(rme) {
  restore.point("rme_ev_multicol_reg_plausibility")

  long_descr = "**Implausible Multi-Column Structure.** For a regression that legitimately spans multiple columns, we expect to find rows with numbers in more than one of those columns. This test flags multi-column regressions where every row *only* has a value in one column, suggesting a 'slip' where different rows of the same conceptual regression were incorrectly assigned to different columns."

  # Identify regressions mapped to more than one column and get their cellids
  multicol_regs = rme$mc_df %>%
    group_by(map_version, tabid, regid) %>%
    filter(has_num, n_distinct(col) > 1) %>%
    summarise(
      cellids = paste(sort(unique(cellid)), collapse=","),
      cols = paste(sort(unique(col)), collapse=","),
      rows = paste(sort(unique(row)), collapse=","),
      .groups="drop")

  if (NROW(multicol_regs) == 0) {
    return(rme_df_descr(tibble::tibble(), "No multi-column regressions to check.", test_type = "flag", long_descr = long_descr))
  }

  # Find regressions where no row has numbers in more than one column
  plausibility_check = rme$mc_df %>%
    semi_join(multicol_regs, by = c("map_version", "tabid", "regid")) %>%
    filter(has_num) %>%
    group_by(map_version, regid, tabid, row) %>%
    summarise(cols_in_row = n_distinct(col), .groups = "drop_last") %>%
    summarise(max_cols_per_row = max(cols_in_row), .groups = "drop") %>%
    filter(max_cols_per_row == 1)

  if (NROW(plausibility_check) == 0) {
    return(rme_df_descr(tibble::tibble(), "No implausible multi-column regressions found.", test_type = "flag", long_descr = long_descr))
  }

  # Join the issues with the cellids
  issues = multicol_regs %>%
    inner_join(plausibility_check, by = c("map_version", "tabid", "regid")) %>%
    rme_df_descr("Multi-column regressions where no row has values in more than one column.", test_type = "flag", long_descr = long_descr)

  return(issues)
}


#' Check for overlapping coefficient cells.
#'
#' This check identifies cells that are heuristically classified as coefficients
#' and are mapped to more than one regression `regid` or multiple `runid`
#' within the same table. This indicates an overlap, which is usually an error.
#'
#' @param rme The rme object.
#' @return A data frame of cells with overlapping coefficient mappings.
rme_ev_overlapping_regs = function(rme) {
  restore.point("rme_ev_overlapping_regs")

  long_descr = "**Overlapping Regression Mappings.** This test flags cells identified as coefficients that have been mapped to *more than one* regression within the *same* map version. This is almost always an error, as a single coefficient should belong to only one regression specification."

  df = rme$mc_df %>%
    filter(reg_role == "coef") %>%
    group_by(map_version, tabid, cellid) %>%
    summarise(
      n_reginds = n_distinct(regid),
      regids = paste0(sort(unique(regid)), collapse=","),
      n_runids = n_distinct(runid),
      runids = paste0(sort(unique(runid)), collapse=","),
      .groups = "drop"
    ) %>%
    filter(n_runids > 1 | n_reginds > 1) %>%
    rme_df_descr("Coefficient cells mapped to multiple regressions.", test_type = "flag", long_descr = long_descr)

  return(df)
}

#' Classify table rows based on stub column keywords.
#' @param cell_df The `cell_df` from an rme object.
#' @return The `cell_df` with an added `row_class` column.
rme_add_row_class = function(cell_df) {
  restore.point("rme_add_row_class")

  if ("row_class" %in% names(cell_df)) return(cell_df)

  stub_df = cell_df %>%
    filter(col == 1 | (col > 1 & lag(col, default=0) == 0) ) %>% # Stub is often col 1
    mutate(text_clean = tolower(stringi::stri_replace_all_regex(text, "[^a-z0-9]", "")))

  stub_df = stub_df %>%
    mutate(
      row_class = dplyr::case_when(
        stringi::stri_detect_regex(text_clean, "observ|nobs|sampsize|numofobs") ~ "nobs",
        stringi::stri_detect_regex(text_clean, "r2|rsquared|rsq") ~ "r2",
        stringi::stri_detect_regex(text_clean, "fstat") ~ "fstat",
        TRUE ~ NA_character_
      )
    ) %>%
    select(tabid, row, row_class) %>%
    filter(!is.na(row_class)) %>%
    distinct(tabid, row, .keep_all = TRUE)

  cell_df = cell_df %>%
    left_join(stub_df, by = c("tabid", "row"))

  return(cell_df)
}

#' Check for consistent vertical structure across regressions.
#'
#' This check verifies that rows for common statistics (like N. of observations,
#' R-squared) appear consistently at the same row index across a table. It first
#' classifies rows based on keywords in the stub column.
#'
#' @param rme The rme object.
#' @return A data frame of inconsistencies found, including a `cellids` column.
rme_ev_consistent_vertical_structure = function(rme) {
  restore.point("rme_ev_consistent_vertical_structure")

  long_descr = "**Inconsistent Summary Stat Rows.** This test checks for consistent table structure. It identifies summary statistics (like 'Observations' or 'R-squared') by keywords and flags cases where the same statistic appears on different row numbers across the columns of a single table. This points to a potentially messy or inconsistent table layout or a mapping error."

  # Add row class if not already present
  if (!"row_class" %in% names(rme$cell_df)) {
    rme$cell_df = rme_add_row_class(rme$cell_df)
  }

  mc_df_ext = rme$mc_df %>%
    left_join(rme$cell_df %>% select(cellid, row_class), by = "cellid") %>%
    filter(!is.na(row_class))

  if (NROW(mc_df_ext) == 0) {
    return(rme_df_descr(tibble::tibble(), "No classified rows (e.g., nobs, r2) found to check for consistency.", test_type = "flag", long_descr = long_descr))
  }

  issues = mc_df_ext %>%
    group_by(map_version, tabid, row_class) %>%
    summarise(
      n_diff_rows = n_distinct(row),
      rows_used = paste0(sort(unique(row)), collapse = ","),
      cellids = paste0(sort(unique(cellid)), collapse = ","),
      .groups = "drop"
    ) %>%
    filter(n_diff_rows > 1) %>%
    rme_df_descr("Inconsistent row indices for the same statistic type (e.g., 'nobs').", test_type = "flag", long_descr = long_descr)

  return(issues)
}

#' Check for unmapped standard errors.
#'
#' This check identifies mapped coefficients where their corresponding standard
#' error cell (heuristically identified as the cell in parentheses below or
#' to the right) has not been included in the same regression mapping.
#' It also verifies if the numeric value in the unmapped SE cell would have
#' matched the standard error, t-statistic, or p-value from the regression output.
#'
#' @param rme The rme object.
#' @return A data frame of missing SE mappings, with a `would_match` column
#'   indicating if the SE value is consistent with the regression output.
rme_ev_missing_se_mapping = function(rme) {
  restore.point("rme_ev_missing_se_mapping")

  long_descr = "**Unmapped Standard Error.** This test flags cases where a mapped coefficient cell has an associated standard error (a value in parentheses, typically below the coefficient) that was *not* included in the regression mapping. It also reports whether the numeric value of that unmapped SE would have been a correct match for the regression's output, helping to distinguish simple mapping omissions from more complex issues."

  # 1. Identify all heuristically found coef-se pairs from cell_df
  coef_se_pairs = rme$cell_df %>%
    filter(reg_role == "coef", !is.na(partner_cellid)) %>%
    select(coef_cellid = cellid, se_cellid = partner_cellid, tabid)

  if (NROW(coef_se_pairs) == 0) {
    return(rme_df_descr(tibble::tibble(), "No coefficient-se pairs found in tables.", test_type = "flag", long_descr = long_descr))
  }

  # 2. Identify all uniquely mapped cells (map_version, runid, cellid)
  mapped_cells = rme$mc_df %>%
    select(map_version, runid, cellid) %>%
    distinct()

  # 3. Join pairs with the mapped coefficients to find the runid for each coef
  mapped_coefs = coef_se_pairs %>%
    inner_join(mapped_cells, by = c("coef_cellid" = "cellid"), relationship = "many-to-many")

  # 4. Create a lookup key for all mapped cells to efficiently check for existence
  mapped_cells_lookup = mapped_cells %>%
    mutate(key = paste(map_version, runid, cellid, sep = "--")) %>%
    pull(key)

  # 5. Filter for cases where the SE cell is NOT mapped to the same runid
  missing_se_df = mapped_coefs %>%
    mutate(se_key = paste(map_version, runid, se_cellid, sep = "--")) %>%
    filter(!se_key %in% mapped_cells_lookup)

  if (NROW(missing_se_df) == 0) {
    return(rme_df_descr(tibble::tibble(), "No missing SE mappings found.", test_type = "flag", long_descr = long_descr))
  }

  # 6. For the missing SEs, check if their value would have matched the regression output
  if (is.null(rme$parcels$regcoef$regcoef)) {
     warning("`regcoef` parcel not found. Cannot check if missing SEs would match.")
     # Return issues without the match check
     issues = missing_se_df %>%
       select(map_version, tabid, runid, coef_cellid, se_cellid) %>%
       mutate(would_match = NA)
  } else {
    # Get numeric values and decimal places for the unmapped SE cells
    se_cell_data = rme$cell_df %>%
        filter(cellid %in% missing_se_df$se_cellid) %>%
        select(se_cellid = cellid, paren_val = num, paren_deci = num_deci)

    # Get relevant regression output
    regcoef_df = rme$parcels$regcoef$regcoef %>%
        select(runid, cterm, reg_se = se, reg_t = t, reg_p = p)

    # Join missing SEs with their values and potential regression matches
    check_df = missing_se_df %>%
        left_join(se_cell_data, by = "se_cellid") %>%
        filter(!is.na(paren_val)) %>%
        left_join(regcoef_df, by = "runid", relationship = "many-to-many")

    # Perform the check
    match_results = check_df %>%
        filter(!is.na(reg_se) | !is.na(reg_t) | !is.na(reg_p)) %>%
        mutate(
            paren_deci = tidyr::replace_na(paren_deci, 0),
            is_perfect_se_match = (round(reg_se, paren_deci) == paren_val),
            is_perfect_t_match = (round(reg_t, paren_deci) == paren_val),
            is_perfect_p_match = (round(reg_p, paren_deci) == paren_val)
        ) %>%
        rowwise() %>%
        mutate(
            is_match = any(c(is_perfect_se_match, is_perfect_t_match, is_perfect_p_match), na.rm = TRUE)
        ) %>%
        ungroup()

    # Summarize results for each missing SE
    issue_summary = match_results %>%
        group_by(map_version, tabid, runid, coef_cellid, se_cellid) %>%
        summarise(would_match = any(is_match, na.rm = TRUE), .groups = "drop")

    # Join back with the full list of missing SEs in case some had no value to check
    issues = missing_se_df %>%
        select(map_version, tabid, runid, coef_cellid, se_cellid) %>%
        left_join(issue_summary, by = c("map_version", "tabid", "runid", "coef_cellid", "se_cellid")) %>%
        mutate(would_match = tidyr::replace_na(would_match, FALSE))
  }

  # 7. Add regid for better reporting and finalize
  final_issues = issues %>%
    left_join(
      rme$mc_df %>% select(map_version, runid, coef_cellid = cellid, regid) %>% distinct(),
      by = c("map_version", "runid", "coef_cellid")
    ) %>%
    select(map_version, tabid, regid, runid, coef_cellid, se_cellid, would_match) %>%
    rme_df_descr("Mapped coefficients with unmapped standard errors.", test_type = "flag", long_descr = long_descr)

  return(final_issues)
}
```
# END OF FILE: rme_eval_structure.R

-----------------------------------------------------------


# FILE: rme_eval_tools.R
```
rme_add_eval = function(rme, eval_steps, ...) {
  if (length(eval_steps)>1) {
    restore.point("rme_add_eval_mult")
    for (eval_step in eval_steps) {
      rme = rme_add_eval(rme, eval_step)
    }
    return(rme)
  }

  restore.point("rme_add_eval")
  eval_step = eval_steps

  eval_fun_name = paste0("rme_ev_", eval_step)

  # Check if the evaluation function exists
  if (!exists(eval_fun_name, mode = "function")) {
    warning("Evaluation function '", eval_fun_name, "' not found.", call. = FALSE)
    return(rme)
  }

  # Call the evaluation function
  ev_df = do.call(eval_fun_name, list(rme = rme, ...))

  if (!is.null(ev_df)) {
    all_keys = c("map_version","tabid","regid", "runid", "cellid","cellids")
    keys = all_keys %in% names(ev_df)
    names(keys) = all_keys
    rme$eval_keys[[eval_step]] = keys
  }

  # Initialize the evals list in the rme object if it doesn't exist
  if (is.null(rme$evals)) {
    rme$evals = list()
  }

  # Store the resulting data frame
  rme$evals[[eval_step]] = ev_df

  # Provide feedback to the user
  num_issues = if (is.null(ev_df)) 0 else NROW(ev_df)
  if (num_issues > 0) {
    cat(paste0("\nFinished '", eval_step, "' check. Found ", num_issues, " issues."))
  } else {
    cat(paste0("\nFinished '", eval_step, "' check. OK."))
  }

  return(rme)
}

#' Print a summary report of evaluation results to the console.
#'
#' @param rme The `rme` object containing evaluation results.
#' @param eval_steps A character vector of specific evaluations to report.
#'   If `NULL` (the default), all available evaluations are reported.
#' @export
rme_print_ev_report = function(rme, eval_steps = NULL) {
  restore.point("rme_print_ev_report")

  if (is.null(rme$evals) || length(rme$evals) == 0) {
    cat("No evaluations have been run yet.\n")
    return(invisible(NULL))
  }

  if (is.null(eval_steps)) {
    eval_steps = names(rme$evals)
  }

  cat("\n--- Evaluation Report ---")

  for (eval_step in eval_steps) {
    if (!eval_step %in% names(rme$evals)) {
      cat(paste0("\n\n* Evaluation '", eval_step, "': Not found."))
      next
    }

    ev_df = rme$evals[[eval_step]]
    n_issues = if (is.null(ev_df)) 0 else NROW(ev_df)

    cat(paste0("\n\n* ", eval_step, ": ", n_issues, " issues found."))

    if (n_issues > 0) {
      cat("\n")
      if ("map_version" %in% names(ev_df)) {
        # The project uses dplyr extensively, so it's assumed to be available.
        summary_df = ev_df %>%
          dplyr::count(map_version, sort = TRUE) %>%
          dplyr::rename(issues = n)
        print(summary_df, n = 10)
      } else {
        # Fallback for checks not related to map_version
        print(utils::head(ev_df))
      }
    }
  }
  cat("\n--- End of Report ---\n")
  invisible(NULL)
}

#' Combine multiple evaluation result data frames into one.
#'
#' @param rme The `rme` object containing evaluation results.
#' @param eval_steps A character vector of specific evaluations to combine.
#'   If `NULL` (the default), all available evaluations are combined.
#' @return A single tibble containing all issues from the specified evaluations.
#' @export
rme_combine_ev_df = function(rme, eval_steps = NULL) {
  restore.point("rme_combine_ev_df")

  if (is.null(rme$evals) || length(rme$evals) == 0) {
    return(tibble::tibble())
  }

  if (is.null(eval_steps)) {
    eval_steps = names(rme$evals)
  }

  # Filter for existing evaluation results
  eval_steps = intersect(eval_steps, names(rme$evals))
  if (length(eval_steps) == 0) {
    return(tibble::tibble())
  }

  eval_list = lapply(eval_steps, function(eval_step) {
    rme$evals[[eval_step]] %>% add_col_left(test_name=eval_step)
  })

  # The evaluation functions are expected to return a tibble/data.frame.
  # We combine them into one big tibble.
  # dplyr::bind_rows is used as it handles differing columns by filling with NA,
  # and it's used throughout the project.
  combined_df = dplyr::bind_rows(eval_list)

  return(combined_df)
}

rme_df_descr = function(df, descr, test_type = c("flag","discrepancy", "note_flag")[1], long_descr = NULL) {
  attr(df, "descr") = descr
  attr(df, "test_type") = test_type
  if (!is.null(long_descr)) {
    attr(df, "long_descr") = long_descr
  }
  df
}

rme_df_merge_cellids = function(df) {
  by = setdiff(names(df),"cellid")
  df %>%
    group_by(across(all_of(by))) %>%
    summarize(cellids = paste0(cellid, collapse=","), .groups="drop")
}
```
# END OF FILE: rme_eval_tools.R

-----------------------------------------------------------


# FILE: rme_init.R
```
# rme stands for repbox map evaluation
#
# We will use an rme object (just an R list)
# that will load and transform all data needed

example = function() {
  library(repboxRegmap)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  rstudioapi::filesPaneNavigate(project_dir)
  options(warn=1)

  rme = rme_init(project_dir)
  rme = rme_eval_all(rme)
  rme_save(rme)
  outfile = file.path(project_dir,"fp","eval_art/rme_report.md")
  str = rme_make_report(rme,outfile = outfile,long_descr = TRUE, map_version = "g25f-mocr--v0", tabid="2")
  rstudioapi::filesPaneNavigate(outfile)

  library(repboxReport)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  opts = rr_map_report_opts(embed_data = FALSE)
  rep_file = rr_map_report(project_dir,opts = opts)
  browseURL(rep_file)
  rstudioapi::filesPaneNavigate(rep_file)

  # Explore the created rme object
  ls(rme)
  head(rme$mc_df)
  head(rme$wrong_numbers_df)


  lapply(rme$evals, function(df) {
    unique(df$tabid)
  })
}


rme_init = function(project_dir, doc_type = "art") {
  restore.point("rme_init")

  # Load repdb parcels
  parcel_names = c("reg_core", "regcoef", "regscalar", "regstring", "regvar", "regxvar", "stata_run_log")
  parcels = repdb_load_parcels(project_dir, parcel_names = parcel_names)
  parcels = parcels_add_runid_step(parcels,map_df = parcels$reg_core$reg)


  # Load fp data
  fp_dir = file.path(project_dir, "fp", paste0("prod_", doc_type))
  eval_dir = file.path(project_dir, "fp", paste0("eval_", doc_type))

  # table structure is best stored in cell_base
  cell_ver = repboxAI::rai_pick_tab_ver(fp_dir, "cell_base")
  cell_df = fp_load_prod_df(ver_dir = cell_ver$ver_dir,add_ids = FALSE)

  #cell_df = FuzzyProduction::fp_pick_and_load_prod_df(fp_dir,"cell_base",add_fp_dir = FALSE,add_ids = FALSE)

  rme = list(
    project_dir = project_dir,
    doc_type = doc_type,
    fp_dir = fp_dir,
    eval_dir = eval_dir,
    cell_verid = cell_ver$ver_id,
    cell_df = cell_df,
    parcels = parcels
  )
  # Add command and run info, including command type and associated regressions
  rme = rme_add_run_df_cmd_df(rme)
  rme = rme_add_cmd_info(rme)

  # Add cell heuristic info (coef, se, etc.)
  rme = rme_add_cell_reg_info(rme)


  # Load all mapping product versions for map_reg_run
  # This is now the only mapping we consider for building the "best" map
  map_reg_run = fp_load_all_prod_df(fp_dir, prod_id = "map_reg_run") %>%
    add_col_left(prod_id = "map_reg_run")

  # Add info from run_df
  map_reg_run = map_reg_run %>%
      left_join(rme$run_df %>% select(runid, cmd, cmd_type, reg_runid, is_reg), by="runid")


  wrong_numbers_df = map_reg_run %>%
    filter(!sapply(wrong_number_cases, is.null)) %>%
    select(ver_id, prod_id, tabid, regid, runid, cmd, cmd_type,  wrong_number_cases) %>%
    tidyr::unnest(wrong_number_cases)


  # Unnest on a cellid level and add additional info
  mc_df = unnest_comma_string_col(map_reg_run, "cell_ids") %>%
    rename(cellid = cell_ids) %>%
    # Focus on mappings with a valid runid, which is our main key
    filter(!is.na(runid))

  mc_df = mc_df %>%
    mutate(map_version = ver_id) %>%
    filter(!is.na(cellid), cellid != "") %>%
    # Select and reorder a common set of columns for the final unified data frame.
    select(
      cellid, map_version, prod_id, ver_id, tabid, any_of("regid"), runid,
      # Keep problem description if present
      any_of("problem")
    ) %>%
    # Enrich with run information (cmd, type, associated reg)
    left_join(rme$run_df %>% select(runid, cmd, cmd_type, reg_runid, is_reg), by="runid") %>%
    # Enrich with cell coordinates and content from the main cell_df
    left_join(rme$cell_df, by=c("tabid", "cellid"))


  # Aggregate on a map_version, cellid level
  map_cell_agg = mc_df %>%
    group_by(map_version, prod_id, tabid, cellid) %>%
    summarize(
      runids = paste0(sort(unique(runid)), collapse=","),
      num_runids = n_distinct(runid),
      .groups = "drop"
    ) %>%
    arrange(tabid, cellid, prod_id, map_version)

  # --- Create rme object ---
  rme = c(rme,list(
    mc_df = mc_df,
    wrong_numbers_df = wrong_numbers_df,
    map_cell_agg = map_cell_agg,
    map_reg_run = map_reg_run,
    map_versions = unique(mc_df$map_version)
  ))


  return(rme)
}

rme_save = function(rme) {
  if (is.null(rme$eval_dir)) {
    cat("\nrem$eval_dir not specified")
    return(NULL)
  }
  file = file.path(rme$eval_dir, "rme.Rds")
  if (!dir.exists(rme$eval_dir)) dir.create(rme$eval_dir)
  saveRDS(rme, file)
  invisible(rme)
}

rme_load = function(project_dir, doc_type = "art") {
  eval_dir = file.path(project_dir, "fp", paste0("eval_", doc_type))
  file = file.path(eval_dir, "rme.Rds")
  readRDS.or.null(file)
}


# Add missing step or runid column if the other exists
parcels_add_runid_step = function(parcels, map_df = parcels$reg_core$reg) {
  restore.point("parcels_map_runid_step")
  map_df = map_df[,c("runid","step")]


  # Add runid to all parcel data frames that have 'step'
  for (pname in names(parcels)) {
    if (is.list(parcels[[pname]])) {
       for (df_name in names(parcels[[pname]])) {
         df = parcels[[pname]][[df_name]]
         if ("step" %in% names(df) && !"runid" %in% names(df)) {
            parcels[[pname]][[df_name]] = left_join(df, map_df, by="step")
         } else if (!"step" %in% names(df) && "runid" %in% names(df)) {
            parcels[[pname]][[df_name]] = left_join(df, map_df, by="runid")
         }
       }
    }
  }
  parcels
}


# return a version of cmd_df and run_df that have script_num and code_line
# given by orgline
rme_add_run_df_cmd_df = function(rme, project_dir=rme$project_dir, parcels=rme$parcels) {
  restore.point("rme_load_run_df_cmd_df")
  # Load repdb parcels
  parcel_names = c("stata_run_cmd", "stata_cmd", "stata_file")
  parcels = repdb_load_parcels(project_dir, parcel_names = parcel_names,parcels=parcels)

  script_df = parcels$stata_file$script_file %>% select(file_path, script_num)
  cmd_df = parcels$stata_cmd$stata_cmd %>%
    left_join(script_df, by = c("file_path")) %>%
    select(artid, script_num, code_line=orgline, line=line, cmd=cmd, is_reg=is_reg) %>%
    mutate(script_line = paste0(script_num, "-", code_line))

  run_df = parcels$stata_run_cmd$stata_run_cmd %>%
    left_join(script_df, by = c("file_path")) %>%
    left_join(cmd_df %>% select(script_num, line, code_line, is_reg, script_line), by=c("script_num","line")) %>%
    select(artid, runid, script_num, code_line=code_line, script_line, cmd=cmd, is_reg, missing_data, errcode)

  rme$cmd_df = cmd_df
  rme$run_df = run_df

  rme
}

```
# END OF FILE: rme_init.R

-----------------------------------------------------------


# FILE: rme_report.R
```
#' Create a markdown report of evaluation results
#'
#' This function generates a markdown-formatted report summarizing the issues
#' found by the various evaluation steps. The report is structured hierarchically:
#' first by map version, then by table ID, and finally by test.
#'
#' @param rme The `rme` object containing evaluation results in `rme$evals`.
#' @param map_version An optional character vector of map versions to report on.
#'   If `NULL`, results from all versions with issues are included.
#' @param tabid A character vector of table IDs to include in the report.
#'   If `NULL`, all tables with issues are included.
#' @param test_names A character vector of specific evaluation step names to include.
#'   If `NULL`, all completed evaluations are reported.
#' @param ignore_tests A character vector of evaluation step names to exclude.
#' @param long_descr A logical value. If `TRUE`, a detailed explanation for each
#'   test is included in the report to help interpret the results. If `FALSE`,
#'   a concise description is used.
#' @param outfile An optional file path. If provided, the report is written to
#'   this file.
#' @return A string containing the markdown report.
#' @export
rme_make_report = function(rme, map_version = NULL, tabid = NULL, test_names = NULL, ignore_tests = NULL, long_descr = TRUE, outfile = NULL) {
  restore.point("rme_make_report")

  # --- Helper Functions ---

  # Converts a data frame to a markdown table string
  df_to_markdown = function(df) {
    if (!is.data.frame(df) || NROW(df) == 0) return("")
    # Clean strings for markdown
    clean_string = function(s) {
      s = as.character(s)
      s[is.na(s)] = ""
      s = stringi::stri_replace_all_fixed(s, "|", "&#124;")
      s = stringi::stri_replace_all_regex(s, "[\r\n]+", " ")
      s
    }
    df_char = as.data.frame(lapply(df, clean_string), stringsAsFactors = FALSE)
    header = paste0("| ", paste(names(df_char), collapse = " | "), " |")
    separator = paste0("|", paste(rep("---", ncol(df_char)), collapse = "|"), "|")
    body_rows = apply(df_char, 1, function(row) paste0("| ", paste(row, collapse = " | "), " |"))
    paste(c(header, separator, body_rows), collapse = "\n")
  }

  # Chooses a display format (table or list) for a given issue data frame
  format_issues_md = function(df, test_name) {
    # For certain tests, a list is more readable than a wide table
    use_list_format = test_name %in% c("multicol_reg_plausibility", "invalid_cellids", "single_col_reg")

    if (use_list_format) {
      # Custom list format for specific tests
      if (test_name == "multicol_reg_plausibility" && all(c("regid", "cellids", "cols") %in% names(df))) {
        items = purrr::pmap_chr(df, function(regid, cellids, cols, ...) {
          paste0("* **Reg. ", regid, "**: Implausible structure for columns `", cols, "`. (Cells: `", cellids, "`)")
        })
        return(paste(items, collapse = "\n"))
      }
      if (test_name %in% c("invalid_cellids", "single_col_reg") && all(c("regid", "cellids") %in% names(df))) {
        items = purrr::map2_chr(df$regid, df$cellids, ~paste0("* **Reg. ", .x, "**: Affects cells `", .y, "`"))
        return(paste(items, collapse = "\n"))
      }
    }
    # Default to a table for all other cases
    return(df_to_markdown(df))
  }

  # --- Main Logic ---

  # 1. Determine tests and get all evaluation data
  all_ev_dfs = rme$evals
  available_tests = names(all_ev_dfs)
  tests_to_report = if (is.null(test_names)) available_tests else intersect(available_tests, test_names)
  if (!is.null(ignore_tests)) tests_to_report = setdiff(tests_to_report, ignore_tests)

  if (length(tests_to_report) == 0 || length(all_ev_dfs) == 0) {
    msg = "No tests to report on (either none run, or all filtered out)."
    if (!is.null(outfile)) writeLines(msg, outfile)
    return(msg)
  }

  # Filter all DFs based on top-level filters (if any) and combine to find relevant scopes
  filtered_ev_list = lapply(all_ev_dfs[tests_to_report], function(df) {
    if (is.null(df) || NROW(df) == 0) return(NULL)
    # Ungroup to prevent dplyr warnings during filtering
    df = dplyr::ungroup(df)
    if (!is.null(map_version) && "map_version" %in% names(df)) df = dplyr::filter(df, .data$map_version %in% .env$map_version)
    if (!is.null(tabid) && "tabid" %in% names(df)) df = dplyr::filter(df, .data$tabid %in% .env$tabid)
    if (NROW(df) > 0) df else NULL
  })
  filtered_ev_list = purrr::compact(filtered_ev_list)
  if(length(filtered_ev_list) == 0) {
     msg = "No issues found for the specified filters."
     if (!is.null(outfile)) writeLines(msg, outfile)
     return(msg)
  }

  combined_issues = dplyr::bind_rows(filtered_ev_list, .id = "test")

  # 2. Build Report Header
  report_parts = list(
    "# Regression Mapping Evaluation Report",
    if (!is.null(rme$project_dir)) paste0("**Project**: `", rme$project_dir, "`"),
    "\n---"
  )

  # 3. Iterate and Build Report Body
  issues_by_version = split(combined_issues, combined_issues$map_version)

  for (mv in sort(names(issues_by_version))) {
    version_issues = issues_by_version[[mv]]
    version_parts = list(paste0("\n## Map Version: `", mv, "`"))

    issues_by_table = split(version_issues, version_issues$tabid)

    for (tid in sort(names(issues_by_table))) {
      table_issues = issues_by_table[[tid]]
      table_parts = list(paste0("\n### Table `", tid, "`"))

      tests_in_table = intersect(tests_to_report, unique(table_issues$test))

      for (test in tests_in_table) {
        current_issues = dplyr::filter(table_issues, .data$test == .env$test)

        table_parts = c(table_parts, paste0("\n#### Test: `", test, "`"))

        test_df = filtered_ev_list[[test]]
        descr = if (long_descr) attr(test_df, "long_descr") else NULL
        if (is.null(descr)) {
          descr = attr(test_df, "descr")
        }

        if (!is.null(descr)) {
            table_parts = c(table_parts, paste0("> ", descr))
        }

        table_parts = c(table_parts,
            paste0("\n**Issues Found**: ", NROW(current_issues)),
            format_issues_md(dplyr::select(current_issues, -any_of(c("map_version", "tabid", "test"))), test)
        )
      }
      table_parts = c(table_parts, "\n---")
      version_parts = c(version_parts, table_parts)
    }
    report_parts = c(report_parts, version_parts)
  }

  # 4. Finalize and Output
  final_report = paste(purrr::compact(unlist(report_parts)), collapse = "\n\n")
  final_report = stringi::stri_replace_all_regex(final_report, "(\n){3,}", "\n\n") # Clean up excess newlines

  if (!is.null(outfile)) {
    writeLines(final_report, outfile)
    cat(paste0("\nReport written to '", outfile, "'."))
  }

  invisible(final_report)
}
```
# END OF FILE: rme_report.R

-----------------------------------------------------------


# FILE: tools.R
```
add_missing_cols = function(df, ...) {
  vals = list(...)
  for (col in names(vals)) {
    if (!col %in% names(df)) {
      df[[col]] = rep(vals[[col]], NROW(df))
    }
  }
  df
}

add_missing_cols_and_na_val = function(df, ...) {
  vals = list(...)
  for (col in names(vals)) {
    if (!col %in% names(df)) {
      df[[col]] = rep(vals[[col]], NROW(df))
    } else {
      na_rows = which(is.na(df[[col]]))
      if (length(na_rows)>0) {
        df[[col]][na_rows] = vals[[col]]
      }
    }
  }
  df
}

unnest_comma_string_col = function(df, comma_string_col,sep=",") {
  restore.point("unnest_comma_string_col")

  # Handle case where column does not exist
  if (!comma_string_col %in% colnames(df)) {
    return(df)
  }
  if (NROW(df)==0) return(df)


  col_vec = df[[comma_string_col]]
  # Treat NA as empty string for consistent processing
  col_vec[is.na(col_vec)] = ""

  # Split strings into a list of character vectors
  s_list = stringi::stri_split_fixed(col_vec, sep)

  # For each element in the list, trim whitespace from its components
  # and remove any resulting empty strings.
  # s_list = lapply(s_list, function(x) {
  #   x = stringi::stri_trim_both(x)
  #   x = x[x != ""]
  #   x
  # })

  # Get the number of non-empty components for each original row
  reps = vapply(s_list, length, integer(1))

  # Replicate the original data frame rows according to the number of elements
  res_df = df[rep(seq_len(nrow(df)), reps), , drop = FALSE]

  # Get the unlisted, non-empty values
  new_col = unlist(s_list)

  # Replace the original comma-separated column with the unnested values
  res_df[[comma_string_col]] = new_col

  # Reset row names for the new data frame
  rownames(res_df) = NULL

  res_df
}

# Helper to convert a data frame to a markdown table
df_to_markdown = function(df) {
  if (!is.data.frame(df) || NROW(df) == 0) return("")

  # Clean strings for markdown and handle list columns
  clean_val = function(v) {
    if (is.list(v)) return("[[...]]")
    s = as.character(v)
    s[is.na(s)] = ""
    s = stringi::stri_replace_all_fixed(s, "|", "|")
    s = stringi::stri_replace_all_regex(s, "[\r\n]+", " ")
    s
  }

  df_char = as.data.frame(lapply(df, function(col) sapply(col, clean_val)), stringsAsFactors = FALSE)
  names(df_char) = names(df)

  header = paste0("| ", paste(names(df_char), collapse = " | "), " |")
  separator = paste0("|", paste(rep("---", ncol(df_char)), collapse = "|"), "|")
  body_rows = apply(df_char, 1, function(row) paste0("| ", paste(row, collapse = " | "), " |"))
  paste(c(header, separator, body_rows), collapse = "\n")
}
```
# END OF FILE: tools.R

-----------------------------------------------------------



# R code (project FuzzyProduction)


# FILE: ddp.R
```
# Directly derived products
# Example: cell_list from tab_html
#
# Each run of tab_html can be deterministically transformed into a cell_list
# We use the same version and runid for directly derived products
#
# Below are helper functions




#' Sometimes we make a deterministic transformation step, like
#' tab_html to cell_list
#' We then may want to simply keep the version and run names of
#' the original run and just change the product.
ddp_get_ver_dir = function(ver_dir, ddp_prod_id, ddp_proc_id = NULL) {
  restore.point("ddp_get_ver_dir")
  ver_id = basename(ver_dir)
  if (is.null(ddp_proc_id)) {
    ddp_proc_id = basename(dirname(ver_dir))
  }
  fp_dir = fp_ver_dir_to_fp_dir(ver_dir)
  ddp_ver_dir = file.path(fp_dir,ddp_prod_id, ddp_proc_id, ver_id)
}

ddp_is_up_to_date = function(ver_dir, ddp_prod_id) {
  restore.point("ddp_is_up_to_date")
  ddp_ver_dir = ddp_get_ver_dir(ver_dir, ddp_prod_id)
  if (!dir.exists(ddp_ver_dir)) return(FALSE)
  ddp_file = file.path(ddp_ver_dir,"prod_df.Rds")
  if (!file.exists(ddp_file)) return(FALSE)

  file = file.path(ver_dir, "prod_df.Rds")
  if (isTRUE(file.mtime(ddp_file) >= file.mtime(file))) return(TRUE)
  return(FALSE)
}

#' Sometimes we make a deterministic transformation step, like
#' tab_html to cell_list
#' We then may want to simply keep the version and run names of
#' the original run and just change the product.
ddp_init_pru = function(pru, ddp_prod_id, prods = repbox_prods(), ver_dir=pru$ver_dir, prod_id = first_nn(pru$prod_id,fp_ver_dir_to_prod_id(ver_dir)), ddp_proc_id=NULL) {
  restore.point("ddp_init_pru")
  if (is.null(ver_dir)) stop("You must provide at least ver_dir (original ver_dir). Or the complete original pru / rais object.")
  input_info = data.frame(prod_id=prod_id, ver_dir = ver_dir, found=TRUE, num_cand = 1)
  ddp_prod = prods[ddp_prod_id]

  fp_dir = dirname(dirname(dirname(ver_dir)))
  ddp_ver_dir = ddp_get_ver_dir(ver_dir, ddp_prod_id, ddp_proc_id)
  ddp_pru = pru_init(fp_dir = fp_dir, prod_id=ddp_prod_id, input_info=input_info, ver_dir=ddp_ver_dir)
  ddp_pru
}


#' Derives all ddp instances
#'
#' Will be typically called from a wrapper function like \code{hx_all_tab_html_to_cell_list}
#'
ddp_derive_all_instances = function(parent_dir,from_prod_id, to_prod_id, convert_fun,  overwrite=FALSE) {
  restore.point("ddp_derive_all_instances")
  from_files = list.files(parent_dir, glob2rx("prod_df.Rds"),full.names = TRUE, recursive=TRUE)
  file = from_files[1]
  for (file in from_files) {
    ver_dir = dirname(file)
    if (!overwrite) {
      if (ddp_is_up_to_date(ver_dir, to_prod_id)) next
    }
    convert_fun(ver_dir = ver_dir)
  }
}

```
# END OF FILE: ddp.R

-----------------------------------------------------------


# FILE: fp_debug.R
```

```
# END OF FILE: fp_debug.R

-----------------------------------------------------------


# FILE: fp_diff_report.R
```
# Like fp_html_report in fp_report.R will make HTML report of fp_products

# But this report focuses on showing differences between versions of the same product.

# Key idea: only one table per product

# For each original row in some product
# Show base line results that are most common across versions
# show below divergent rows for some versions, only show divergence for columns
# that differ.

# How to build table:

# First determine for each column the communalty: the largest set ()


#' Generate an HTML report showing differences between product versions
#'
#' This function takes a set of version directories, groups them by product,
#' and for each product, generates a "diff" table comparing the different
#' versions. The final output is a single HTML file containing these diff tables.
#'
#' @param ver_dirs A character vector of version directories, typically from `fp_all_ok_ver_dirs`.
#' @param outfile The path for the output HTML file. If NULL, a default is created.
#' @param prods Optionally a list of product definitions (from `prods_define`) to determine the order and keys for products in the report.
#' @param max_rows The maximum number of rows to display for each diff table.
#' @param max_cols The maximum number of columns to display for each diff table.
#' @param title The title of the HTML report.
#' @return The path to the created `outfile`, invisibly.
fp_html_diff_report = function(ver_dirs, outfile=NULL, prods = NULL, max_rows=100, max_cols=5, title = "Fuzzy Production Difference Report") {
  restore.point("fp_html_diff_report")

  if (length(ver_dirs) == 0) {
    cat("No version directories provided to fp_html_diff_report.\n")
    return(invisible(NULL))
  }

  if (is.null(outfile)) {
    fp_dir = fp_ver_dir_to_fp_dir(ver_dirs[1])
    outfile = file.path(fp_dir, "fp_diff_report.html")
  }

  # 1. Get info for all ver_dirs and find unique products
  info_df = fp_ver_dir_to_ids(ver_dirs)
  all_prod_ids = unique(info_df$prod_id)

  # 2. Determine order of products for the report
  if (!is.null(prods)) {
    # Order by prods list, but only include products present in ver_dirs
    report_prod_ids = intersect(names(prods), all_prod_ids)
  } else {
    # Default to alphabetical order
    report_prod_ids = sort(all_prod_ids)
  }

  # 3. Start building HTML content
  css = fp_report_css()

  html_head = paste0(
    "<!DOCTYPE html><html><head><meta charset='UTF-8'><title>", .html_escape(title), "</title>",
    "<style>", css, "</style></head><body><h1>", .html_escape(title), "</h1>"
  )

  html_body_parts = c()

  # 4. Loop through products and generate diff tables
  for (prod_id in report_prod_ids) {
    html_body_parts = c(html_body_parts, paste0("<h2>Product: ", .html_escape(prod_id), "</h2>"))

    # Get ver_dirs for this product
    prod_info = dplyr::filter(info_df, .data$prod_id == !!prod_id)

    # Load all prod_df into a named list
    df_list = list()
    for (i in 1:NROW(prod_info)) {
      row = prod_info[i, ]
      ver_dir = row$ver_dir
      df = tryCatch({
        fp_load_prod_df(ver_dir)
      }, error = function(e) {
        warning("Could not load prod_df from ", ver_dir, ": ", e$message)
        return(NULL)
      })
      if (!is.null(df) && NROW(df) > 0) {
        # Name of the list element is the version identifier
        df_list[[row$ver_id]] = df
      }
    }

    if (length(df_list) < 2) {
      html_body_parts = c(html_body_parts, "<p><em>Not enough versions with data to compare for this product.</em></p>")
      next
    }

    # Determine key for diffing
    key = NULL
    if (!is.null(prods) && !is.null(prods[[prod_id]]$keys)) {
      key = prods[[prod_id]]$keys
    }

    # Generate diff table
    diff_df = tryCatch({
      fp_diff_tables(df_list, key = key)
    }, error = function(e) {
      warning("Could not generate diff table for ", prod_id, ": ", e$message)
      # Create a simple data frame with the error message
      data.frame(error = paste("Failed to generate diff table:", e$message))
    })

    # Truncate if necessary
    orig_rows = NROW(diff_df)
    orig_cols = NCOL(diff_df)

    df_display = diff_df
    truncated = FALSE
    trunc_msg = c()

    if (orig_rows > max_rows) {
      df_display = df_display[1:max_rows, , drop = FALSE]
      trunc_msg = c(trunc_msg, paste0("rows truncated to ", max_rows, " (from ", orig_rows, ")"))
      truncated = TRUE
    }
    if (orig_cols > max_cols) {
      df_display = df_display[, 1:max_cols, drop = FALSE]
      trunc_msg = c(trunc_msg, paste0("columns truncated to ", max_cols, " (from ", orig_cols, ")"))
      truncated = TRUE
    }

    # Generate table HTML
    table_html = .df_to_html_table(df_display)
    html_body_parts = c(html_body_parts, "<div class='table-container'>", table_html, "</div>")

    # Add truncation info message if needed
    if (truncated) {
        msg = paste0("<p class='trunc-info'>Note: ", paste(trunc_msg, collapse=", "), ".</p>")
        html_body_parts = c(html_body_parts, msg)
    }
  }

  html_foot = "</body></html>"

  # 5. Combine all parts and write to file
  final_html = paste(c(html_head, html_body_parts, html_foot), collapse="\n")
  writeLines(final_html, outfile)

  cat("HTML diff report written to:", outfile, "\n")
  invisible(outfile)
}

example = function() {
    # toy data: two versions of the small table in your screenshot
  mocr = tibble::tribble(
    ~tabid, ~otabid, ~tabtitle,
      1,     "001",  "Table 1—Baseline Characteristics",
      2,     "002",  "Table 2—Balancing Test at Baseline",
      3,     "003",  "Table 3—OLS Estimates on Profit at Different Time Periods (ANCOVA)"
  )

  pdf_txt = tibble::tribble(
    ~tabid, ~otabid, ~tabtitle,
      1,     "001",  "Table 1—Baseline Characteristics",
      2,     "002",  "Table 2 — Balancing Test at Baseline",   # note the extra spaces
      3,     "003",  "Table 3 — OLS Estimates on Profit at Different Time Periods (ANCOVA)"
  )

  library(tidyr)
  diff_df = fp_diff_tables(list(mocr = mocr, pdf_txt = pdf_txt))

}


# ---------- helper: detect a key ----------
fp_detect_df_li_key = function(df_list) {
  library(purrr)

  # step 1: common columns
  common_cols = reduce(map(df_list, names), intersect)
  if (length(common_cols) == 0) stop("No common columns across versions.")

  # step 2: score columns
  score_col = function(col) {
    map_dbl(df_list, function(df) {
      x = df[[col]]
      uniqueness = n_distinct(x, na.rm = TRUE) / nrow(df)
      completeness = 1 - mean(is.na(x))
      # Jaccard stability: overlap of value sets with first version
      stab = length(intersect(df_list[[1]][[col]], x)) /
             length(union   (df_list[[1]][[col]], x))
      mean(c(uniqueness, completeness, stab))
    }) %>% mean()
  }
  scores = setNames(map_dbl(common_cols, score_col), common_cols)
  ranked = names(sort(scores, decreasing = TRUE))

  # greedy search
  best = NULL
  for (k in seq_along(ranked)) {
    cand = ranked[seq_len(k)]
    unique_ok = map_lgl(df_list, ~ n_distinct(select(.x, all_of(cand))) == nrow(.x))
    if (mean(unique_ok) >= 0.8) { best = cand; break }
  }
  if (is.null(best)) best = "row_index"
  best
}

# create diff tables as data frame
# create diff tables as data frame
fp_diff_tables = function(df_list, key = NULL) {
  restore.point("fp_diff_tables")
  library(purrr)

  if (is.null(key)) key = fp_detect_df_li_key(df_list)

  # add row_index if needed
  df_list = imap(df_list, function(df, nm) {
    if ("row_index" %in% key) df = mutate(df, row_index = row_number())
    df$.version = nm
    df
  })

  df = do.call(bind_rows, df_list)
  org_cols = setdiff(names(df),".version")
  key_cols = key
  val_cols = setdiff(names(df), union(key_cols, ".version"))

  # Serialize columns to create a comparable key/value string.
  # This now handles list-columns by converting them to canonical JSON.
  serialize_for_compare = function(df_part) {
    if (length(names(df_part)) == 0) return(rep("", NROW(df_part)))
    s_cols <- lapply(df_part, function(col) {
      if (is.list(col)) {
        sapply(col, function(x) {
          if (is.null(x) || (is.list(x) && length(x) == 0) || (is.data.frame(x) && nrow(x) == 0)) return("[]")
          jsonlite::toJSON(x, auto_unbox = TRUE, sort = TRUE)
        })
      } else {
        as.character(col)
      }
    })
    do.call(stringi::stri_join, c(s_cols, list(sep = "|")))
  }

  df$.value = serialize_for_compare(df[, val_cols, drop = FALSE])
  df$.key = serialize_for_compare(df[, key_cols, drop = FALSE])
  df$.order = 1:NROW(df)

  df = df %>%
    group_by(.key) %>%
    mutate(.key_order = min(.order)) %>%
    ungroup()

  # Convert all columns to character for display, pretty-printing list-cols
  df_char = df
  for (col in org_cols) {
      if(is.list(df_char[[col]])) {
          df_char[[col]] = sapply(df_char[[col]], function(x) {
            if (is.null(x)) return(NA_character_)
            if ((is.list(x) && length(x) == 0) || (is.data.frame(x) && nrow(x) == 0)) return("[]")
            jsonlite::toJSON(x, auto_unbox = TRUE, pretty = TRUE)
          })
      }
  }
  df = as.data.frame(lapply(df_char, as.character))


  diff_str = function(x) {
    x[x==x[1] & seq_along(x)>1]=""
    x
  }

  key_val_cols = c(key_cols, val_cols)

  diff_df = df %>%
    group_by(.key_order, .value) %>%
    mutate(
      .group_n = n(),
    ) %>%
    group_by(.key_order) %>%
    arrange(desc(.group_n), .value) %>%
    mutate(.in_order = 1:n()) %>%
    mutate(.differs = .value != first(.value)) %>%
    filter( (1:n())==1 | .differs) %>%
    mutate(across(all_of(c(key_cols,val_cols)), diff_str)) %>%
    ungroup() %>%
    arrange(.key_order, .in_order)

  diff_df = diff_df[, union(".version",org_cols)]

  diff_df
}

paste_cols = function(df, cols = names(df), sep="|") {
  if (length(cols) == 0) return(rep("", NROW(df)))
  do.call(stri_join,c(df[cols],list(sep="|")))
}
```
# END OF FILE: fp_diff_report.R

-----------------------------------------------------------


# FILE: fp_dirs.R
```
example = function() {
  project_dir = "~/repbox/projects_share/aejapp_1_2_4"
  fp_dir = file.path(project_dir,"fp")

  fp_all_proc_id(fp_dir,"cell_base")
  prod_id = "cell_base"
  proc_id = "tab_html_hx_pdf"
  fp_newest_ver_dir(fp_dir, prod_id, proc_id)

  fp_all_outage_ver_dirs(project_dir)
  fp_all_error_ver_dirs(project_dir)

  df = fp_all_ver_info(fp_dir)

  df = fp_load_all_prod_df(fp_dir, "cell_base")
}

fp_load_pru = function(ver_dir) {
  pru_file = file.path(ver_dir, "pru.Rds")
  if (!file.exists(pru_file)) return(NULL)
  pru = readRDS(pru_file)
  pru
}

fp_newest_ver_dir = function(fp_dir, prod_id, proc_id=NULL) {
  restore.point("fp_newest_ver_dir")
  info = fp_all_ver_info(fp_dir, prod_id=prod_id, proc_id=proc_id)
  if (NROW(info)==0) return(NULL)
  info %>%
    group_by(prod_id, proc_id) %>%
    arrange(desc(mtime)) %>%
    slice(1) %>%
    pull(ver_dir)
}

fp_ver_id_to_ver_dir = function(fp_dir, prod_id, ver_id) {
  loc = stri_locate_first_fixed(ver_id,"--")
  proc_id = stri_sub(ver_id,1, loc[,1]-1)
  ver_ind = stri_sub(ver_id,loc[,2]+2)

  fp_all_ver_dirs(fp_dir, prod_id=prod_id, proc_id=proc_id,ver_ind=ver_ind)


}

fp_ver_dir_to_fp_dir = function(ver_dir) {
  dirname(dirname(dirname(ver_dir)))
}

fp_ver_dir_to_prod_dir = function(ver_dir) {
  dirname(dirname(ver_dir))
}

fp_ver_dir_to_ver_id = function(ver_dir) {
  dname = dirname(ver_dir)
  proc_id = basename(dname)
  ver_id = paste0(proc_id,"--", basename(ver_dir))
  ver_id
}


fp_ver_dir_to_ids = function(ver_dir) {
  dname = dirname(ver_dir)
  proc_id = basename(dname)
  prod_id = basename(dirname(dname))
  ver_id = paste0(proc_id,"--", basename(ver_dir))
  ver_ind = as.integer(stri_sub(basename(ver_dir),2))
  data.frame(prod_id=prod_id, proc_id=proc_id, ver_id=ver_id, ver_ind=ver_ind, ver_dir=ver_dir)
}

fp_ver_dir_to_prod_id = function(ver_dir) {
  dname = dirname(ver_dir)
  prod_id = basename(dirname(dname))
  prod_id
}


fp_ver_dir_to_proc_id = function(ver_dir) {
  dname = dirname(ver_dir)
  proc_id = basename(dname)
  proc_id
}


fp_ver_dir_to_proc_dir = function(ver_dir) {
  dirname(ver_dir)
}

fp_proc_dir_to_prod_dir = function(proc_dir) {
  dirname(proc_dir)
}

fp_proc_dir_to_proc_id = function(proc_dir) {
  basename(proc_dir)
}

fp_proc_dir_to_fp_dir = function(proc_dir) {
  dirname(dirname(proc_dir))
}

fp_proc_dir_to_ver_dirs = function(proc_dir, ver_ind=NULL) {
  if (!is.null(ver_ind)) {
    file.path(proc_dir,paste0("v", ver_ind))
  } else {
    list.dirs(proc_dir,full.names = TRUE, recursive = FALSE)
  }
}

fp_prod_dir_to_fp_dir = function(prod_dir) {
  dirname(prod_dir)
}

fp_prod_dir_to_prod_id = function(prod_dir) {
  basename(prod_dir)
}

fp_all_prod_id = function(fp_dir) {
  list.dirs(fp_dir,full.names = FALSE,recursive = FALSE)
}


fp_all_proc_id = function(fp_dir, prod_id=NULL, ends_with=NULL) {
  if (is.null(prod_id)) {
    prod_id_dirs = list.dirs(fp_dir,full.names = TRUE,recursive = FALSE)
  } else {
    prod_id_dirs = file.path(fp_dir, prod_id)
  }
  proc_ids = list.dirs(prod_id_dirs,full.names = FALSE,recursive = FALSE)
  if (!is.null(ends_with)) {
    proc_ids = proc_ids[endsWith(proc_ids, ends_with())]
  }
  proc_ids
}


fp_all_proc_dir = function(fp_dir, prod_id, only_success=TRUE) {
  fp_dir = file.path(fp_dir,"fp","prod_vers",prod_id)
  if (only_success) {
    files = list.files(fp_dir, glob2rx("prod_df.Rds"),full.names = TRUE, recursive=TRUE)
    ver_dirs = dirname(files)
  } else {
    stop("Not yet implemented with only_success = FALSE")
  }
  unique(ver_dirs)
}

fp_all_outage_ver_dirs = function(fp_dir, prod_id=NULL,proc_id=NULL, search_file = if (need_backup) "outage_pru.Rds" else "has_outage.txt", need_backup=TRUE) {
  restore.point("fp_all_outage_ver_dirs")
  fp_all_ver_dirs(fp_dir, prod_id=prod_id, proc_id=proc_id, search_file=search_file)
}



fp_all_error_ver_dirs = function(fp_dir, prod_id=NULL,proc_id=NULL, search_file = if (need_backup) "error_pru.Rds" else "has_error.txt",  need_backup=FALSE) {
  fp_all_ver_dirs(fp_dir, prod_id=prod_id, proc_id=proc_id, search_file=search_file)
}

fp_has_prod_df = function(ver_dir) {
  file.exists(file.path(ver_dir, "prod_df.Rds"))
}

fp_ver_dir_ok = function(ver_dir, search_file = "prod_df.Rds") {
  file.exists(file.path(ver_dir, search_file))
}

fp_all_ok_ver_dirs = function(fp_dir, prod_id=NULL,proc_id=NULL, search_file = "prod_df.Rds") {
  fp_all_ver_dirs(fp_dir, prod_id, proc_id, search_file=search_file)
}



fp_all_ver_dirs = function(fp_dir, prod_id=NULL,proc_id=NULL, ver_ind = NULL, search_file = "prod_df.Rds", strict_fp_dir=FALSE) {
  restore.point("fp_all_ver_dirs")
  parent_dir = fp_dir
  if (!is.null(prod_id) & strict_fp_dir) parent_dir = file.path(fp_dir, prod_id)
  if (!is.null(prod_id) & !is.null(proc_id) & strict_fp_dir) parent_dir = file.path(fp_dir, proc_id)

  if (!is.null(search_file)) {
    files = list.files(parent_dir, glob2rx(search_file),full.names = TRUE, recursive=TRUE)
    ver_dirs = dirname(files)
  } else {
    stop("Not yet implemented with only_success = FALSE")
  }
  if (!is.null(prod_id) & !strict_fp_dir) {
    prod_ids = fp_ver_dir_to_prod_id(ver_dirs)
    ver_dirs = ver_dirs[prod_ids %in% prod_id]
  }
  if (!is.null(proc_id) & !strict_fp_dir) {
    proc_ids = fp_ver_dir_to_proc_id(ver_dirs)
    ver_dirs = ver_dirs[proc_ids %in% proc_id]
  }
  if (!is.null(ver_ind)) {
    ver_ind_str = paste0("v", ver_ind)
    ver_dirs[basename(ver_dirs) %in% ver_ind_str]
  }

  unique(ver_dirs)
}


fp_all_ver_info = function(fp_dir, prod_id=NULL,proc_id=NULL, search_file = "prod_df.Rds", strict_fp_dir=FALSE) {
  restore.point("fp_all_ver_info")
  if (!is.null(prod_id) & strict_fp_dir) fp_dir = file.path(fp_dir, prod_id)
  if (!is.null(prod_id) & !is.null(proc_id) & strict_fp_dir) fp_dir = file.path(fp_dir, proc_id)

  if (!is.null(search_file)) {
    files = list.files(fp_dir, glob2rx(search_file),full.names = TRUE, recursive=TRUE)
    if (NROW(files)==0) return(NULL)

    ver_dirs = dirname(files)
    if (!is.null(prod_id) & !strict_fp_dir) {
      prod_ids = fp_ver_dir_to_prod_id(ver_dirs)
      ver_dirs = ver_dirs[prod_ids %in% prod_id]
      files = files[prod_ids %in% prod_id]
    }
    if (!is.null(proc_id) & !strict_fp_dir) {
      proc_ids = fp_ver_dir_to_proc_id(ver_dirs)
      ver_dirs = ver_dirs[proc_ids %in% proc_id]
      files = files[proc_ids %in% proc_id]
    }
    if (NROW(ver_dirs)==0) return(NULL)

    df = fp_ver_dir_to_ids(ver_dirs) %>%
      mutate(
        mtime = file.mtime(files),
        prod_df_file = files,
        prod_df_mb = file.size(files) / 1e6
      )
    return(df)

  } else {
    stop("Not yet implemented with only_success = FALSE")
  }
}


fp_rerun_error_ver = function(ver_dir, pru_file = file.path(ver_dir, "error_pru.Rds")) {
  if (!file.exists(pru_file)) {
    cat("\n", pru_file, " does not exist. Cannot re-run.\n")
    return(NULL)
  }
  pru = readRDS(pru_file)
  pru_rerun(pru)
}

fp_rerun_outage_ver = function(ver_dir,pru_file = file.path(ver_dir, "outage_pru.Rds")) {
  fp_rerun_error_ver(ver_dir, pru_file)
}

fp_rerun_all_outage_ver = function(fp_dir, prod_id=NULL,proc_id=NULL, ver_dirs = fp_outage_ver_dirs(fp_dir, prod_id, proc_id,need_backup = TRUE)) {
  restore.point("fp_rerun_all_outage_ver")
  if (length(ver_dirs)==0) return("\nNo outage version found.")
  for (ver_dir in ver_dirs) {
    fp_rerun_outage_ver(ver_dir)
  }
}

fp_rerun_all_error_ver = function(fp_dir, prod_id=NULL,proc_id=NULL, ver_dirs = fp_error_ver_dirs(fp_dir, prod_id, proc_id,need_backup = TRUE)) {
  restore.point("fp_rerun_all_outage_ver")
  if (length(ver_dirs)==0) return("\nNo outage version found.")
  for (ver_dir in ver_dirs) {
    fp_rerun_error_ver(ver_dir)
  }
}

fp_proc_dir_to_new_ver_dir = function(proc_dir, to_v0=TRUE) {
  if (to_v0) {
    ver_ind = 0
  } else {
    ver_dirs = list.dirs(proc_dir, full.names=FALSE, recursive=TRUE)
    if (length(ver_dirs)==0) {
      ver_ind = 1
    } else {
      ver_nums = as.integer(substr(ver_dirs, 2))
      max_ver = max(ver_nums)
      ver_ind = max_ver+1
    }
  }
  ver_dir = paste0(proc_dir, "/v", ver_ind)
  ver_dir
}

fp_save_prod_df = function(prod_df, ver_dir, overwrite=overwrite) {
  if (!dir.exists(ver_dir)) dir.create(ver_dir, recursive = TRUE)
  saveRDS(prod_df, file.path(ver_dir, "prod_df.Rds"))
}
```
# END OF FILE: fp_dirs.R

-----------------------------------------------------------


# FILE: fp_err.R
```

fp_is_err_obj = function(x) {
  is(x,"fp_err_obj")
}

fp_has_err_obj = function(li) {
  any(sapply(li, fp_is_err_obj))
}

fp_err_obj = function(msg, show=TRUE) {
  x = list(msg=msg)
  class(x) = c("fp_err_obj","list")
  if (show) cat(msg)
  x
} 
```
# END OF FILE: fp_err.R

-----------------------------------------------------------


# FILE: fp_load_prod_df.R
```

fp_load_all_prod_df = function(fp_dir, prod_id, add_ids=TRUE, as_df = TRUE, add_ver_dir=FALSE) {
  restore.point("fp_load_all_prod_df")
  ver_dirs = fp_all_ver_dirs(fp_dir, prod_id)
  df_li = lapply(ver_dirs, fp_load_prod_df, add_ids=add_ids, add_ver_dir=add_ver_dir)
  if (!as_df) return(df_li)
  df = bind_rows(df_li)
  df
}


fp_load_prod_df = function(ver_dir = ver_dir, prod_df=NULL, add_ids=FALSE, extra_cols=NULL, add_ver_dir=FALSE) {
  restore.point("fp_load_prod_df")
  if (!is.null(prod_df)) return(prod_df)
  prod_df = readRDS(file.path(ver_dir, "prod_df.Rds"))
  if (add_ids) {
    id_df = fp_ver_dir_to_ids(ver_dir)
    prod_df = add_col_left(prod_df, proc_id=id_df$proc_id, ver_id=id_df$ver_id)
  }
  if (!is.null(extra_cols)) {
    prod_df = add_col_left(prod_df, extra_cols)
  }
  if (add_ver_dir & !is.null(prod_df)) {
    prod_df$ver_dir = rep(ver_dir, NROW(prod_df))
  }

  prod_df

}

```
# END OF FILE: fp_load_prod_df.R

-----------------------------------------------------------


# FILE: fp_patch.R
```
#' Get the proc_id for a revised version
#'
#' Increments the revision number in a process ID.
#' E.g., 'proc-id' -> 'r1_proc-id', 'r1_proc-id' -> 'r2_proc-id'.
#'
#' @param base_proc_id The process ID of the base version.
#' @return A character string with the new process ID for the revised version.
#' @export
fp_get_revised_proc_id = function(base_proc_id) {
  restore.point("fp_get_revised_proc_id")
  if (stringi::stri_startswith_fixed(base_proc_id, "r") &&
      stringi::stri_detect_regex(base_proc_id, "^r[0-9]+_")) {

    rev_num_str = stringi::stri_extract_first_regex(base_proc_id, "[0-9]+")
    rev_num = as.integer(rev_num_str) + 1

    rest = stringi::stri_sub(base_proc_id, from = nchar(rev_num_str) + 3)

    return(paste0("r", rev_num, "_", rest))
  } else {
    return(paste0("r1_", base_proc_id))
  }
}

#' Create a revised product version by applying a patch via group replacement
#'
#' This function creates a new, revised product version by applying a patch.
#' It works by replacing entire groups of rows from the base data frame with
#' corresponding groups from the patch data frame, based on a key column.
#' This method is suitable when a patch can change the number of rows within a group.
#'
#' @param base_ver_dir The version directory of the original product.
#' @param patch_ver_dir The version directory of the product containing corrections.
#' @param revised_ver_dir The directory where the new, revised version will be saved.
#' @param key_col The column name to group by for patching (e.g., "regid").
#' @return The path to the directory of the newly created revised version, invisibly.
#' @export
fp_create_revised_version = function(base_ver_dir, patch_ver_dir, revised_ver_dir, key_col) {
  restore.point("fp_create_revised_version")

  # 1. Load data
  base_df = fp_load_prod_df(base_ver_dir)
  patch_df = fp_load_prod_df(patch_ver_dir)

  if (!key_col %in% names(base_df) || !key_col %in% names(patch_df)) {
    stop(paste0("Key column '", key_col, "' not found in both data frames."))
  }

  # 2. Handle patch counter (.times_patched)
  if (!".times_patched" %in% names(base_df)) {
    base_df$.times_patched = 0L
  }

  base_counts = base_df %>%
    dplyr::group_by(dplyr::across(dplyr::all_of(key_col))) %>%
    dplyr::summarize(.old_patched_count = max(.data$.times_patched, 0L, na.rm = TRUE), .groups = "drop")

  patch_df_with_counts = patch_df %>%
    dplyr::left_join(base_counts, by = key_col) %>%
    dplyr::mutate(
      .times_patched = tidyr::replace_na(.data$.old_patched_count, 0L) + 1L
    ) %>%
    dplyr::select(-.data$.old_patched_count)

  # 3. Create revised data frame by replacing patched parts
  keys_to_patch = unique(patch_df[[key_col]])
  base_df_unpatched_part = base_df %>%
    dplyr::filter(!.data[[key_col]] %in% keys_to_patch)

  revised_df = dplyr::bind_rows(base_df_unpatched_part, patch_df_with_counts)

  # 4. Save revised version
  if (!dir.exists(revised_ver_dir)) {
    dir.create(revised_ver_dir, recursive = TRUE)
  }

  fp_save_prod_df(revised_df, revised_ver_dir)

  # Save metadata for provenance
  revision_info = list(
    type = "revised_version_by_group_replace",
    base_ver_dir = base_ver_dir,
    patch_ver_dir = patch_ver_dir,
    key_col = key_col,
    revision_time = Sys.time()
  )
  saveRDS(revision_info, file.path(revised_ver_dir, "revision_info.Rds"))

  cat("\nSuccessfully created revised version in:", revised_ver_dir, "\n")
  return(invisible(revised_ver_dir))
}
```
# END OF FILE: fp_patch.R

-----------------------------------------------------------


# FILE: fp_report.R
```
# FILE: fp_report.R
# Make a an HTML report of fuzzy products


# ver_dirs will typically be generated by fp_all_ver_dirs

# Look at all prod_df in the ver_dirs and generate HTML report
# Arrange by prod_id, proc_id, ver_id
# Show prod_df data frames in as compact, striped HTML tables with y-scroll and x-scroll enabled. Possibly limit to max_rows and max_cols


# Helper function to escape HTML special characters
# Not exported.
.html_escape = function(text) {
  text = as.character(text)
  if (length(text) == 0) return("")
  text[is.na(text)] = ""
  return(text)
}

# Helper function to generate an HTML table from a data frame
# Not exported.
# Helper function to generate an HTML table from a data frame
# Not exported.
.df_to_html_table = function(df) {
  if (is.null(df) || NROW(df) == 0) {
    return("<p><em>No data available.</em></p>")
  }

  # Header
  header_html <- paste0("<th>", .html_escape(names(df)), "</th>", collapse = "")
  header_html <- paste0("<thead><tr>", header_html, "</tr></thead>")

  # Body - process column by column in a vectorized manner
  td_cols <- lapply(df, function(col) {

    # 1. Handle list-columns (for nested data)
    if (is.list(col)) {
      # Convert each list element to a pretty JSON string
      cell_content <- sapply(col, function(item) {
        # Explicitly handle NA, which would otherwise become "null" in JSON
        if (is.atomic(item) && length(item) == 1 && is.na(item)) {
          return(NA_character_)
        }
        # Represent nulls or empty lists/dfs as empty JSON array for consistency
        if (is.null(item) || (is.list(item) && length(item) == 0) || (is.data.frame(item) && nrow(item) == 0)) {
           return("[]")
        }
        # Convert to JSON
        jsonlite::toJSON(item, pretty = TRUE, auto_unbox = TRUE)
      })

      is_na <- is.na(cell_content)
      escaped_content <- .html_escape(cell_content)
      html_cells <- ifelse(is_na, "<em>NA</em>", paste0("<pre>", escaped_content, "</pre>"))

    # 2. Handle atomic columns (including stringified JSON from diff reports)
    } else {
      cell_content <- as.character(col)
      is_na <- is.na(cell_content)
      # Check for strings that look like JSON objects or arrays
      is_json_like <- !is_na & (startsWith(cell_content, "{") | startsWith(cell_content, "["))
      escaped_content <- .html_escape(cell_content)
      # Apply formatting conditionally
      html_cells <- ifelse(is_na, "<em>NA</em>",
                           ifelse(is_json_like, paste0("<pre>", escaped_content, "</pre>"),
                                  escaped_content))
    }
    # Wrap all processed cells in <td> tags
    paste0("<td>", html_cells, "</td>")
  })

  # Combine columns into rows using vectorized paste
  if (length(td_cols) > 0) {
    body_rows <- do.call(paste0, td_cols)
  } else {
    body_rows <- rep("", NROW(df)) # Handles df with 0 columns
  }


  # Wrap each row in <tr> tags
  body_html <- paste0("<tr>", body_rows, "</tr>", collapse = "\n")
  body_html <- paste0("<tbody>\n", body_html, "\n</tbody>")

  # Final assembly
  paste(
    "<table>",
    header_html,
    body_html,
    "</table>",
    collapse = "\n"
  )
}

fp_report_css = function() {
  css = "
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; margin: 2em; color: #333; }
    h1 { color: #111; border-bottom: 2px solid #ccc; padding-bottom: 0.1em; }
    h2 { background-color: #eef; color: #333; padding: 0.1em; border-left: 5px solid #66b; margin-top: 0.1em;}
    .table-container { max-height: 600px; max-width: 100%; overflow: auto; border: 1px solid #ccc; margin-bottom: 0.5em; }
    table { border-collapse: collapse; font-size: 0.8em; white-space: nowrap; }
    th, td { border: 1px solid #ddd; padding: 4px 6px; text-align: left; vertical-align: top; }
    th { background-color: #f2f2f2; position: sticky; top: 0; z-index: 1; }
    tr:nth-child(even) { background-color: #f9f9f9; }
    tr:hover { background-color: #f1f1f1; }
    .trunc-info { font-style: italic; color: #888; font-size: 0.8em; margin-bottom: 1em;}
    em { color: #999; }
  "

}


#' Generate an HTML report for fuzzy products
#'
#' This function takes a set of version directories, loads the `prod_df.Rds`
#' from each, and compiles them into a single HTML report file. The report
#' is organized by product, process, and version. Each data frame is
#' displayed in a scrollable, styled table.
#'
#' @param ver_dirs A character vector of version directories. Typically generated by `fp_all_ok_ver_dirs`.
#' @param outfile The path for the output HTML file.
#' @param prods Optionally a list of product definitions (from `prods_define`) to determine the order of products in the report.
#' @param max_rows The maximum number of rows to display for each table.
#' @param max_cols The maximum number of columns to display for each table.
#' @param title The title of the HTML report.
#' @return The path to the created `outfile`, invisibly.
fp_html_report = function(ver_dirs, outfile=NULL, prods = NULL, max_rows=100, max_cols=50, title = "Fuzzy Production Report") {
  restore.point("fp_html_report")


  if (length(ver_dirs) == 0) {
    cat("No version directories provided to fp_html_report.")
    return(invisible(NULL))
  }

  if (is.null(outfile)) {
    fp_dir = fp_ver_dir_to_fp_dir(ver_dirs[1])
    outfile = file.path(fp_dir, "fp_report.html")
  }

  # 1. Get info for all ver_dirs and sort them
  info_df = fp_ver_dir_to_ids(ver_dirs)

  # Determine sorting order based on prods list if provided
  if (!is.null(prods)) {
      all_prod_ids = unique(info_df$prod_id)
      # Order by prods list, but only include products present in ver_dirs
      prod_order_from_prods = intersect(names(prods), all_prod_ids)
      # Add any other products from ver_dirs not in the prods list, sorted alphabetically
      other_prod_ids = sort(setdiff(all_prod_ids, prod_order_from_prods))
      final_prod_order = c(prod_order_from_prods, other_prod_ids)

      # Use a factor to enforce the custom sort order for prod_id
      info_df$prod_id = factor(info_df$prod_id, levels = final_prod_order)
  }

  # Sort the info data frame (by factor levels if prods was given, else alphabetically)
  info_df = dplyr::arrange(info_df, .data$prod_id, .data$proc_id, .data$ver_ind)


  # 2. Start building HTML content
  css = fp_report_css()

  html_head = paste0(
    "<!DOCTYPE html><html><head><meta charset='UTF-8'><title>", .html_escape(title), "</title>",
    "<style>", css, "</style></head><body><h1>", .html_escape(title), "</h1>"
  )

  html_body_parts = c()

  # 3. Iterate through sorted dirs, load data, and generate HTML for each table
  for (i in 1:NROW(info_df)) {
    row = info_df[i, ]
    ver_dir = row$ver_dir

    header = paste(as.character(row$prod_id), row$proc_id, paste0("v", row$ver_ind), sep = " / ")
    html_body_parts = c(html_body_parts, paste0("<h2>", .html_escape(header), "</h2>"))

    # Load prod_df
    df = tryCatch({
      fp_load_prod_df(ver_dir)
    }, error = function(e) {
      warning("Could not load prod_df from ", ver_dir, ": ", e$message)
      return(NULL)
    })

    if (is.null(df)) {
       html_body_parts = c(html_body_parts, "<p><em>Could not load data.</em></p>")
       next
    }

    # Truncate if necessary
    orig_rows = NROW(df)
    orig_cols = NCOL(df)

    df_display = df
    truncated = FALSE
    trunc_msg = c()

    if (orig_rows > max_rows) {
      df_display = df_display[1:max_rows, ]
      trunc_msg = c(trunc_msg, paste0("rows truncated to ", max_rows, " (from ", orig_rows, ")"))
      truncated = TRUE
    }
    if (orig_cols > max_cols) {
      df_display = df_display[, 1:max_cols, drop = FALSE]
      trunc_msg = c(trunc_msg, paste0("columns truncated to ", max_cols, " (from ", orig_cols, ")"))
      truncated = TRUE
    }

    # Generate table HTML
    table_html = .df_to_html_table(df_display)
    html_body_parts = c(html_body_parts, "<div class='table-container'>", table_html, "</div>")

    # Add truncation info message if needed
    if (truncated) {
        msg = paste0("<p class='trunc-info'>Note: ", paste(trunc_msg, collapse=", "), ".</p>")
        html_body_parts = c(html_body_parts, msg)
    }
  }

  html_foot = "</body></html>"

  # 4. Combine all parts and write to file
  final_html = paste(c(html_head, html_body_parts, html_foot), collapse="\n")
  writeLines(final_html, outfile)

  cat("HTML report written to:", outfile, "\n")
  invisible(outfile)
}
```
# END OF FILE: fp_report.R

-----------------------------------------------------------


# FILE: fp_tools.R
```

add_col_left = function(df, ...) {
  args = list(...)
  restore.point("add_col_left")

  len = NROW(df)
  args = lapply(args, function(x) rep(x, length=len))
  bind_cols(as_tibble(args), df)
}


na_val = function(x, val=0) {
  x[is.na(x)] = val
  x
}

first_nn = function (...)
{
  args = list(...)
  for (val in args) {
    if (!is.null(val))
      return(val)
  }
  return(NULL)
}

file_remove_existing = function(file) {
  if (file.exists(file)) file.remove(file)
}

rename_cols = function (x, old_cols, new_cols)
{
  restore_point("rename_cols")
  inds = match(old_cols, names(x))
  old_cols = old_cols[!is.na(inds)]
  new_cols = new_cols[!is.na(inds)]
  inds = inds[!is.na(inds)]
  names(x)[inds] = new_cols
  x
}


select_cols = function(df, cols) {
  restore.point("select_cols")
  use = cols %in% names(df)
  cols = cols[use]
  df = df[, cols]
  if (!is.null(names(cols))) {
    names(df) = names(cols)
  }
  df
}
```
# END OF FILE: fp_tools.R

-----------------------------------------------------------


# FILE: pick_prod.R
```
# Functions that help to pick and load particular versions of a product

example = function() {
  parent_dir =  "~/repbox/projects_share"

  project_dirs = repboxExplore::get_project_dirs(parent_dir)
  res_df = fp_pick_prod_ver(parent_dir, c("readme_var", "readme_overview"))
  res_df = fp_pick_prod_ver(project_dirs, "readme_var")

  pref = fp_pref(proc_regex = c("_mocr$"))
  ver_df = fp_pick_prod_ver(project_dirs, "tab_html", pref = pref)
  ver_df

  df = fp_pick_and_load_prod_df(project_dirs,"readme_overview")
}

#' Picks a single product version
#'
#' Allows for fp_dir to be a large directory with multiple
fp_pick_prod_ver = function(parent_dir, prod_id, proc_id=NULL, pref=fp_pref(), return_score_df = FALSE) {
  restore.point("fp_pick_prod_ver")
  pref = fp_normalize_pref(pref)
  #prod_dir = file.path(fp_dir, prod_id)

  df = fp_all_ver_info(parent_dir, prod_id,proc_id = proc_id)
  if (NROW(df)==0) {
    return(fp_err_obj(paste0("\nNo version of product ", prod_id, " found in ", parent_dir,"\n")))
    return(NULL)
  }
  df$prod_dir = fp_ver_dir_to_prod_dir(df$ver_dir)

  match_score = function(ver_val, pref_val) {
    if (length(pref_val)==0) {
      return(rep(0, length(ver_val)))
    }
    -na_val(match(ver_val, pref_val), length(ver_val)+1)
  }

  if (!is.null(pref$proc_regex)) {
    for (regex in pref$proc_regex) {
      new_ids =  df$proc_id[stri_detect_regex(df$proc_id, regex)]
      pref$proc_id = union(pref$proc_id,new_ids)
    }
  }

  df$score_proc_id = match_score(df$proc_id, pref$proc_id)
  df$score_ver_ind = match_score(df$ver_ind, pref$ver_ind)
  df$score_younger = as.numeric(df$mtime) / 1e6

  arrange_cols = paste0("score_",pref$order_by)
  df_sorted <- df %>%
    group_by(prod_dir) %>%
    arrange(across(all_of(arrange_cols), desc))
  if (return_score_df) return(ungrouo(df_sorted))

  res_df = df_sorted %>%
    slice(1) %>%
    ungroup()
  res_df
}



fp_pick_and_load_prod_df = function(parent_dir, prod_id,proc_id=NULL, pref=fp_pref(),add_ids=TRUE, extra_cols=NULL, add_fp_dir=TRUE) {
  restore.point("fp_pick_and_load_prod_df")
  ver_df =fp_pick_prod_ver(parent_dir, prod_id, proc_id, pref)
  if (NROW(ver_df)==0) return(NULL)
  df_li = lapply(seq_len(NROW(ver_df)), function(i) {
    ver_dir = ver_df$ver_dir[[i]]
    df = fp_load_prod_df(ver_dir, add_ids=add_ids, extra_cols=extra_cols)
    if (add_fp_dir) {
      fp_dir = fp_ver_dir_to_fp_dir(ver_dir)
      df = add_col_left(df, fp_dir=fp_dir)
    }
    df
  })
  bind_rows(df_li)
}


fp_load_newest_prod_df = function(fp_dir, prod_id, proc_id, add_ids=TRUE) {
  ver_dir = fp_newest_ver_dir(fp_dir, prod_id, proc_id)
  fp_load_prod_df(ver_dir,add_ids=add_ids)
}


#' create a product version preference
#'
#' TO DO: expand function in future
fp_pref = function(order_by = c("proc_id","ver_ind","younger"), proc_id = NULL, ver_ind=NULL, proc_regex=NULL) {
  pref = list(order_by=order_by, proc_id=proc_id, ver_ind=ver_ind, proc_regex=proc_regex)
  class(pref) = c("fp_pref","list")
  pref
}

fp_normalize_pref = function(pref) {
  if (is.character(pref)) {
    pref = fp_pref(proc_regex = pref)
  }
  pref
}

fp_has_prod = function(fp_dir,prod_id, proc_id=NULL) {
  dirs = fp_all_ok_ver_dirs(fp_dir, prod_id,proc_id)
  length(dirs) > 0
}


# Pick input versions and create / update pru$input_infos
pru_pick_inputs = function(pru, prod_ids, pref=fp_pref(), load=TRUE) {
  restore.point("pru_pick_input_version")
  fp_dir = pru$fp_dir

  input_infos = lapply(prod_ids, function(prod_id) {
    fp_pick_prod_ver(fp_dir, prod_id, pref)
  })
  names(input_infos) = prod_ids
  if (is.null(pru$input_infos)) {
    pru$input_infos = input_infos
  } else {
    pru$input_infos[names(input_infos)] = input_infos
  }
  if (load) pru = pru_load_inputs(pru, prod_ids)
  pru
}

pru_has_input_err = function(pru) {
  if (fp_has_err_obj(pru$input$infos)) return(TRUE)
  #if (fp_has_err_obj(pru$inputs)) return(TRUE)
  return(FALSE)
}

pru_get_input = function(pru, prod_id)  {
  pru$inputs[[prod_id]]
}

pru_load_inputs = function(pru, prod_ids=names(pru$input_infos), overwrite=FALSE) {
  if (is.null(pru$inputs)) {
    pru$inputs = list()
  }
  if (!overwrite) {
    prod_ids = setdiff(prod_ids, names(pru$inputs))
  }
  for (prod_id in prod_ids) {
    info = pru$input_infos[[prod_id]]
    if (is.null(info)) stop(paste0("No input version for ", prod_id, " had been picked. First call pru_pick_input"))
    pru$inputs[[prod_id]] = readRDS(info$prod_df_file)
  }
  pru
}

```
# END OF FILE: pick_prod.R

-----------------------------------------------------------


# FILE: preserve_call.R
```
# Function to preserve a function call in a serializable format
preserve_call <- function(fun_name=NULL) {
  # Get the parent frame
  parent_frame <- parent.frame(1)
  
  # Get function from parent frame
  parent_function <- sys.function(-1)
  
  # Try to find the function name by matching in the global environment
  fun_name <- NULL
  if (is.null(fun_name)) {
    for (name in ls(.GlobalEnv)) {
      if (identical(parent_function, get(name, envir = .GlobalEnv))) {
        fun_name <- name
        break
      }
    }
  }
  
  # If we couldn't find it in the global environment, check the search path
  if (is.null(fun_name)) {
    for (pkg in search()) {
      if (pkg == ".GlobalEnv") next # Already checked
      pkg_env <- as.environment(pkg)
      for (name in ls(pkg_env)) {
        if (identical(parent_function, get(name, envir = pkg_env))) {
          fun_name <- name
          break
        }
      }
      if (!is.null(fun_name)) break
    }
  }
  
  # Throw error if function name could not be determined
  if (is.null(fun_name)) {
    stop("Could not determine function name. Function must be defined in the global environment or a loaded package.")
  }
  
  # Get formal arguments of the parent function to know what to capture
  fun_formals <- names(formals(parent_function))
  
  # Get the values of those arguments from the parent frame
  args_to_capture <- mget(fun_formals, envir = parent_frame, ifnotfound = list(NULL))
  
  # Create the preserved call object
  fun_call <- list(
    fun_name = fun_name,
    args = args_to_capture
  )
  
  class(fun_call) <- "preserved_call"
  return(fun_call)
}

# Function to run a preserved call
run_preserved_call <- function(fun_call) {
  if (!inherits(fun_call, "preserved_call")) {
    stop("Input must be a preserved_call object")
  }
  
  # Look up the function by name
  fun_to_call <- tryCatch({
    get(fun_call$fun_name, envir = .GlobalEnv)
  }, error = function(e) {
    # Try to find it in the search path
    for (pkg in search()) {
      if (exists(fun_call$fun_name, where = pkg, mode = "function")) {
        return(get(fun_call$fun_name, pos = pkg))
      }
    }
    stop(paste("Function", fun_call$fun_name, "not found. Make sure it exists in the global environment or a loaded package."))
  })
  
  # Execute the call with do.call
  do.call(fun_to_call, fun_call$args)
}
```
# END OF FILE: preserve_call.R

-----------------------------------------------------------


# FILE: prod.R
```
example = function() {

}

prod_to_rclasses = function(prod) {
  schema_fields_to_rclasses(prod$fields)
}

prod_to_json_schema = function(prod,obj_arr=c("obj","arr")[1], add_description=TRUE, allow_null_def=TRUE) {
  restore.point("prod_to_json_schema")
  schema = prod_to_schema(prod,obj_arr)
  DataSchema::to_json_schema(schema,add_description = TRUE, allow_null_def=allow_null_def)
}

prod_to_schema = function(prod, obj_arr=c("obj","arr")[1]) {
  if (obj_arr=="arr") {
    schema_arr(schema_obj(prod$fields))
  } else {
    schema_obj(prod$fields)
  }
}

prod_empty_df = function(prod) {
  as_tibble(lapply(prod$fields, schema_empty_instance))
}

prod_na_df = function(prod, len=1) {
  as_tibble(lapply(prod$fields, schema_na_instance, len))
}



prods_define = function(..., .lists=NULL) {
  prods = list(...)
  restore.point("fp_prods")
  if (!is.null(.lists)) {
    prods = c(prods, do.call(c, .lists))
  }


  names(prods) = sapply(prods, function(prod) prod$prod_id)

  for (i in seq_along(prods)) {
    prod = prods[[i]]
    #if (i == 4) stop()

    if (!is.null(prod$parent)) {
      pprod = prods[[prod$parent]]
      prod$fields = c(pprod$fields[ prod[["from_parent"]] ], prod$fields)
      prod$vars = names(prod$fields)
      prod$parent_keys = pprod$keys
    } else if (!is.null(prod$widens)) {
      #if (length(prod$widens)>1)
      #  restore.point("kshjkdhsfksjflksdlj")
      vars = unique(unlist(lapply(prod$widens, function(w)
        prods[[w]]$vars
      )))
      for (w in rev(prod$widens)) {
        pprod = prods[[w]]
        add_fields = setdiff(pprod$vars, prod$vars)
        prod$fields = c(pprod$fields[add_fields], prod$fields)
        prod$keys = union(prod$keys, pprod$keys)
        prod$vars = names(prod$fields)
      }
      vars = union(vars, names(prod$fields))
      prod$fields = prod$fields[vars]
      prod$vars = vars
    }
    #is_key = sapply(prod$fields, function(field) isTRUE(field$is_key))
    #prod$keys = prod$vars[is_key]
    prods[[i]] = prod
  }
  prods
}

prod_define = function(prod_id, fields=NULL, widens=NULL, parent=NULL, from_parent=NULL, descr=NULL,keys=NULL, ...) {
  restore.point("prod_define")
  #fields = lapply(fields, function(field) {
    #field$is_key = first.non.null(field$is_key, FALSE)
  #  field
  #})
  if (is.null(fields)) fields = list()
  prod = list(prod_id=prod_id, fields=fields,keys=keys, vars=names(fields), widens = widens, parent = parent, from_parent=from_parent, descr=descr)
  class(prod) = c("fp_prod","list")
  prod
}


#' Somewhat flexible function to convert results data frames into proper
#' product data frames
df_to_prod_df = function(df, prod,  df_to_prod_cols=NULL,prod_to_df_cols=NULL, prod_df=NULL, convert_class=TRUE) {
  restore.point("df_to_prod_df")
  if (isTRUE(is.character(prod))) stop("prod must be the actual product (an R list) not just the prod_id.")

  if (is(df, "try-error")) return(prod_empty_df(prod))
  len = NROW(df)
  if (len==0) return(prod_empty_df(prod))

  if (is.null(df_to_prod_cols) & !is.null(prod_to_df_cols)) {
    df_to_prod_cols = invert_names_values(prod_to_df_cols)
  } else if (is.null(df_to_prod_cols)) {
    df_to_prod_cols = intersect(names(df), prod$vars)
  }


  # Not all prod cols are in df
  mod_df = select_cols(df, df_to_prod_cols)
  if (NCOL(mod_df)<length(prod$vars)) {
    if (is.null(prod_df)) prod_df = prod_na_df(prod, len)
    prod_df[names(mod_df)] = mod_df
  } else {
    prod_df = mod_df
  }
  # Ensures correct order and possibly removes extra variables
  if (!identical(names(prod_df), prod$vars))
    prod_df = prod_df[, prod$vars]

  if (convert_class) {
    prod_df = prod_set_df_col_class(prod_df, prod)
  }
  prod_df
}

prod_set_df_col_class = function(df, prod) {
  schema = prod_to_schema(prod)
  schema_set_df_col_class(df, schema)

}
```
# END OF FILE: prod.R

-----------------------------------------------------------


# FILE: prod_tests.R
```

example = function() {
  library(repboxAI)
  run_dir = "~/repbox/projects_share/aejapp_1_2_4/rai/prod_runs/cell_base/tab_html_hx_pdf/r0"
  prod_df = rai_load_prod_df(run_dir=run_dir)

  library(restorepoint)
  library(dplyr)
  #df = readRDS("C:/libraries/FuzzyProduction/prod_df.Rds")
  tests = tests_cell_base()
  prod_run_tests(tests, prod_df, keys="tabid")
  prod_run_tests(tests, prod_df, return_details=TRUE, keys="cellid")
}

prod_run_tests = function(prod_tests,prod_df, prod=NULL, return_details=FALSE, keys=first.non.null(prod_tests$keys,prod$keys)) {
  restore.point("prod_run_tests")

  res_li = lapply(prod_tests$fun, prod_run_test_fun, prod_df=prod_df, prod=prod, return_details = return_details, keys=keys)
  if (length(res_li)==0) return(NULL)
  if (!return_details) {
    res = as.data.frame(do.call(c,res_li))
  } else if (return_details & is.null(keys)) {
    res = do.call(bind_cols, res_li)
  } else {
    res = res_li[[1]]
    for (i in setdiff(seq_len(length(res_li)),1) ) {
      res = full_join(res, res_li[[i]], by=keys)
    }
  }
  res
}

prod_run_test_fun = function(test_fun, prod_df, prod=prod, return_details=FALSE, keys=prod$keys) {
  restore.point("prod_run_test_fun")
  df = test_fun(prod_df, prod)
  if (is(test_fun, "flag_test_fun")) {
    test_prefix = "flag_"
  } else {
    stop("unknown test function of class ", class(test_fun)[1])
  }
  cols = names(df)

  key_cols = intersect(keys, cols)
  test_cols = cols[startsWith(cols, test_prefix)]
  details_df = df[,c(key_cols, test_cols)]

  if (is(test_fun, "flag_test_fun")) {
    i = 1
    for (i in seq_along(test_cols)) {
      test = stri_sub(test_cols[i],nchar(test_prefix)+1)
      rel_col = paste0("relevant_", test)

      if (!rel_col %in% cols) next

      details_df[[ test_cols[i] ]] = ifelse(is.true(df[[rel_col]]),details_df[[ test_cols[i] ]], NA_integer_)
    }
    for (i in seq_along(test_cols)) {
      details_df[[ test_cols[i] ]] = add_class(details_df[[ test_cols[i] ]], "flag_test")
    }
  }
  details_df = rename.cols(details_df, test_cols, stri_sub(test_cols, nchar(test_prefix)+1))
  if (return_details) return(details_df)
  return(prod_agg_test_details(details_df, keys=keys))
}

prod_agg_test_details = function(details, keys=NULL) {
  restore.point("prod_add_test_details")
  cols = setdiff(names(details), keys)

  agg = lapply(details[cols], function(vals) {
    num_rel = sum(!is.na(vals))
    ifelse(num_rel == 0,0, sum(vals, na.rm=TRUE) / num_rel)
  })
  agg
}


flag_test_funs = function(...) {
  funs = list(...)
  funs = lapply(funs, add_class, new_class = "flag_test_fun")
  funs
}

prod_tests_define = function(..., descr=NULL, keys=NULL) {
  funs = unlist(list(...))
  restore.point("prod_tests_define")
  x = list(funs=funs, descr=descr, keys=keys)
  x
}
```
# END OF FILE: prod_tests.R

-----------------------------------------------------------


# FILE: prod_tools.R
```

add_class = function(x, new_class=NULL) {
  class(x) = union(new_class, class(x))
  x
}
```
# END OF FILE: prod_tools.R

-----------------------------------------------------------


# FILE: pru.R
```
#' Initialize a production run
pru_init = function(fp_dir,prod_id, proc_id=proc_info$proc_id, to_v0=TRUE, ver_dir=NULL,proc_info=NULL, ..., pru=NULL) {
  if (is.null(pru)) {
    pru = list(fp_dir=fp_dir,prod_id=prod_id, proc_id=proc_id, to_v0=to_v0, proc_info=proc_info, ...)
  } else {
    copy.into.env(pru)
  }

  restore.point("pru_init")
  pru = pru_init_dirs(pru, ver_dir=ver_dir)

  if (length(pru$proc_dir)==0) stop("proc_dir not correctly specified.")
  pru
}

pru_init_dirs = function(pru, ver_dir=pru[["ver_dir"]], to_v0 = pru[["to_v0"]]) {
  restore.point("pru_init_dirs")
  fp_dir = pru$fp_dir
  proc_id = pru$proc_id
  prod_id = pru$prod_id

  if (is.null(ver_dir)) {
    pru$proc_dir = proc_dir = file.path(fp_dir,pru$prod_id, pru$proc_id)
    if (to_v0) {
      pru$ver_ind = 0
    } else {
      ver_dirs = list.dirs(proc_dir, full.names=FALSE, recursive=FALSE)
      if (length(ver_dirs)==0) {
        pru$ver_ind = 1
      } else {
        ver_nums = as.integer(stri_sub(ver_dirs, 2))
        max_ver = max(ver_nums)
        pru$ver_ind = max_ver+1
      }
    }
    pru$ver_id = paste0(prod_id, "-", proc_id, "--v", pru$ver_ind)
    pru$ver_dir = paste0(pru$proc_dir, "/v", pru$ver_ind)
  } else {
    pru$proc_dir = proc_dir = dirname(ver_dir)
    pru$proc_id = basename(pru$proc_dir)
    pru$ver_dir = ver_dir
    pru$ver_id = basename(ver_dir)
    pru$ver_ind = as.integer(stri_sub(pru$ver_id, 2))
  }
  pru
}

# Save a temporary pru object for debugging purposes
temp_pru_save = function(pru) {
  if (!dir.exists(pru$ver_dir)) dir.create(pru$ver_dir, recursive = TRUE)
  saveRDS(pru, file.path(pru$ver_dir, "temp_pru.Rds"))
}

pru_save = function(pru, prod_df, save_fields=c("proc_info","issues"), save_pru=TRUE) {
  restore.point("pru_save")
  if (!dir.exists(pru$ver_dir)) dir.create(pru$ver_dir, recursive = TRUE)

  ver_dir = pru$ver_dir
  if (save_pru) {
    saveRDS(pru,file.path(ver_dir,"pru.Rds"))
  }
  err_file = file.path(ver_dir, "has_err.txt")
  if (file.exists(err_file)) file.remove(err_file)

  for (field in save_fields) {
    if (!is.null(pru[[field]])) {
      saveRDS(pru[[field]],  file.path(ver_dir, paste0(field,".Rds") ))
    }
  }

  saveRDS(prod_df, file.path(ver_dir, "prod_df.Rds"))
  pru$prod_df = prod_df
  invisible(pru)
}

pru_backport_save = function(pru,bp_prod, prod_df=pru$prod_df) {
  restore.point("pru_backport_save")
  if (is.null(bp_prod)) stop("Need bp_prod.")
  bp_prod_df = df_to_prod_df(prod_df, bp_prod)
  pru$prod_id = bp_prod$prod_id
  if (!is.null(pru$proc_info)) {
    pru$proc_info$prod_id = pru$prod_id
  }
  # proc_id and ver_ind stays same
  pru$proc_dir = file.path(pru$fp_dir,pru$prod_id, pru$proc_id)
  if (length(pru$proc_dir)==0) stop("pru$proc_dir not well defined")
  pru$ver_dir = paste0(pru$proc_dir, "/v", pru$ver_ind)
  bp = pru_save(pru,bp_prod_df)
  invisible(bp)
}


pru_next_stage = function(pru, stage_fn) {
  restore.point("pru_next_stage")
  pru$cur_stage = stage_fn
  pru$items = NULL
  pru$outage_inds = pru$item_status = pru$item_cat = pru$num_outage = pru$num_error =  pru$num_ok  = pru$all_ok = NULL
  do.call(stage_fn, list(pru))
}


pru_make_items = function(pru, run_fun, num_items=NROW(df), fill_outage=TRUE, backup_outage=TRUE, verbose = TRUE, df=NULL, cont_fine_statuss = c("ok","no_status")) {
  restore.point("pru_make_items")
  items = pru[["items"]]
  if (is.null(items)) items = vector("list", num_items)

  if(fill_outage & length(pru$outage_inds)>0) {
    broad_statuss = sapply(items, fp_broad_status)
    inds = which(broad_statuss != "ok")
  } else {
    inds = seq_along(items)
  }
  start_time = as.numeric(Sys.time())
  if (verbose) cat(paste0("\nRun ",NROW(inds)," AI calls "))
  ind = 1
  for (ind in inds) {
    item = run_fun(ind, pru)
    items[[ind]] = item
    status = fp_fine_status(item)
    if (verbose) {
      if (status %in% c("ok")) {
        cat(".")
      } else if (status == "no_status") {
        cat("?")
      } else {
        cat(paste0("\n  item ",ind, ": ", status,"\n"))
      }
    }
    if (!status %in% cont_fine_statuss) {
      break
    }
  }
  pru$items = items
  restore.point("pru_run_all_rai_post")
  if (verbose) cat(paste0(" ", round(as.numeric(Sys.time())-start_time), " sec.\n"))
  pru
}


pru_add_issue = function(pru, type,details="") {
  issue = tibble(type=type, details=details)
  pru$issues = c(pru$issues, list(issue))
  pru
}

pru_rerun = function(pru) {
  restore.point("pru_rerun")
  if (!is.null(pru$cur_stage)) {
    return(pru_next_stage(pru, pru$cur_stage))
  }
  if (is.null(pru[["fun_call"]])) {
    cat("\nNo stage nor fun_call object stored in pru. Cannot re-run it.")
    pru
  }
  run_preserved_call(pru$fun_call)
}
```
# END OF FILE: pru.R

-----------------------------------------------------------


# FILE: pru_status.R
```
# Helper function to deal with unreliable production stages
#
# 3 broad_status:
#
# "ok"
# "outage" e.g. due to rate limit. attempt again in a latter run
# "error"

pru_set_status = function(pru, items=pru[[items]],forbid_any_error=TRUE, forbid_any_outage=TRUE, save_pru_if_outage = TRUE, save_pru_if_error = TRUE) {
  restore.point("pru_set_status")
  fine_status = fp_fine_status(items)
  if (fine_status == "no_status" & isTRUE(is.list(items))) {
    all_broad_status = sapply(items, fp_broad_status)
  } else {
    all_broad_status = fp_fine_status_to_broad_status(fine_status)
  }
  pru$forbid_any_outage = forbid_any_outage
  pru$forbid_any_error = forbid_any_error


  n_all = length(all_broad_status)
  pru$num_error = sum(all_broad_status=="error")
  pru$num_outage = sum(all_broad_status=="outage")
  pru$num_ok = sum(all_broad_status=="ok")

  broad_status = "ok"
  if (n_all == 0) {
    broad_status = "ok" # empty is ok
  } else if ( (forbid_any_error & pru$num_error > 0) | pru$num_error == n_all) {
    broad_status = "error"
  } else if ((forbid_any_outage & pru$num_outage > 0)) {
    broad_status = "outage"
  } else if (pru$num_error + pru$num_outage == n_all) {
    broad_status = "outage"
  } else {
    broad_status = "ok"
  }
  pru$broad_status = broad_status
  file_remove_existing(file.path(pru$ver_dir,"has_outage.txt"))
  file_remove_existing(file.path(pru$ver_dir,"has_error.txt"))
  file_remove_existing(file.path(pru$ver_dir,"error_pru.Rds"))
  file_remove_existing(file.path(pru$ver_dir,"outage_pru.Rds"))

  if (pru$num_outage > 0) {
    if (!dir.exists(pru$ver_dir)) dir.create(pru$ver_dir, recursive = TRUE)
    writeLines("outage", file.path(pru$ver_dir,"has_outage.txt"))
    if (save_pru_if_outage) {
      saveRDS(pru,file.path(pru$ver_dir,"outage_pru.Rds") )
      cat("Could not compute complete version ", pru$ver_dir, " due temporary service inavailability. Can be continued in another run.\n")
    }
  }
  if (broad_status == "error") {
    if (!dir.exists(pru$ver_dir)) dir.create(pru$ver_dir, recursive = TRUE)
    writeLines("error", file.path(pru$ver_dir,"has_error.txt"))
    if (save_pru_if_outage) {
      saveRDS(pru,file.path(pru$ver_dir,"error_pru.Rds") )
      cat("Could not compute complete version ", pru$ver_dir, " service invailabilities. Can be resumed in another run.\n")
    }
  }
  pru$broad_status = broad_status
  pru
}

pru_is_ok = function(pru) {
  !isTRUE(pru$broad_status != "ok")
}


fp_fine_status = function(x) {
  if (is.null(x) | length(x)==0) return("empty")
  first.non.null(x[["fine_status"]], attr(x, "fine_status"), "no_status")
}

fp_broad_status = function(x) {
  restore.point("fp_broad_status")
  broad_status = first.non.null(x[["broad_status"]], attr(x, "broad_status"))
  if (!is.null(broad_status)) return(broad_status)
  fp_fine_status_to_broad_status(fp_fine_status(x))
}


fp_fine_status_to_broad_status = function(fine_status) {
  case_when(
    fine_status %in% c("rate_limit","unavailable", "permission") ~ "outage",
    fine_status %in% c("error", "input_limit", "timeout") ~ "error",
    fine_status == "empty" ~ "empty",
    TRUE ~ "ok"
  )
}
```
# END OF FILE: pru_status.R

-----------------------------------------------------------


# Documentation for repboxAI


# FILE: report_map.R
```
# FILE: report_map.R

example = function() {
  library(repboxReport)
  # The project_dir needs to be set to a valid repbox project
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"

  options(warn=2)
  # Generate report with default options (embedded data)
  opts = rr_map_report_opts(embed_data = FALSE)
  rep_file = rr_map_report(project_dir,opts = opts)
  browseURL(rep_file)
  rstudioapi::filesPaneNavigate(rep_file)

  rstudioapi::filesPaneNavigate(project_dir)


  # Example with external JSON files
  # This report will need to be viewed via a web server.
  opts <- rr_map_report_opts(embed_data = FALSE)
  opts$output_for <- "all"
  output_dir_ext = file.path(project_dir, "reports_external")
  rr_map_report(project_dir, opts = opts, output_dir = output_dir_ext, output_file = "map_report_full_logs.html")
  # To view, you would run this in the R console:
  # servr::httd(output_dir_ext)

  rstudioapi::filesPaneNavigate(project_dir)
}

#' This function provides a list of default settings that can be passed to `rr_map_report`.
#'
#' **Options:**
#' * `output_for`: Determines for which commands the log output is shown.
#'   - `"reg"`: Show output only for regression commands (`is_reg=TRUE`).
#'   - `"reg_and_map"`: (Default) Show output for all regression commands and for all other commands that are part of a map.
#'   - `"all"`: Show output for all executed commands.
#'   - `"none"`: Do not show any log output.
#' * `map_prod_ids`: A character vector of map product IDs to load and display in the report. The user can switch between these map types in the UI.
#' * `embed_data`: A logical value. If `TRUE` (default), all map data is embedded into a single self-contained HTML file. If `FALSE`, map data is written to external JSON files, which makes the initial report load faster but requires a web server for viewing.
#' * `show_wrong_number_report`: A logical value or `NA`. If `TRUE`, shows the "Discrepancies Found" report based on `wrong_number_cases` from the map itself. If `FALSE`, this report is hidden. If `NA` (default), the report is hidden if a `rme.Rds` evaluation file is found for the document type, otherwise it is shown. This allows the more comprehensive evaluation report to supersede the basic one.
#' * `only_tests`: An optional character vector of evaluation test names (e.g., `c("coef_se_match", "invalid_runids")`). If provided, only these tests will be included in the evaluation report.
#' * `ignore_tests`: An optional character vector of evaluation test names to exclude from the report.
#' * `test_order`: An optional character vector specifying the desired order of tests in the report. Tests included in this vector will appear first, in the specified order. Any remaining tests will be appended alphabetically.
#'
#' @return A list of default options.
#' @export
rr_map_report_opts <- function(output_for = c("all", "reg", "reg_and_map", "none")[3],map_prod_ids = c("map_reg_run", "map_inv_reg_run", "map_reg_static")[1], embed_data = TRUE,show_wrong_number_report = NA, only_tests = NULL,ignore_tests = c("non_reg_cmd"), test_order = NULL) {
  as.list(environment())
}


#' Creates an interactive HTML report to visualize maps
#'
#' This function generates a self-contained HTML report that visualizes the
#' maps between Stata do-files and regression tables. The report features
#' static color-coding for regression cells and interactive highlighting.
#'
#' @param project_dir The root directory of the project.
#' @param output_dir Directory for the report. Defaults to 'reports' in project_dir.
#' @param output_file The name of the HTML report file.
#' @param doc_type The document type (e.g., "art", "app1").
#' @param opts A list of options, typically generated by `rr_map_report_opts()`.
#' @return The path to the generated HTML report file.
#' @export
rr_map_report <- function(project_dir,
                          output_dir = file.path(project_dir, "reports"),
                          output_file = "map_report.html",
                          doc_type = "art",
                          opts = rr_map_report_opts()) {
  restore.point("rr_map_report")
  # --- 0. Check dependencies & Options ---
  pkgs <- c("dplyr", "tidyr", "stringi", "htmltools", "jsonlite", "purrr", "randtoolbox")
  for (pkg in pkgs) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      stop(paste("Please install the '", pkg, "' package."), call. = FALSE)
    }
  }

  if (is.null(opts)) {
    opts <- rr_map_report_opts()
  }

  # Decide on show_wrong_number_report if it's NA
  rme_file_for_check <- file.path(project_dir, "fp", paste0("eval_", doc_type), "rme.Rds")
  if (is.na(opts$show_wrong_number_report)) {
      opts$show_wrong_number_report <- !file.exists(rme_file_for_check)
  }

  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }

  # --- 1. Setup asset paths ---
  assets_dir <- file.path(output_dir, "shared")
  if (!dir.exists(assets_dir)) {
    dir.create(assets_dir, recursive = TRUE)
  }

  # --- 2. Load data ---
  cat("\nLoading data...")
  parcels <- repboxDB::repdb_load_parcels(project_dir, c("stata_source", "stata_run_cmd", "stata_run_log", "stata_cmd"))

  fp_dir <- file.path(project_dir, "fp", paste0("prod_", doc_type))
  tab_main_info <- rai_pick_tab_ver(fp_dir, "tab_main")
  if(nrow(tab_main_info) == 0) {
      stop("Could not find a suitable 'tab_main' product for doc_type '", doc_type, "'")
  }
  tab_main <- fp_load_prod_df(tab_main_info$ver_dir)

  cat("\nLoading maps...")
  all_map_types <- list()
  prod_id = "map_reg_run"
  for (prod_id in opts$map_prod_ids) {
    map_list <- rr_load_all_map_versions(project_dir, doc_type, prod_id = prod_id)
    if (length(map_list) > 0) {
      all_map_types[[prod_id]] <- map_list
    }
  }
  if (length(all_map_types) == 0) {
    warning("No map versions found for any prod_id in opts$map_prod_ids. The report will be generated without interactive links.")
  }

  # --- 3. Generate HTML & JS components ---
  cat("\nGenerating HTML components...")

  # Pre-compute conflict information for tooltips.
  # A conflict exists if a cell_id maps to different script/line combinations
  # across different map versions, or if it has a wrong_number_case.
  cell_conflict_data <- {
    all_maps_flat <- unlist(all_map_types, recursive = FALSE)
    all_maps_flat_df <- purrr::map_dfr(all_maps_flat, ~.x, .id = "map_version_id")

    # 1. Mapping conflicts
    mapping_conflict_msgs <- list()
    if (length(all_maps_flat) > 1 && nrow(all_maps_flat_df) > 0) {
      conflict_df <- all_maps_flat_df %>%
        rr_robust_script_num_join(parcels$stata_source$script_source) %>%
        filter(!is.na(cell_ids), cell_ids != "", !is.na(script_num), !is.na(code_line)) %>%
        select(map_version_id, script_num, code_line, cell_ids) %>%
        mutate(cell_id = strsplit(as.character(cell_ids), ",")) %>%
        tidyr::unnest(cell_id) %>%
        mutate(cell_id = trimws(cell_id)) %>%
        filter(cell_id != "") %>%
        select(map_version_id, cell_id, script_num, code_line) %>%
        distinct()

      if (nrow(conflict_df) > 0) {
          conflict_summary <- conflict_df %>%
            group_by(cell_id) %>%
            mutate(target_key = paste(script_num, code_line, sep = ":")) %>%
            summarise(
              num_unique_targets = n_distinct(target_key),
              conflict_info = if (n_distinct(target_key) > 1) {
                list(tibble(map_version_id = map_version_id, script_num = script_num, code_line = code_line))
              } else {
                list(NULL)
              },
              .groups = "drop"
            ) %>%
            filter(num_unique_targets > 1)

          if (nrow(conflict_summary) > 0) {
            mapping_conflict_msgs <- setNames(
              lapply(conflict_summary$conflict_info, function(info_df) {
                details <- info_df %>%
                  distinct(map_version_id, script_num, code_line) %>%
                  mutate(msg = paste0(map_version_id, " -> S", script_num, " L", code_line)) %>%
                  pull(msg)
                paste0("Note: Mapped differently in other versions:\n - ", paste(details, collapse="\n - "))
              }),
              conflict_summary$cell_id
            )
          }
      }
    }

    # 2. Wrong number conflicts
    wrong_num_msgs <- list()
    if (isTRUE(opts$show_wrong_number_report) && "wrong_number_cases" %in% names(all_maps_flat_df) && nrow(all_maps_flat_df) > 0) {
        wnc_df <- all_maps_flat_df %>%
            select(map_version_id, wrong_number_cases) %>%
            filter(!sapply(wrong_number_cases, function(x) is.null(x) || NROW(x) == 0))

        if (nrow(wnc_df) > 0) {
            wrong_num_conflict_df <- wnc_df %>%
                tidyr::unnest(cols = wrong_number_cases) %>%
                select(map_version_id, cell_id) %>%
                distinct() %>%
                filter(!is.na(cell_id)) %>%
                group_by(cell_id) %>%
                summarise(versions = paste(sort(unique(map_version_id)), collapse=", "), .groups="drop")

            if (nrow(wrong_num_conflict_df) > 0) {
                wrong_num_msgs <- setNames(
                    paste0("Note: Has wrong number discrepancy in version(s): ", wrong_num_conflict_df$versions),
                    wrong_num_conflict_df$cell_id
                )
            }
        }
    }

    # 3. Combine messages
    all_conflict_cells <- unique(c(names(mapping_conflict_msgs), names(wrong_num_msgs)))
    if (length(all_conflict_cells) > 0) {
        li = lapply(all_conflict_cells, function(cid) {
          restore.point("ssfk")
          msg1 = msg2 = ""
          if (cid %in% names(mapping_conflict_msgs)) {
            msg1 <- mapping_conflict_msgs[[cid]]
          }
          if (cid %in% names(wrong_num_msgs)) {
            msg2 <- wrong_num_msgs[[cid]]
          }
          # Filter NULLs and join with newline
          paste(c(msg1, msg2)[!sapply(c(msg1, msg2), is.null)], collapse="\n")
        })
        li = setNames(li,all_conflict_cells)
    } else {
        li =list()
    }
    li
  }
  js_conflict_data <- jsonlite::toJSON(cell_conflict_data, auto_unbox = TRUE)


  # Generate color map for consistent colors across all loaded maps
  all_map_dfs <- unlist(all_map_types, recursive = FALSE)
  all_regids <- unique(unlist(lapply(all_map_dfs, function(df) if("regid" %in% names(df)) unique(df$regid) else NULL)))
  all_regids <- stats::na.omit(all_regids)
  reg_color_map <- rr_make_distinct_colors(length(all_regids))
  names(reg_color_map) <- all_regids



  # Add a script_file column for easier access
  parcels$stata_source$script_source$script_file <- basename(parcels$stata_source$script_source$file_path)

  do_panel_html <- rr_make_do_panel_html(
    stata_source = parcels$stata_source$script_source,
    stata_cmd = parcels$stata_cmd$stata_cmd,
    stata_run_cmd = parcels$stata_run_cmd$stata_run_cmd,
    stata_run_log = parcels$stata_run_log$stata_run_log,
    opts = opts,
    all_map_types = all_map_types
  )

  tab_panel_html <- rr_make_tab_panel_html(tab_main)
  controls_html <- rr_make_controls_html(all_map_types)

  # --- 4. Write JS and CSS assets ---
  cat("\nWriting JS and CSS assets...")
  rr_copy_pkg_assets(output_dir)

  # --- 5. Generate JS data and Assemble final HTML report ---

  # Conditionally generate map data (embedded vs. external)
  js_maps_data <- "{}"
  js_manifest_data <- "{}"

  if (isTRUE(opts$embed_data)) {
    cat("\nGenerating and embedding map data...")
    processed_types <- purrr::map(all_map_types, function(map_list_for_type) {
        purrr::map(map_list_for_type, function(map_df) {
            rr_process_single_map_for_js(map_df, reg_color_map, parcels$stata_source$script_source)
        })
    })
    js_maps_data <- jsonlite::toJSON(processed_types, auto_unbox = TRUE, null = "null", na = "null")
  } else {
    cat("\nGenerating external JSON files for map data...")
    maps_data_dir <- file.path(output_dir, "maps_data")
    if (!dir.exists(maps_data_dir)) dir.create(maps_data_dir, recursive = TRUE)

    manifest <- list()
    for (map_type in names(all_map_types)) {
        manifest[[map_type]] <- list()
        for (version_id in names(all_map_types[[map_type]])) {
            map_df <- all_map_types[[map_type]][[version_id]]
            processed_map_list <- rr_process_single_map_for_js(map_df, reg_color_map, parcels$stata_source$script_source)
            json_content <- jsonlite::toJSON(processed_map_list, auto_unbox = TRUE, null = "null", na = "null")

            file_name <- paste0(map_type, "_", version_id, ".json")
            file_path <- file.path(maps_data_dir, file_name)
            relative_path <- file.path("maps_data", file_name)

            writeLines(json_content, file_path)
            manifest[[map_type]][[version_id]] <- relative_path
        }
    }
    js_manifest_data <- jsonlite::toJSON(manifest, auto_unbox = TRUE)
    cat("\n\nExternal JSONs generated. Note: This report must now be viewed via a web server.")
    cat("\nYou can start one from R with: servr::httd('", normalizePath(output_dir, mustWork=FALSE), "')")
  }

  # --- 5b. Process and generate evaluation data from rme.Rds ---
  js_evals_data <- "{}"
  js_eval_manifest_data <- "{}"
  rme_file <- file.path(project_dir, "fp", paste0("eval_", doc_type), "rme.Rds")

  processed_eval_data <- NULL
  if (file.exists(rme_file)) {
      cat("\nLoading and processing evaluation data from rme.Rds...")
      tryCatch({
          rme <- readRDS(rme_file)
          processed_eval_data <- rr_process_eval_data(rme, all_map_types, parcels$stata_source$script_source, opts)
          if (!is.null(processed_eval_data) && length(processed_eval_data) > 0) {
            cat("\nSuccessfully processed evaluation data.")
          } else {
            cat("\nEvaluation data found, but no applicable issues to report after processing.")
          }
      }, error = function(e) {
          warning("Could not load or process rme.Rds: ", e$message)
      })
    } else {
      cat("\nNo evaluation data file (rme.Rds) found, skipping.")
  }

  if (!is.null(processed_eval_data)) {
      if (isTRUE(opts$embed_data)) {
          js_evals_data <- jsonlite::toJSON(processed_eval_data, auto_unbox = TRUE, null = "null", na = "null")
      } else {
          eval_data_dir <- file.path(output_dir, "eval_data")
          if (!dir.exists(eval_data_dir)) dir.create(eval_data_dir, recursive = TRUE)

          manifest <- list()
          for (version_id in names(processed_eval_data)) {
              json_content <- jsonlite::toJSON(processed_eval_data[[version_id]], auto_unbox = TRUE, null = "null", na = "null")

              file_name <- paste0("eval_", version_id, ".json")
              file_path <- file.path(eval_data_dir, file_name)
              relative_path <- file.path("eval_data", file_name)

              writeLines(json_content, file_path)
              manifest[[version_id]] <- relative_path
          }
          js_eval_manifest_data <- jsonlite::toJSON(manifest, auto_unbox = TRUE)
      }
  }


  cat("\nAssembling final HTML report...")
  html_content <- htmltools::tagList(
    htmltools::tags$head(
      htmltools::tags$meta(charset = "UTF-8"),
      htmltools::tags$meta(`http-equiv` = "X-UA-Compatible", content = "IE=edge"),
      htmltools::tags$meta(name="viewport", content="width=device-width, initial-scale=1"),
      htmltools::tags$title(paste("map Report:", basename(project_dir))),
      htmltools::tags$link(href = "shared/bootstrap.min.css", rel = "stylesheet"),
      htmltools::tags$link(href = "shared/repbox.css", rel = "stylesheet")
    ),
    htmltools::tags$body(
      htmltools::tags$div(class = "container-fluid",
        htmltools::HTML(controls_html),
        htmltools::tags$div(class = "row", style = "height: 95vh;",
          htmltools::tags$div(id = "do-col-div", class = "col-sm-7",
            htmltools::HTML(do_panel_html)
          ),
          htmltools::tags$div(id = "tabs-col-div", class = "col-sm-5",
            htmltools::HTML(tab_panel_html)
          )
        )
      ),
      htmltools::tags$script(src = "shared/jquery.min.js"),
      htmltools::tags$script(src = "shared/bootstrap.min.js"),
      htmltools::tags$script(htmltools::HTML(paste0("var data_is_embedded = ", tolower(isTRUE(opts$embed_data)), ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var show_wrong_number_report_opt = ", tolower(isTRUE(opts$show_wrong_number_report)), ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var all_maps = ", js_maps_data, ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var report_manifest = ", js_manifest_data, ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var cell_conflict_data = ", js_conflict_data, ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var all_evals = ", js_evals_data, ";"))),
      htmltools::tags$script(htmltools::HTML(paste0("var eval_manifest = ", js_eval_manifest_data, ";"))),
      htmltools::tags$script(src = "shared/report_map.js")
    )
  )

  report_path <- file.path(output_dir, output_file)
  htmltools::save_html(html_content, file = report_path)

  message(paste("\nReport generated successfully at:", report_path))
  return(invisible(report_path))
}


# --- Helper Functions ---
#' Process rme$evals data for the report
#' @param rme The loaded rme.Rds object
#' @param all_map_types The loaded map data, used to resolve runids to code locations
#' @param stata_source The stata_source parcel, used for robust script_num joins
#' @param opts A list of options, typically from `rr_map_report_opts()`.
#' @return A nested list structured for JSON output
#' @noRd
rr_process_eval_data <- function(rme, all_map_types, stata_source, opts = list()) {
    restore.point("rr_process_eval_data")

    #rme$evals = rme$evals[!names(rme$evals) %in% ignore_evals]

    if (is.null(rme) || is.null(rme$evals)) return(NULL)



    # 1. Create robust lookup from runid to code location from rme$run_df (ground truth)
    runid_to_code_lookup <- NULL
    if ("run_df" %in% names(rme) && !is.null(rme$run_df)) {
        runid_to_code_lookup <- rme$run_df %>%
            select(runid, script_num, code_line) %>%
            filter(!is.na(runid)) %>%
            distinct(runid, .keep_all = TRUE)
    }

    # 2. Create lookup from map info to get runid for tests that don't have it
    all_maps_flat_df <- purrr::map_dfr(unlist(all_map_types, recursive = FALSE), ~ if(!is.null(.x) && nrow(.x)>0) .x else NULL, .id = "map_version_id")

  # FIX: unlist() combined with map_dfr creates a composite ID (e.g., "map_reg_run.g25f-mocr--v0").
  # We must strip the product ID prefix so the join with rme data works on just the version ID.
  restore.point("before_fix")
  if (nrow(all_maps_flat_df) > 0) {
    all_maps_flat_df$map_version_id = stringi::stri_replace_first_regex(all_maps_flat_df$map_version_id, "^[^\\.]+\\.", "")
  }

    regid_to_runid_lookup <- NULL
    if (nrow(all_maps_flat_df) > 0) {
        regid_to_runid_lookup <- all_maps_flat_df %>%
            select(map_version_id, tabid, regid, runid) %>%
            filter(!is.na(regid), !is.na(runid)) %>%
            distinct()
    }

    # 3. Filter and order tests to be processed
    eval_tests <- rme$evals[sapply(rme$evals, function(x) !is.null(x) && NROW(x) > 0)]
    if (length(eval_tests) == 0) return(NULL)

    tests_to_process <- names(eval_tests)

    if (!is.null(opts$only_tests)) {
      tests_to_process <- intersect(tests_to_process, opts$only_tests)
    }
    if (!is.null(opts$ignore_tests)) {
      tests_to_process <- setdiff(tests_to_process, opts$ignore_tests)
    }

    if (!is.null(opts$test_order)) {
        ordered_part <- intersect(opts$test_order, tests_to_process)
        remaining_part <- sort(setdiff(tests_to_process, opts$test_order))
        final_test_order <- c(ordered_part, remaining_part)
    } else {
        final_test_order <- sort(tests_to_process)
    }


    processed_evals <- list()
    for (test_name in final_test_order) {
        df <- eval_tests[[test_name]]
        df <- ungroup(df)

        if (!"map_version" %in% names(df)) next

        df$ver_id <- df$map_version
        df <- filter(df, !is.na(ver_id))
        if(nrow(df) == 0) next

        # Standardize runid column if it's named 'runids'
        if ("runids" %in% names(df) && !"runid" %in% names(df)) {
             df <- df %>% rename(runid = runids)
        }

        # Ensure runid is numeric for joining
        if ("runid" %in% names(df) && !is.numeric(df$runid)) {
            df$runid <- suppressWarnings(as.numeric(as.character(df$runid)))
        }

        # If df has regid but no runid, try to get runid from maps
        if (!"runid" %in% names(df) && "regid" %in% names(df) && !is.null(regid_to_runid_lookup)) {
            df <- left_join(df, regid_to_runid_lookup, by = c("map_version" = "map_version_id", "tabid", "regid"))
        }

        # If df now has runid, join with ground truth to get code location.
        # This join will not filter out rows if runid is NA, which is what we want.
        if ("runid" %in% names(df) && !is.null(runid_to_code_lookup)) {
            df <- df %>%
                left_join(runid_to_code_lookup, by = "runid")
        }

        # Standardize cellids: if only cellid exists, copy it to cellids for the JS
        if ("cellid" %in% names(df) && !"cellids" %in% names(df)) {
            df$cellids <- df$cellid
        }

        # Convert all columns to character to avoid JSON issues
        is_list_col <- sapply(df, is.list)
        df[!is_list_col] <- lapply(df[!is_list_col], as.character)

        df_split_ver <- split(df, df$ver_id)

        for (ver_id in names(df_split_ver)) {
            ver_df <- df_split_ver[[ver_id]]
            if (!"tabid" %in% names(ver_df)) next

            ver_df$tabid <- as.character(ver_df$tabid)
            df_split_tab <- split(ver_df, ver_df$tabid)

            for (tabid in names(df_split_tab)) {
                issue_df <- df_split_tab[[tabid]]
                cols_to_keep <- setdiff(names(issue_df), c("map_version", "ver_id", "tabid"))

                records <- purrr::transpose(issue_df[, cols_to_keep, drop = FALSE])

                description <- attr(eval_tests[[test_name]], "long_descr") %||%
                               attr(eval_tests[[test_name]], "descr") %||%
                               paste0("No description available for test: '", test_name, "'.")

                processed_evals[[ver_id]][[tabid]][[test_name]] <- list(
                    description = description,
                    issues = records
                )
            }
        }
    }
    return(processed_evals)
}
#' @noRd
#' @description Robustly adds script_num to a map data frame by joining on
#' full path and then falling back to basename.
#' @param map_df The map data frame, which must contain a 'script_file' column.
#' @param stata_source The 'script_source' data frame from parcels.
#' @return The map_df with an added 'script_num' column.
rr_robust_script_num_join <- function(map_df, stata_source) {
  if (is.null(map_df) || nrow(map_df) == 0) return(map_df)
  if (!"script_file" %in% names(map_df)) return(map_df)

  # If script_num exists and is fully populated, we are done
  if ("script_num" %in% names(map_df) && !any(is.na(map_df$script_num))) return(map_df)

  # If script_num column exists but has NAs, remove it to be re-created cleanly.
  if ("script_num" %in% names(map_df)) {
    map_df <- map_df %>% select(-script_num)
  }

  script_info <- stata_source %>%
    select(file_path, script_num) %>%
    distinct() %>%
    mutate(script_basename = basename(file_path))

  # Attempt 1: Join by full path
  map_df_with_num_path <- map_df %>%
    left_join(script_info %>% select(file_path, script_num), by = c("script_file" = "file_path"))

  # Attempt 2: Join by basename
  script_info_basename_lookup <- script_info %>%
    select(script_basename = script_basename, script_num_base = script_num)
  if (any(duplicated(script_info_basename_lookup$script_basename))) {
    warning("Duplicate script basenames found. Matching by basename may be ambiguous. Taking first match.")
    script_info_basename_lookup <- script_info_basename_lookup %>%
      distinct(script_basename, .keep_all = TRUE)
  }

  map_df_with_num_base <- map_df %>%
    left_join(script_info_basename_lookup, by = c("script_file" = "script_basename"))

  # Coalesce the results. script_num from path join takes precedence.
  map_df$script_num <- coalesce(map_df_with_num_path$script_num, map_df_with_num_base$script_num_base)

  return(map_df)
}

# Helper to process a single map data frame into a JS-ready list structure
rr_process_single_map_for_js <- function(map_df, reg_color_map, stata_source) {
    restore.point("rr_process_single_map_for_js")
    if (is.null(map_df) || nrow(map_df) == 0) {
      # Return the new empty structure
      return(list(code_locations = list(), cell_to_code_idx = list(),
                  code_to_cells = list(), reg_info = setNames(list(), character(0)),
                  wrong_number_info = list(), cell_map = list()))
    }

    # Robustly join to get script_num
    map_df <- rr_robust_script_num_join(map_df, stata_source)

    ensure_cols <- c("runid", "script_num", "code_line", "cell_ids", "tabid", "regid")
    for (col in ensure_cols) {
      if (!col %in% names(map_df)) {
        map_df[[col]] <- if (col %in% c("runid", "script_num", "code_line", "regid")) NA_integer_ else NA_character_
      }
    }

    # --- NEW LOGIC FOR COMPACT CELL->CODE MAPPING (for highlighting) ---
    cell_map_df_highlight <- map_df %>%
      filter(!is.na(cell_ids), cell_ids != "", !is.na(script_num), !is.na(code_line)) %>%
      mutate(cell_id = strsplit(as.character(cell_ids), ",")) %>%
      tidyr::unnest(cell_id) %>%
      mutate(cell_id = trimws(cell_id)) %>%
      select(cell_id, runid, script_num, code_line)

    if (nrow(cell_map_df_highlight) > 0) {
        # 1. Find unique code locations and assign a 0-based index
        unique_locations <- cell_map_df_highlight %>%
          select(runid, script_num, code_line) %>%
          distinct() %>%
          arrange(runid, script_num, code_line) %>%
          mutate(location_idx = row_number() - 1)

        # 2. Create the list of location arrays for JSON
        code_locations_list <- purrr::pmap(unique_locations[, c("runid", "script_num", "code_line")], c)

        # 3. Join back to get the index for each cell and create the named list
        cell_to_idx_df <- left_join(cell_map_df_highlight, unique_locations, by = c("runid", "script_num", "code_line"))
        cell_to_code_idx <- setNames(as.list(cell_to_idx_df$location_idx), cell_to_idx_df$cell_id)
    } else {
        code_locations_list <- list()
        cell_to_code_idx <- list()
    }
    # --- END OF COMPACT LOGIC ---

    # --- NEW logic for cell_map for tooltips ---
    cell_map_df_tooltip <- map_df %>%
      filter(!is.na(cell_ids), cell_ids != "", !is.na(script_num)) %>%
      select(runid, script_num, code_line, regid, cell_ids) %>%
      mutate(cell_id = strsplit(as.character(cell_ids), ",")) %>%
      tidyr::unnest(cell_id) %>%
      mutate(cell_id = trimws(cell_id)) %>%
      filter(cell_id != "") %>%
      group_by(cell_id) %>%
      summarise(
        runid = first(runid),
        script_num = first(script_num),
        code_line = first(code_line),
        regid = first(regid),
        .groups = "drop"
      )

    if(nrow(cell_map_df_tooltip) > 0) {
        script_file_lookup <- stata_source %>%
            select(script_num, file_path) %>%
            mutate(script_file = basename(file_path)) %>%
            select(-file_path) %>%
            distinct()

        cell_map_df_tooltip <- cell_map_df_tooltip %>%
            left_join(script_file_lookup, by = "script_num") %>%
            select(-script_num)
    }


    cell_map <- if (nrow(cell_map_df_tooltip) > 0) {
      purrr::transpose(cell_map_df_tooltip[, -1]) %>%
        setNames(cell_map_df_tooltip$cell_id)
    } else {
      list()
    }


    code_map_df <- map_df %>%
      filter(!is.na(code_line), !is.na(script_num)) %>%
      select(script_num, code_line, tabid, cell_ids) %>%
      distinct()
    code_to_cells <- if (nrow(code_map_df) > 0) setNames(lapply(1:nrow(code_map_df), function(i) as.list(code_map_df[i, c("tabid", "cell_ids")])), paste0("s", code_map_df$script_num, "_l", code_map_df$code_line)) else list()
    reg_info <- setNames(list(), character(0))
    if ("regid" %in% names(map_df) && length(reg_color_map) > 0) {
      reg_df <- map_df %>%
        filter(!is.na(regid), !is.na(cell_ids), cell_ids != "") %>%
        select(regid, cell_ids) %>%
        group_by(regid) %>%
        summarise(all_cell_ids = paste(unique(trimws(unlist(strsplit(cell_ids, ",")))), collapse = ","), .groups = "drop")
      if (nrow(reg_df) > 0) {
        reg_info_list <- lapply(1:nrow(reg_df), function(i) {
            regidex_char <- as.character(reg_df$regid[i])
            if (regidex_char %in% names(reg_color_map)) list(color = reg_color_map[[regidex_char]], cell_ids = reg_df$all_cell_ids[i]) else NULL
          })
        names(reg_info_list) <- reg_df$regid
        reg_info <- reg_info_list[!sapply(reg_info_list, is.null)]
      }
    }

# --- Process wrong number cases ---
    wrong_number_info <- list()
    if ("wrong_number_cases" %in% names(map_df) && "tabid" %in% names(map_df)) {
        # The list column from JSON can contain NULLs for empty arrays. Filter these out.
        # We select the key identifiers to link the discrepancy back to its source.
        wnc_df <- map_df %>%
            select(tabid, runid, script_num, code_line, wrong_number_cases) %>%
            filter(!sapply(wrong_number_cases, function(x) is.null(x) || NROW(x) == 0))

        if (nrow(wnc_df) > 0) {
            wrong_number_info <- wnc_df %>%
                tidyr::unnest(cols = wrong_number_cases) %>%
                select(
                    tabid,
                    cell_id,
                    wrong_number_in_cell,
                    number_in_stata_output,
                    runid,
                    script_num,
                    code_line
                ) %>%
                distinct()
        }
    }

    # Return the new structure
    list(
      code_locations = code_locations_list,
      cell_to_code_idx = cell_to_code_idx,
      code_to_cells = code_to_cells,
      reg_info = reg_info,
      wrong_number_info = wrong_number_info,
      cell_map = cell_map
    )
}

#' @describeIn rr_map_report Load all available versions of a given product.
rr_load_all_map_versions <- function(project_dir, doc_type, prod_id) {
  restore.point("rr_load_all_map_versions")
  fp_dir <- file.path(project_dir, "fp", paste0("prod_", doc_type))
  prod_path <- file.path(fp_dir, prod_id)
  if (!dir.exists(prod_path)) return(list())


  ver_dirs = fp_all_ok_ver_dirs(fp_dir,prod_id = prod_id)
  if (length(ver_dirs) == 0) return(list())

  df_list <- lapply(ver_dirs, function(ver_dir) {
    df = fp_load_prod_df(ver_dir)
    df
  })

  # The old logic shortened names for v0 versions (e.g., 'model--v0' became 'model'),
  # which created an inconsistency with how evaluation data is keyed.
  # We will always use the full ver_id as the name.
  ind_df = fp_ver_dir_to_ids(ver_dirs)
  if (length(df_list) > 0 && !is.null(ind_df) && nrow(ind_df) == length(df_list)) {
    names(df_list) = ind_df$ver_id
  }

  df_list

}

# NOTE: rr_make_js_maps_data is now obsolete and has been replaced by an
# internal helper function and logic within rr_map_report.

#' @describeIn rr_map_report Generate HTML for the Stata do-file panel.
rr_make_do_panel_html <- function(stata_source, stata_cmd, stata_run_cmd, stata_run_log, opts, all_map_types = NULL) {
  restore.point("rr_make_do_panel_html")
  # --- Data Preparation ---
  run_df <- stata_run_cmd %>%
    left_join(stata_source %>% select(artid, file_path, script_num), by=c("artid", "file_path")) %>%
    left_join(stata_run_log, by = c("artid", "runid", "script_num"))

  ldf <- stata_source %>%
    mutate(text_lines = stringi::stri_split_lines(text)) %>%
    select(script_num, file_path, text_lines) %>%
    tidyr::unnest(text_lines) %>%
    group_by(script_num) %>%
    mutate(orgline = row_number()) %>%
    ungroup()

  cmd_info <- stata_cmd %>%
    select(file_path, line, orgline_start, orgline_end, is_reg) %>%
    mutate(orgline = purrr::map2(orgline_start, orgline_end, seq)) %>%
    tidyr::unnest(orgline)

  ldf <- left_join(ldf, cmd_info, by = c("file_path", "orgline"))

  run_info <- run_df %>%
    group_by(file_path, line) %>%
    summarise(runs = list(tibble(runid = runid, logtxt = logtxt, errcode = errcode, missing_data = missing_data)), .groups = "drop")

  first_lines <- stata_cmd %>%
    select(file_path, line, orgline = orgline_start) %>%
    distinct() %>%
    left_join(run_info, by = c("file_path", "line"))

  ldf <- left_join(ldf, first_lines, by = c("file_path", "line", "orgline"))

  # --- Filter log output based on opts ---
  if (!is.null(opts$output_for)) {
    if (opts$output_for == "none") {
      ldf$runs <- list(NULL)
    } else if (opts$output_for == "reg") {
      # is_reg can be NA, so is.true is correct
      ldf$runs[!is.true(ldf$is_reg)] <- list(NULL)
    } else if (opts$output_for == "reg_and_map" && !is.null(all_map_types)) {
      # Get all mapped lines from all_map_types
      all_map_dfs <- unlist(all_map_types, recursive = FALSE)
      all_map_dfs <- all_map_dfs[sapply(all_map_dfs, function(df) !is.null(df) && nrow(df) > 0)]

      if (length(all_map_dfs) > 0) {

        # Use the robust join function to add script_num if missing.
        # 'stata_source' is available from the parent function's arguments.
        all_map_dfs_norm <- lapply(all_map_dfs, function(df) {
          rr_robust_script_num_join(df, stata_source)
        })

        # The map's 'code_line' corresponds to the original line number ('orgline')
        mapped_orglines_df <- bind_rows(all_map_dfs_norm) %>%
          filter(!is.na(script_num), !is.na(code_line)) %>%
          select(script_num, orgline = code_line) %>%
          distinct()

        # Create an indicator column for mapped lines. This join works because `ldf` has one row per `orgline`.
        ldf <- ldf %>%
          left_join(
            mapped_orglines_df %>% mutate(is_mapped = TRUE),
            by = c("script_num", "orgline")
          )
      } else {
        ldf$is_mapped <- FALSE
      }

      # Keep runs for lines that are regressions OR are in a map.
      # is_reg can be NA -> is.true handles this.
      # is_mapped will be NA for non-matches -> is.true handles this.
      # This works because both `runs` and `is_mapped` are attached only to the starting `orgline` of a command.
      keep_runs <- is.true(ldf$is_reg) | is.true(ldf$is_mapped)
      ldf$runs[!keep_runs] <- list(NULL)

      # Clean up the temporary column
      if ("is_mapped" %in% names(ldf)) {
        ldf <- ldf %>% select(-is_mapped)
      }
    }
  }

  # --- HTML Generation (Vectorized) ---
  script_tabs_content <- lapply(split(ldf, ldf$script_num), function(df) {
    script_num_val <- df$script_num[1]
    has_run <- !sapply(df$runs, is.null)

    line_class <- ifelse(has_run,
      sapply(df$runs, function(r) {
        if (is.null(r) || nrow(r) == 0) return("norun-line")
        cls <- if (any(isTRUE(r$errcode != 0))) "err-line" else "noerr-line"
        if (any(isTRUE(r$missing_data))) cls <- "mida-line"
        cls
      }),
      "norun-line"
    )
    line_class[is.true(df$is_reg)] <- paste(line_class[is.true(df$is_reg)], "reg-cmd")

    title <- ifelse(has_run,
      sapply(df$runs, function(r) {
        if (is.null(r) || nrow(r) == 0) return("NA")
        t <- paste0("runid: ", paste(r$runid, collapse=", "))
        if (any(r$missing_data)) t <- paste(t, " missing data")
        t
      }),
      "NA"
    )

    log_divs <- ifelse(has_run, sapply(1:nrow(df), function(i) {
        paste0('<div class="collapse" id="loginfo-', df$orgline[i], '-', script_num_val, '">', rr_make_log_html(df$runs[[i]]), '</div>')
    }), "")

    button_tds <- ifelse(has_run, paste0('<td><a class="btn btn-xs" role="button" data-toggle="collapse" href="#loginfo-', df$orgline, '-', script_num_val, '" aria-expanded="false">▼</a></td>'), "<td></td>")

    code_tags <- paste0('<code id="L', df$orgline, '___', script_num_val, '" class="', line_class, '" title="', htmltools::htmlEscape(title), '">', htmltools::htmlEscape(df$text_lines), '</code>')

    rows_html <- paste0(
      '<tr>',
        button_tds,
        '<td class="code-line-td">', df$orgline, '</td>',
        '<td><pre class="do-pre">', code_tags, '</pre>', log_divs, '</td>',
      '</tr>'
    )

    toggle_btn <- paste0('<tr><td colspan="3"><button class="toogle-all-results btn btn-xs" title="Show or hide all results" onclick="$(\'#dotab_', script_num_val, ' .collapse\').collapse(\'toggle\');">▼</button></td></tr>')

    pane_class <- if(script_num_val==1) "tab-pane active" else "tab-pane"

    paste0(
      '<div class="', pane_class, '" id="dotab_', script_num_val, '">',
        '<table class="code-tab">',
          toggle_btn,
          paste(rows_html, collapse="\n"),
        '</table>',
      '</div>'
    )
  })

  script_pills <- paste0(
    '<ul id="dotabs" class="nav nav-pills" role="tablist">',
    paste(
      mapply(function(num, file, i) {
        active_class <- if(i == 1) ' class="active"' else ''
        paste0('<li', active_class, '><a href="#dotab_', num, '" role="tab" data-toggle="tab">', file, '</a></li>')
      }, stata_source$script_num, stata_source$script_file, 1:nrow(stata_source)),
      collapse="\n"
    ),
    '</ul>'
  )
  paste0(script_pills, '<div class="tab-content">', paste(script_tabs_content, collapse="\n"), '</div>')
}

#' @describeIn rr_map_report Generate log HTML for a line, handling multiple runs.
rr_make_log_html <- function(runs_for_line) {
  if (is.null(runs_for_line) || nrow(runs_for_line) == 0) return("")

  if (nrow(runs_for_line) == 1) {
    run <- runs_for_line[1, ]
    return(paste0('<pre id="runid-', run$runid, '" class="logtxt-pre"><code class="logtxt-code">', htmltools::htmlEscape(run$logtxt), '</code></pre>'))
  }

  random_id <- function() paste0(sample(c(letters, LETTERS), 12, replace = TRUE), collapse = "")

  tabset_id <- paste0("tabset_", random_id())
  tab_ids <- replicate(nrow(runs_for_line), paste0("tab_", random_id()))

  tab_pills <- paste0(
    '<ul id="', tabset_id, '" class="nav nav-tabs small-tab-ul" role="tablist">',
    paste(
      sapply(1:nrow(runs_for_line), function(i) {
        active_class <- if (i == 1) ' class="active"' else ''
        paste0('<li', active_class, '><a href="#', tab_ids[i], '" role="tab" data-toggle="tab">Run ', i, '</a></li>')
      }),
      collapse = "\n"
    ),
    '</ul>'
  )

  tab_content <- paste0(
    '<div class="tab-content">',
    paste(
      sapply(1:nrow(runs_for_line), function(i) {
        run <- runs_for_line[i, ]
        pane_class <- if (i == 1) "tab-pane active" else "tab-pane"
        paste0(
          '<div class="', pane_class, '" id="', tab_ids[i], '">',
            '<pre id="runid-', run$runid, '" class="logtxt-pre"><code class="logtxt-code">', htmltools::htmlEscape(run$logtxt), '</code></pre>',
          '</div>'
        )
      }),
      collapse="\n"
    ),
    '</div>'
  )
  paste0(tab_pills, tab_content)
}


#' @describeIn rr_map_report Generate HTML for the table display panel.
rr_make_tab_panel_html <- function(tab_main) {
  tab_pills <- paste0(
    '<ul id="tabtabs" class="nav nav-pills" role="tablist">',
    paste(
      sapply(1:nrow(tab_main), function(i) {
        row <- tab_main[i,]
        active_class <- if(i == 1) ' class="active"' else ''
        paste0('<li', active_class, '><a href="#tabtab', row$tabid, '" role="tab" data-toggle="tab">Tab ', row$tabid, '</a></li>')
      }),
      collapse = "\n"
    ),
    '</ul>'
  )

  tab_content <- paste0(
    '<div class="tab-content">',
    paste(
      sapply(1:nrow(tab_main), function(i) {
        row <- tab_main[i,]
        active_class <- if(i == 1) "tab-pane active" else "tab-pane"
        paste0(
          '<div class="', active_class, '" id="tabtab', row$tabid, '">',
            '<div class="art-tab-div">',
              '<h5>', htmltools::htmlEscape(row$tabtitle), '</h5>',
              row$tabhtml,
            '</div>',
          '</div>'
        )
      }),
      collapse="\n"
    ),
    '</div>'
  )
  paste0(tab_pills, tab_content)
}

#' @describeIn rr_map_report Generate HTML for the top control bar.
rr_make_controls_html <- function(all_map_types) {
  map_type_names <- names(all_map_types)
  if (length(map_type_names) == 0) return("")

  type_options <- paste0('<option value="', map_type_names, '">', map_type_names, '</option>', collapse = "\n")

  # The version selector is populated by JavaScript, so it's initially empty.
  paste0(
    '<div class="controls-div form-inline">',
      '<div class="form-group">',
        '<label for="map_type_selector">Map Type:</label>',
        '<select id="map_type_selector" class="form-control input-sm">', type_options, '</select>',
      '</div>',
      '<div class="form-group">',
        '<label for="version_selector">Map Version:</label>',
        '<select id="version_selector" class="form-control input-sm"></select>',
      '</div>',
    '</div>'
  )
}

#' @describeIn rr_map_report Copy package assets to the report directory.
rr_copy_pkg_assets <- function(output_dir) {
  pkg_www_dir <- system.file("www", package = "repboxReport")
  if (pkg_www_dir == "") {
    warning("Could not find 'inst/www' directory in repboxReport package. Report assets will be missing.")
    return()
  }

  shared_dir <- file.path(output_dir, "shared")
  if (!dir.exists(shared_dir)) dir.create(shared_dir, recursive = TRUE)

  files_to_copy <- list.files(pkg_www_dir, recursive = TRUE, full.names = TRUE)

  if (length(files_to_copy) > 0) {
    file.copy(files_to_copy, shared_dir, recursive = TRUE, overwrite = TRUE)
  } else {
     warning("No assets found in 'inst/www' to copy.")
  }
}
```
# END OF FILE: report_map.R

-----------------------------------------------------------


# FILE: rr_table.R
```
# FILE: rr_table.R

example = function() {
  library(repboxReport)
  project_dir = "/home/rstudio/repbox/projects_gha_new/aejapp_10_4_6"

  options(warn=2)
  opts = rr_map_report_opts(embed_data = FALSE, only_tests = "multicol_reg_plausibility")
  res = rr_single_table(project_dir, tabid="3", opts=opts)

  # This should now show classes like "issue_wrong_number" and a list at the bottom
  browseURL(res$with_issues_html[[1]])

  rstudioapi::filesPaneNavigate(project_dir)
}

#' Create per-table HTML (and optional PNG) snapshots from the map report inputs
#'
#' @param project_dir The root directory of the project.
#' @param tabid One or more table ids (character or numeric).
#' @param output_dir Directory for outputs. Defaults to 'reports_tables' in project_dir.
#' @param doc_type Document type (e.g., "art", "app1").
#' @param opts Options list, typically from rr_map_report_opts().
#' @param map_prod_id Which map product id to use for coloring. Defaults to the first in opts$map_prod_ids.
#' @param map_version_id Which map version id to use.
#' @param table_png Logical. If TRUE, write a PNG of the colored table using webshot2.
#' @param png_zoom Zoom factor for PNG rendering.
#' @param png_delay Delay (seconds) before screenshot.
#' @param png_vwidth Viewport width for rendering.
#' @param png_vheight Viewport height for rendering.
#' @return Invisibly returns a data.frame with produced file paths.
#' @export
rr_single_table = function(project_dir,
  tabid,
  output_dir = file.path(project_dir, "reports_tables"),
  doc_type = "art",
  opts = NULL,
  map_prod_id = NULL,
  map_version_id = NULL,
  table_png = TRUE,
  png_zoom = 2,
  png_delay = 0.2,
  png_vwidth = 1400,
  png_vheight = 900,
  rme = NULL
) {
  restore.point("rr_single_table")

  pkgs = c("dplyr", "tidyr", "stringi", "htmltools", "jsonlite", "purrr", "randtoolbox")
  for (pkg in pkgs) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      stop(paste0("Package '", pkg, "' needed. Please install it."), call. = FALSE)
    }
  }

  if (isTRUE(table_png) && !requireNamespace("webshot2", quietly = TRUE)) {
    stop("table_png=TRUE requires package 'webshot2'.", call. = FALSE)
  }

  if (is.null(opts)) opts = rr_map_report_opts()

  if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

  # Check wrong number reporting option
  rme_file_for_check = file.path(project_dir, "fp", paste0("eval_", doc_type), "rme.Rds")
  if (is.null(opts$show_wrong_number_report)) opts$show_wrong_number_report = NA
  if (is.na(opts$show_wrong_number_report)) {
    opts$show_wrong_number_report = !file.exists(rme_file_for_check)
  }

  # Load parcels and table html
  parcels = repboxDB::repdb_load_parcels(project_dir, c("stata_source", "stata_run_cmd", "stata_run_log", "stata_cmd"))

  fp_dir = file.path(project_dir, "fp", paste0("prod_", doc_type))
  tab_main_info = rai_pick_tab_ver(fp_dir, "tab_main")
  if (nrow(tab_main_info) == 0) {
    stop(paste0("Could not find 'tab_main' for doc_type '", doc_type, "'"), call. = FALSE)
  }
  tab_main = fp_load_prod_df(tab_main_info$ver_dir)
  tab_main$tabid = as.character(tab_main$tabid)

  tabids = as.character(tabid)
  if (length(tabids)==0) tabids=unique(tab_main$tabid)
  tab_rows = dplyr::filter(tab_main, tabid %in% tabids)

  # Load maps
  all_map_types = list()
  for (prod_id in opts$map_prod_ids) {
    map_list = rr_load_all_map_versions(project_dir, doc_type, prod_id = prod_id)
    if (length(map_list) > 0) all_map_types[[prod_id]] = map_list
  }

  if (length(all_map_types) == 0) stop("No map versions found.", call. = FALSE)
  if (is.null(map_prod_id)) map_prod_id = opts$map_prod_ids[1]

  available_versions = names(all_map_types[[map_prod_id]])
  if (is.null(map_version_id)) map_version_id = sort(available_versions)[length(available_versions)]

  # Color map
  all_map_dfs = unlist(all_map_types, recursive = FALSE)
  all_regids = stats::na.omit(unique(unlist(lapply(all_map_dfs, function(df) if ("regid" %in% names(df)) unique(df$regid) else NULL))))
  reg_color_map = rr_make_distinct_colors(length(all_regids))
  names(reg_color_map) = all_regids

  # Evaluation data
  processed_eval_data = NULL
  if (is.null(rme)) {
    rme_file = file.path(project_dir, "fp", paste0("eval_", doc_type), "rme.Rds")
    if (file.exists(rme_file)) {
      tryCatch({
        rme = readRDS(rme_file)

      }, error = function(e) warning(paste0("Could not load rme.Rds: ", e$message)))
    }
  }
  processed_eval_data = rr_process_eval_data(rme, all_map_types, parcels$stata_source$script_source, opts)

  out_rows = list()
  for (i in seq_len(nrow(tab_rows))) {
    row = tab_rows[i, ]
    tabid_i = as.character(row$tabid)

    map_df = all_map_types[[map_prod_id]][[map_version_id]]
    if (is.null(map_df) || nrow(map_df) == 0) next

    map_df_tab = dplyr::filter(map_df, as.character(tabid) == tabid_i)
    mapping = rr_process_single_map_for_js(map_df_tab, reg_color_map, parcels$stata_source$script_source)

    eval_for_tab = NULL
    if (!is.null(processed_eval_data) && map_version_id %in% names(processed_eval_data)) {
      if (tabid_i %in% names(processed_eval_data[[map_version_id]])) {
        eval_for_tab = processed_eval_data[[map_version_id]][[tabid_i]]
      }
    }

    # Extract issues for classification
    issues_info = rr_extract_all_issues(mapping, eval_for_tab, isTRUE(opts$show_wrong_number_report))

    base_name = rr_safe_filename(paste0("tab", tabid_i, "__", map_prod_id, "__", map_version_id))
    file_table_only = file.path(output_dir, paste0(base_name, "__table_only.html"))
    file_with_issues = file.path(output_dir, paste0(base_name, "__with_issues.html"))
    file_png = file.path(output_dir, paste0(base_name, ".png"))

    # 1. Visual HTML (JS/CSS) for PNG
    html_visual = rr_make_single_table_html(
      tabtitle = row$tabtitle,
      tabhtml = row$tabhtml,
      mapping = mapping,
      show_wrong_number = isTRUE(opts$show_wrong_number_report),
      issue_cellids = names(issues_info)
    )

    # 2. AI Prompt HTML (Semantic HTML + Issue List)
    # Inject classes into the table
    ai_table = rr_make_ai_table_html(row$tabhtml, issues_info)
    # Generate the text/table summary of issues
    ai_issues_list = rr_make_ai_issue_summary(mapping, eval_for_tab, isTRUE(opts$show_wrong_number_report))

    # Combine
    full_ai_html = paste0("<html><body>", ai_table, "\n<br><hr>\n", ai_issues_list, "</body></html>")

    htmltools::save_html(html_visual, file = file_table_only)
    writeLines(full_ai_html, file_with_issues)

    png_done = FALSE
    if (isTRUE(table_png)) {
      tryCatch({
        webshot2::webshot(url = file_table_only, file = file_png, selector = "#rr-table-container",
          zoom = png_zoom, delay = png_delay, vwidth = png_vwidth, vheight = png_vheight, quiet = TRUE)
        png_done = TRUE
      }, error = function(e) warning(paste0("PNG failed for tab ", tabid_i, ": ", e$message)))
    }

    out_rows[[length(out_rows) + 1]] = data.frame(
      tabid = tabid_i,
      map_prod_id = map_prod_id,
      map_version_id = map_version_id,
      table_only_html = file_table_only,
      with_issues_html = file_with_issues,
      png = if (png_done) file_png else NA_character_,
      stringsAsFactors = FALSE
    )
  }

  if (length(out_rows) == 0) return(invisible(data.frame()))
  res = dplyr::bind_rows(out_rows)
  message(paste0("rr_single_table wrote outputs to: ", normalizePath(output_dir, mustWork = FALSE)))
  invisible(res)
}


# ---- Helpers ----

rr_extract_all_issues = function(mapping, eval_for_tab, show_wrong_number) {
  issues_map = list()
  add_issue = function(cids, cls) {
    if (length(cids) == 0) return()
    cids = unique(cids)
    for (cid in cids) issues_map[[cid]] <<- c(issues_map[[cid]], cls)
  }

  if (isTRUE(show_wrong_number) && !is.null(mapping$wrong_number_info) && is.data.frame(mapping$wrong_number_info)) {
    if ("cell_id" %in% names(mapping$wrong_number_info)) {
       add_issue(as.character(mapping$wrong_number_info$cell_id), "issue_wrong_number")
    }
  }

  if (!is.null(eval_for_tab)) {
    for (test_name in names(eval_for_tab)) {
      issues = eval_for_tab[[test_name]]$issues
      if (!is.null(issues)) {
        cids = rr_extract_cellids_from_issue_records(issues)
        add_issue(cids, paste0("issue_", test_name))
      }
    }
  }
  issues_map
}


#' Injects issue classes into HTML table string. Handles loose whitespace in attributes.
rr_make_ai_table_html = function(tabhtml, issues_info) {
  if (length(issues_info) == 0) return(tabhtml)

  res_html = tabhtml
  affected_cells = names(issues_info)

  for (cid in affected_cells) {
    classes = paste(unique(issues_info[[cid]]), collapse = " ")

    # 1. Check if class attribute already exists on this cell
    # Pattern: <td ... id = "cid" ... class = "..."
    # We use \\s* to handle "id =" and "id="
    pat_existing_class = paste0("(<td\\b[^>]*\\bid\\s*=\\s*['\"]", cid, "['\"][^>]*\\bclass\\s*=\\s*['\"])([^'\"]*)")

    if (stringi::stri_detect_regex(res_html, pat_existing_class)) {
      # Append new classes to existing class attribute
      res_html = stringi::stri_replace_first_regex(res_html, pat_existing_class, paste0("$1$2 ", classes, " "))
    } else {
      # 2. No class attribute exists
      # Pattern: <td ... id = "cid" ... >
      pat_no_class = paste0("(<td\\b[^>]*\\bid\\s*=\\s*['\"]", cid, "['\"][^>]*)>")
      # Insert class attribute before closing >
      res_html = stringi::stri_replace_first_regex(res_html, pat_no_class, paste0("$1 class=\"", classes, "\">"))
    }
  }
  res_html
}


#' Generates a static HTML summary of issues for the AI prompt
rr_make_ai_issue_summary = function(mapping, eval_for_tab, show_wrong_number) {
  parts = list("<h3>Issues Report</h3>")

  # 1. Wrong Number Cases
  if (isTRUE(show_wrong_number) && !is.null(mapping$wrong_number_info) && is.data.frame(mapping$wrong_number_info) && nrow(mapping$wrong_number_info) > 0) {
    parts[[length(parts)+1]] = "<h4>issue_wrong_number</h4>"
    wdf = mapping$wrong_number_info
    # Select only useful columns for AI to understand the mismatch
    cols = intersect(names(wdf), c("cell_id", "wrong_number_in_cell", "number_in_stata_output", "runid", "script_num", "code_line"))
    parts[[length(parts)+1]] = rr_df_to_html_table(wdf[, cols, drop=FALSE])
  }

  # 2. Evaluation Tests
  if (!is.null(eval_for_tab)) {
    for (test_name in sort(names(eval_for_tab))) {
      test_block = eval_for_tab[[test_name]]
      issues = test_block$issues
      if (is.null(issues) || length(issues) == 0) next

      parts[[length(parts)+1]] = paste0("<h4>issue_", test_name, "</h4>")
      if (!is.null(test_block$description)) {
        parts[[length(parts)+1]] = paste0("<p><i>", htmltools::htmlEscape(test_block$description), "</i></p>")
      }

      # Convert issues list to DF for printing
      df = tryCatch({
        dplyr::bind_rows(lapply(issues, as.data.frame, stringsAsFactors=FALSE))
      }, error = function(e) NULL)

      if (!is.null(df) && nrow(df) > 0) {
        parts[[length(parts)+1]] = rr_df_to_html_table(df)
      }
    }
  }

  if (length(parts) == 1) return("<p>No issues found.</p>")
  paste(unlist(parts), collapse="\n")
}

rr_df_to_html_table = function(df) {
  if (nrow(df) == 0) return("")
  # Convert all to character for safe printing
  df[] = lapply(df, as.character)

  header = paste0("<tr>", paste0("<th>", names(df), "</th>", collapse=""), "</tr>")
  rows = apply(df, 1, function(row) {
    paste0("<tr>", paste0("<td>", htmltools::htmlEscape(row), "</td>", collapse=""), "</tr>")
  })

  paste0("<table border='1' style='border-collapse:collapse; width:100%;'>",
         header, paste(rows, collapse="\n"), "</table>")
}

# (Existing helpers: rr_safe_filename, rr_single_table_css, rr_single_table_js, rr_extract_cellids_from_issue_records, rr_make_single_table_html remain the same)
rr_safe_filename = function(x) {
  x = as.character(x)
  x = stringi::stri_replace_all_regex(x, "[^A-Za-z0-9_\\-\\.]+", "_")
  x = stringi::stri_replace_all_regex(x, "_{2,}", "_")
  x = stringi::stri_replace_all_regex(x, "^_+|_+$", "")
  x
}
rr_single_table_css = function() {
  paste0(
    "html, body { margin: 0; padding: 0; background: #ffffff; }\n",
    "#rr-page { padding: 0; margin: 0; }\n",
    "#rr-table-container { padding: 0; margin: 0; display: inline-block; }\n",
    ".rr-title { font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Arial, sans-serif; ",
    "font-size: 16px; margin: 6px 6px 2px 6px; color: #222; }\n",
    ".art-tab-div table { font-size: 12px; font-family: Trebuchet MS, Arial Narrow, Tahoma, sans-serif; ",
    "width: auto; white-space: nowrap; border-collapse: collapse; margin: 6px; }\n",
    ".art-tab-div table td, .art-tab-div table th { padding: 2px 4px; border: 1px solid #ddd; text-align: center; position: relative; }\n",
    ".art-tab-div table th { background-color: #f5f5f5; }\n",
    ".statically-colored { transition: background-color 0.3s ease; }\n",
    ".wrong-number-cell { }\n",
    ".issue-cell { outline: 2px solid #dc3545 !important; outline-offset: -2px; }\n"
  )
}
rr_single_table_js = function(reg_info_json, wrong_info_json, show_wrong_number = TRUE, issues_cellids_json = "[]") {
  show_flag = if (isTRUE(show_wrong_number)) "true" else "false"
  paste0(
    "(function(){\n",
    "  var reg_info = ", reg_info_json, " || {};\n",
    "  var wrong_info = ", wrong_info_json, " || [];\n",
    "  var show_wrong = ", show_flag, ";\n",
    "  var issue_cellids = ", issues_cellids_json, " || [];\n",
    "\n",
    "  function apply_static_coloring() {\n",
    "    for (var regid in reg_info) {\n",
    "      if (!reg_info.hasOwnProperty(regid)) continue;\n",
    "      var info = reg_info[regid];\n",
    "      if (!info || !info.color || !info.cell_ids) continue;\n",
    "      var ids = String(info.cell_ids).split(',');\n",
    "      for (var i = 0; i < ids.length; i++) {\n",
    "        var cid = ids[i].trim();\n",
    "        if (!cid) continue;\n",
    "        var el = document.getElementById(cid);\n",
    "        if (!el) continue;\n",
    "        el.style.setProperty('background-color', info.color, 'important');\n",
    "        el.classList.add('statically-colored');\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  function apply_wrong_number_info() {\n",
    "    if (!show_wrong) return;\n",
    "    if (!wrong_info || !Array.isArray(wrong_info) || wrong_info.length === 0) return;\n",
    "\n",
    "    var cell_to_color = {};\n",
    "    for (var regid in reg_info) {\n",
    "      if (!reg_info.hasOwnProperty(regid)) continue;\n",
    "      var info = reg_info[regid];\n",
    "      if (!info || !info.color || !info.cell_ids) continue;\n",
    "      String(info.cell_ids).split(',').forEach(function(x){ cell_to_color[x.trim()] = info.color; });\n",
    "    }\n",
    "\n",
    "    wrong_info.forEach(function(case_item){\n",
    "      if (!case_item || !case_item.cell_id) return;\n",
    "      var cid = String(case_item.cell_id).trim();\n",
    "      var el = document.getElementById(cid);\n",
    "      if (!el) return;\n",
    "      el.classList.add('wrong-number-cell');\n",
    "      var reg_color = cell_to_color[cid] || '#f0f0f0';\n",
    "      var gradient = 'linear-gradient(45deg, #cccccc, ' + reg_color + ')';\n",
    "      el.style.setProperty('background-image', gradient, 'important');\n",
    "    });\n",
    "  }\n",
    "\n",
    "  function apply_issue_cell_outlines() {\n",
    "    if (!issue_cellids || !Array.isArray(issue_cellids) || issue_cellids.length === 0) return;\n",
    "    issue_cellids.forEach(function(cid_raw){\n",
    "      var cid = String(cid_raw || '').trim();\n",
    "      if (!cid) return;\n",
    "      var el = document.getElementById(cid);\n",
    "      if (!el) return;\n",
    "      el.classList.add('issue-cell');\n",
    "    });\n",
    "  }\n",
    "\n",
    "  apply_static_coloring();\n",
    "  apply_wrong_number_info();\n",
    "  apply_issue_cell_outlines();\n",
    "})();\n"
  )
}
rr_extract_cellids_from_issue_records = function(issues) {
  cids = character(0)
  for (i in seq_along(issues)) {
    rec = issues[[i]]
    if (is.null(rec) || !is.list(rec)) next

    if (!is.null(rec$cellids)) {
      vals = unlist(stringi::stri_split_fixed(as.character(rec$cellids), ",", omit_empty = TRUE))
      vals = stringi::stri_trim_both(vals)
      cids = c(cids, vals)
    } else if (!is.null(rec$cellid)) {
      vals = stringi::stri_trim_both(as.character(rec$cellid))
      cids = c(cids, vals)
    }
  }
  cids
}
rr_make_single_table_html = function(tabtitle, tabhtml, mapping, show_wrong_number = TRUE, issue_cellids = NULL) {
  restore.point("rr_make_single_table_html")

  reg_info_json = jsonlite::toJSON(mapping$reg_info %||% list(), auto_unbox = TRUE, null = "null", na = "null")
  wrong_info_json = jsonlite::toJSON(mapping$wrong_number_info %||% list(), auto_unbox = TRUE, null = "null", na = "null")

  issues_cellids_json = if (!is.null(issue_cellids)) {
     jsonlite::toJSON(as.list(unique(issue_cellids)), auto_unbox = TRUE, null = "null", na = "null")
  } else {
     "[]"
  }

  css = rr_single_table_css()
  js = rr_single_table_js(
    reg_info_json = reg_info_json,
    wrong_info_json = wrong_info_json,
    show_wrong_number = isTRUE(show_wrong_number),
    issues_cellids_json = issues_cellids_json
  )

  htmltools::tagList(
    htmltools::tags$html(
      htmltools::tags$head(
        htmltools::tags$meta(charset = "UTF-8"),
        htmltools::tags$meta(name = "viewport", content = "width=device-width, initial-scale=1"),
        htmltools::tags$title(paste0("Table ", htmltools::htmlEscape(as.character(tabtitle)))),
        htmltools::tags$style(htmltools::HTML(css))
      ),
      htmltools::tags$body(
        htmltools::tags$div(
          id = "rr-page",
          htmltools::tags$div(
            id = "rr-table-container",
            htmltools::tags$div(class = "art-tab-div",
              htmltools::tags$h4(class = "rr-title", htmltools::htmlEscape(as.character(tabtitle))),
              htmltools::HTML(tabhtml)
            )
          )
        ),
        htmltools::tags$script(htmltools::HTML(js))
      )
    )
  )
}
```
# END OF FILE: rr_table.R

-----------------------------------------------------------


# FILE: rr_utils.R
```
#' Generate a set of visually distinct colors
#'
#' Uses a Halton sequence to generate colors with distinct hues, suitable for
#' color-coding different categories.
#'
#' @param n The number of colors to generate.
#' @param h.range Range of hue (0-360).
#' @param s.range Range of saturation (0-1).
#' @param l.range Range of lightness (0-1).
#' @return A vector of hexadecimal color strings.
#' @export
rr_make_distinct_colors = function(n, h.range=c(15, 345), s.range=c(0.6, 0.9), l.range=c(0.7, 0.85)) {
  if (n == 0) return(character(0))
  if (n == 1) return(rr_hsl_to_rgb(h.range[1], s.range[2], l.range[2]))

  # Use Halton sequence for nicely spaced values
  halton_seq <- randtoolbox::halton(n, dim = 2)

  h <- h.range[1] + (h.range[2] - h.range[1]) * halton_seq[, 1]
  s <- s.range[1] + (s.range[2] - s.range[1]) * halton_seq[, 2]
  l <- l.range[1] + (l.range[2] - l.range[1]) * halton_seq[, 2]

  rr_hsl_to_rgb(h, s, l)
}

#' Convert HSL color values to RGB
#'
#' @param h Hue (0-360).
#' @param s Saturation (0-1).
#' @param l Lightness (0-1).
#' @return A hexadecimal color string.
#' @export
rr_hsl_to_rgb <- function(h, s, l) {
  h <- h / 360

  hue_to_rgb <- function(p, q, t) {
    t[t < 0] <- t[t < 0] + 1
    t[t > 1] <- t[t > 1] - 1

    ifelse(t < 1/6, p + (q - p) * 6 * t,
      ifelse(t < 1/2, q,
        ifelse(t < 2/3, p + (q - p) * (2/3 - t) * 6, p)
      )
    )
  }

  q <- ifelse(l < 0.5, l * (1 + s), l + s - l * s)
  p <- 2 * l - q

  r <- hue_to_rgb(p, q, h + 1/3)
  g <- hue_to_rgb(p, q, h)
  b <- hue_to_rgb(p, q, h - 1/3)

  grDevices::rgb(r, g, b)
}
```
# END OF FILE: rr_utils.R

-----------------------------------------------------------


# FILE: pipeline.md
```
# Default Analysis Pipeline

Main goal is to build a database that maps regressions from article to regressions in code and the results from the reproduction run.

Our small pipeline below uses AI ideally just 3 times per document (article / appendix):

1. PDF to MD via Mistral OCR (mocr).
2. Regression mapping `map_reg_run` via Gemini.
3. Regression classification `reg_classify` via Gemini.


## 1. Table extraction up to step tab_main

### For PDF documents (article or appendix)

We use the Mistral OCR (mocr) representation of the PDF document. It performed pretty well in table extraction.

The folder `ejd_files` now contains for most articles mocr Rds files that contain per page md code and extracted figures with base64 encoding.

`repboxDoc` performs preliminary computation for such an Rds file for a project and stores all files in the projects's `doc` folder. E.g. it extracts the tables from the markdown representation.

Conversion to the `fp` products like `tab_main` is done by ???

## 2. Regression mapping

We experimented with different ways to map regressions between code and article. Our default pipeline only uses `map_reg_run`. 

- `map_reg_run` uses the log file with output from regressions and post regression commands like tests. The AI is asked to go through all tables in the document (article or appendix) and identify cells that belong to a regression and map them to code line and runid. Mapping shall also take place for regressions that may not have successfully run.


Comparision to alternative mapping approaches:

- `map_reg_static` is similar to `map_reg_run` but does not use run logs and only maps to code lines. Problems: in loops a code line corresponds to multiple regressions, we don't have an exact mapping. Also possibily less precise given that no regression output is given. May be an alternative for articles that we cannot reproduce, but mapping for those articles is less important as they cannot be used for methodological meta studies where we run variations of the original regression. So low priority.

- `map_inv_reg_run` asks to find for each regression in the code a regression in a table or image. Seems to work less well than asking the AI to go systematically through the tables and then find map in the code. So not currently used.

## 3. Regression classification 

`reg_classify` will the main product that classifies regressions, e.g. the variable of interest or method of causal identification. The prompt and product is not yet fully fletched out. In particular, we have not yet added a map between regression labels and variable names shown in the Stata output.


### Alternatives

`reg_classify_static` performs regression classification that would work without reproduction using only the regression classifications from `map_reg_static`



```
# END OF FILE: pipeline.md

-----------------------------------------------------------



##################################
# Your Task
##################################


Tell me a bit about the pipeline implementation. I want to implement the steps in


# Implement default pipeline
# see pipeline.md
repbox_fp_steps_pipeline = function(tab_mocr=TRUE, tab_main=FALSE,map_reg_run=TRUE, ev_tab=TRUE, reg_classify=TRUE) {
      as.list(sys.frame(sys.parent(0)))
}

But I don't know what needs to be done. 

1. Is the chain a mocr doc generated by reportDoc to tab_main already implemented? Which steps would I need to run? Makes it sense to define a single tab_mocr step or is all already done? How to come to tab_main and cell_base products?

2. reg_classify is probably not yet correctly implemented, previously I had only reg_classify_static and just replaced all code with reg_classify. But here we have a nested json structure. Where do I have to replace stuff.

3. Some other changes needed?

Mainly explain to me, you can also suggest some concrete code changes, where and what. Don't rewrite complete files, but as little changes as needed.



